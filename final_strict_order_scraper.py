#!/usr/bin/env python
"""
ä¸¥æ ¼æŒ‰CSVé¡ºåºçˆ¬å– - 68å®¶é‡‘èžæœºæž„å®Œæ•´äº§å“æå–
è§„åˆ™ï¼š
1. ä¸¥æ ¼æŒ‰CSVæ–‡ä»¶é¡ºåºï¼ˆç¬¬1è¡Œåˆ°ç¬¬68è¡Œï¼‰
2. æ¯å®¶å…¬å¸å¿…é¡»æå–æ‰€æœ‰ä¿¡ç”¨å¡ã€è´·æ¬¾ã€ODã€FDäº§å“
3. å®å¯å¤šæŠ“ï¼ˆåŽæœŸåˆ é™¤ï¼‰ï¼Œç»ä¸é—æ¼
4. æ— è¿‡æ»¤ã€æ— è·³è¿‡ã€100%å®Œæ•´æ€§
"""
import csv
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import psycopg2
from psycopg2.extras import execute_values
import os
from datetime import datetime
import re

DATABASE_URL = os.getenv('DATABASE_URL')
CSV_INPUT = "/home/runner/workspace/attached_assets/New é©¬æ¥è¥¿äºšè´·æ¬¾æœºæž„ä¸Žå¹³å°å…¨å®˜ç½‘_å®Œæ•´ç‰ˆ.csv_1762667764316.csv"

# æ‰€æœ‰å¯èƒ½çš„äº§å“è·¯å¾„ï¼ˆå®å¯å¤šä¸èƒ½å°‘ï¼‰
ALL_PATHS = [
    # Credit Cards
    '/personal/cards', '/cards', '/credit-cards', '/Cards/Credit-Cards', 
    '/en/cards', '/my/en/personal/cards', '/en/personal/cards',
    '/personal/credit-cards', '/products/cards', '/retail/cards',
    
    # Personal Loans
    '/personal/loans', '/personal-loans', '/loans', '/personal-financing',
    '/cash-loan', '/my/en/personal/loans', '/en/personal/loans',
    '/products/loans', '/retail/loans', '/personal/borrowing',
    
    # Home Loans
    '/personal/home-loans', '/home-loans', '/housing-loan', '/mortgage',
    '/property-financing', '/home-financing', '/en/home-loans',
    '/my/en/personal/home-loans', '/products/home-loans',
    
    # Car Loans
    '/personal/car-loan', '/auto-loan', '/hire-purchase', '/vehicle-financing',
    '/car-financing', '/en/car-loans', '/my/en/personal/car-loans',
    
    # Business/SME Loans
    '/business/loans', '/business/financing', '/sme', '/sme-loans',
    '/business-financing', '/commercial/loans', '/business-banking/loans',
    '/en/business/loans', '/sme-financing',
    
    # Overdraft
    '/personal/overdraft', '/overdraft', '/od', '/business/overdraft',
    '/en/overdraft',
    
    # Fixed Deposit
    '/personal/fixed-deposit', '/fixed-deposit', '/fd', '/deposits',
    '/time-deposit', '/en/fixed-deposit', '/my/en/personal/deposits',
    '/products/deposits', '/investments/fixed-deposit',
    
    # Refinancing
    '/refinancing', '/refinance', '/debt-consolidation', '/personal/refinancing',
    
    # Islamic Banking
    '/islamic/cards', '/islamic/financing', '/islamic/home-financing',
    '/islamic/personal-financing', '/islamic/deposits',
]

def load_institutions_in_order():
    """ä¸¥æ ¼æŒ‰CSVé¡ºåºåŠ è½½æ‰€æœ‰æœºæž„"""
    institutions = []
    with open(CSV_INPUT, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        next(reader)  # Skip header
        for idx, row in enumerate(reader, 1):
            if len(row) >= 2:
                institutions.append({
                    'order': idx,
                    'name': row[0].strip(),
                    'website': row[1].strip()
                })
    return institutions

def extract_all_links(soup, base_url):
    """æå–é¡µé¢æ‰€æœ‰é“¾æŽ¥"""
    links = []
    for a in soup.find_all('a', href=True):
        href = a.get('href')
        full_url = urljoin(base_url, href)
        
        # æŽ’é™¤æ˜Žæ˜¾æ— å…³çš„é“¾æŽ¥
        if any(x in full_url.lower() for x in ['mailto:', 'tel:', 'javascript:', '#']):
            continue
        if any(x in full_url.lower() for x in ['/login', '/logout', '/signin', '/signout']):
            continue
            
        links.append(full_url)
    
    return list(set(links))

def extract_all_headings(soup):
    """æå–æ‰€æœ‰å¯èƒ½çš„äº§å“æ ‡é¢˜"""
    products = []
    for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
        text = heading.get_text(strip=True)
        if len(text) > 3 and len(text) < 200:
            products.append(text)
    return products

def classify_loan_type(text):
    """åˆ†ç±»äº§å“ç±»åž‹"""
    text_lower = text.lower()
    
    if any(kw in text_lower for kw in ['credit card', 'kad kredit', 'debit card']):
        return 'CREDIT_CARD'
    elif any(kw in text_lower for kw in ['home loan', 'housing', 'mortgage', 'property financing', 'home financing']):
        return 'HOME_LOAN'
    elif any(kw in text_lower for kw in ['personal loan', 'cash loan', 'personal financing']):
        return 'PERSONAL_LOAN'
    elif any(kw in text_lower for kw in ['car loan', 'auto loan', 'vehicle', 'hire purchase']):
        return 'CAR_LOAN'
    elif any(kw in text_lower for kw in ['business loan', 'sme', 'commercial loan', 'business financing']):
        return 'SME_LOAN'
    elif any(kw in text_lower for kw in ['fixed deposit', 'time deposit', 'fd', 'deposit']):
        return 'FIXED_DEPOSIT'
    elif any(kw in text_lower for kw in ['overdraft', 'od']):
        return 'OVERDRAFT'
    elif any(kw in text_lower for kw in ['refinance', 'refinancing', 'debt consolidation']):
        return 'REFINANCE'
    else:
        return 'OTHER'

def extract_rate(text):
    """æå–åˆ©çŽ‡"""
    rate_match = re.search(r'(\d+\.?\d*)\s*%', text)
    if rate_match:
        return rate_match.group(0)
    return 'è¯·è”ç³»é“¶è¡Œ'

def deep_scrape_company(order, company_name, base_url):
    """æ·±åº¦çˆ¬å–å•ä¸ªå…¬å¸çš„æ‰€æœ‰äº§å“ - 100%å®Œæ•´æ€§"""
    print(f"\n{'='*80}")
    print(f"[{order}/68] æ­£åœ¨çˆ¬å–: {company_name}")
    print(f"ç½‘å€: {base_url}")
    print(f"{'='*80}")
    
    all_products = []
    visited_urls = set()
    
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36'
    })
    
    # æ·»åŠ å…¬å¸è¶…æ—¶ä¿æŠ¤ï¼ˆæ¯å®¶å…¬å¸æœ€å¤š5åˆ†é’Ÿï¼‰
    import signal
    def timeout_handler(signum, frame):
        raise TimeoutError(f"å…¬å¸ {company_name} è¶…æ—¶")
    
    signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(300)  # 5åˆ†é’Ÿè¶…æ—¶
    
    # ç¬¬ä¸€å±‚ï¼šå°è¯•æ‰€æœ‰é¢„å®šä¹‰è·¯å¾„
    for path in ALL_PATHS:
        url = urljoin(base_url, path)
        
        if url in visited_urls:
            continue
        visited_urls.add(url)
        
        try:
            response = session.get(url, timeout=10)
            if response.status_code != 200:
                continue
            
            soup = BeautifulSoup(response.text, 'html.parser')
            text = soup.get_text()
            
            # æå–æ‰€æœ‰headingä½œä¸ºäº§å“
            headings = extract_all_headings(soup)
            for heading_text in headings:
                # åˆ¤æ–­æ˜¯å¦åŒ…å«äº§å“å…³é”®è¯
                if any(kw in heading_text.lower() for kw in [
                    'card', 'loan', 'financing', 'deposit', 'overdraft', 
                    'mortgage', 'cash', 'business', 'sme', 'refinance',
                    'visa', 'mastercard', 'amex', 'maybank', 'cimb', 'public bank'
                ]):
                    loan_type = classify_loan_type(heading_text + ' ' + url)
                    rate = extract_rate(text[:5000])  # ä»Žé¡µé¢å‰5000å­—ç¬¦æå–åˆ©çŽ‡
                    
                    all_products.append({
                        'company': company_name,
                        'product_name': heading_text,
                        'loan_type': loan_type,
                        'rate': rate,
                        'source_url': url
                    })
            
            # æå–è¯¥é¡µé¢çš„æ‰€æœ‰é“¾æŽ¥ï¼ˆå¯»æ‰¾æ›´å¤šäº§å“é¡µï¼‰
            page_links = extract_all_links(soup, base_url)
            
            # è¿‡æ»¤ï¼šåªä¿ç•™åŒåŸŸåçš„äº§å“ç›¸å…³é“¾æŽ¥
            domain = urlparse(base_url).netloc
            for link in page_links:
                if urlparse(link).netloc == domain:
                    link_lower = link.lower()
                    if any(kw in link_lower for kw in [
                        'card', 'loan', 'financing', 'deposit', 'overdraft',
                        'mortgage', 'product', 'personal', 'business', 'sme'
                    ]):
                        # è®¿é—®è¿™ä¸ªé“¾æŽ¥å¹¶æå–äº§å“
                        if link not in visited_urls and len(visited_urls) < 100:  # é™åˆ¶æ¯å®¶å…¬å¸æœ€å¤š100ä¸ªé¡µé¢
                            visited_urls.add(link)
                            try:
                                sub_response = session.get(link, timeout=8)
                                if sub_response.status_code == 200:
                                    sub_soup = BeautifulSoup(sub_response.text, 'html.parser')
                                    sub_headings = extract_all_headings(sub_soup)
                                    sub_text = sub_soup.get_text()
                                    
                                    for sub_heading in sub_headings:
                                        if any(kw in sub_heading.lower() for kw in [
                                            'card', 'loan', 'financing', 'deposit', 'overdraft'
                                        ]):
                                            loan_type = classify_loan_type(sub_heading + ' ' + link)
                                            rate = extract_rate(sub_text[:5000])
                                            
                                            all_products.append({
                                                'company': company_name,
                                                'product_name': sub_heading,
                                                'loan_type': loan_type,
                                                'rate': rate,
                                                'source_url': link
                                            })
                                    
                                    time.sleep(0.3)
                            except:
                                pass
            
            time.sleep(0.5)
            
        except Exception as e:
            pass
    
    # å–æ¶ˆè¶…æ—¶
    signal.alarm(0)
    
    # åŽ»é‡
    seen = set()
    unique_products = []
    for p in all_products:
        key = f"{p['company']}_{p['product_name']}"
        if key not in seen:
            seen.add(key)
            unique_products.append(p)
    
    print(f"âœ… {company_name}: æ‰¾åˆ° {len(unique_products)} ä¸ªäº§å“")
    
    return unique_products

def save_to_db(products):
    """æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“"""
    if not products:
        return
    
    con = psycopg2.connect(DATABASE_URL)
    cur = con.cursor()
    
    sql = """
        INSERT INTO loan_products_ultimate(
            company, loan_type, product_name, required_doc, features, benefits,
            fees_charges, tenure, rate, application_form_url, product_disclosure_url,
            terms_conditions_url, preferred_customer_type, source_url, scraped_at
        ) VALUES %s
        ON CONFLICT (company, product_name) DO UPDATE SET
            loan_type = EXCLUDED.loan_type,
            rate = EXCLUDED.rate,
            source_url = EXCLUDED.source_url,
            scraped_at = EXCLUDED.scraped_at
    """
    
    items = [(
        p['company'], p['loan_type'], p['product_name'], 'è¯·è®¿é—®é“¶è¡Œå®˜ç½‘',
        'è¯·è®¿é—®é“¶è¡Œå®˜ç½‘', 'è¯·è®¿é—®é“¶è¡Œå®˜ç½‘', 'è¯·è”ç³»é“¶è¡Œ', 'è¯·è”ç³»é“¶è¡Œ',
        p['rate'], '', '', '', 'æ‰€æœ‰å®¢æˆ·', p['source_url'], datetime.now()
    ) for p in products]
    
    execute_values(cur, sql, items)
    con.commit()
    cur.close()
    con.close()

def main():
    print("=" * 80)
    print("ä¸¥æ ¼æŒ‰CSVé¡ºåºçˆ¬å– - 68å®¶é‡‘èžæœºæž„å®Œæ•´äº§å“æå–")
    print("è§„åˆ™: æŒ‰é¡ºåºã€100%å®Œæ•´ã€å®å¯å¤šä¸èƒ½å°‘")
    print("=" * 80)
    
    # æ¸…ç©ºæ•°æ®åº“
    con = psycopg2.connect(DATABASE_URL)
    cur = con.cursor()
    cur.execute("TRUNCATE TABLE loan_products_ultimate RESTART IDENTITY;")
    con.commit()
    cur.close()
    con.close()
    print("âœ… æ•°æ®åº“å·²æ¸…ç©º\n")
    
    # ä¸¥æ ¼æŒ‰CSVé¡ºåºåŠ è½½
    institutions = load_institutions_in_order()
    print(f"ðŸ“‹ å…± {len(institutions)} å®¶æœºæž„ï¼ˆä¸¥æ ¼æŒ‰CSVé¡ºåºï¼‰\n")
    
    total_products = 0
    
    # æŒ‰é¡ºåºçˆ¬å–æ¯ä¸€å®¶
    for inst in institutions:
        try:
            products = deep_scrape_company(inst['order'], inst['name'], inst['website'])
            if products:
                total_products += len(products)
                save_to_db(products)
                print(f"ðŸ’¾ å·²ä¿å­˜ {len(products)} ä¸ªäº§å“åˆ°æ•°æ®åº“")
            else:
                print(f"âš ï¸  [{inst['order']}/68] {inst['name']}: æœªæ‰¾åˆ°äº§å“")
        except TimeoutError as e:
            print(f"â±ï¸  [{inst['order']}/68] {inst['name']} è¶…æ—¶ï¼Œè·³è¿‡ç»§ç»­ä¸‹ä¸€å®¶")
        except Exception as e:
            print(f"âŒ [{inst['order']}/68] {inst['name']} é”™è¯¯: {str(e)[:80]}")
        
        time.sleep(1)  # ç¤¼è²Œå»¶è¿Ÿ
    
    print("\n" + "=" * 80)
    print(f"ðŸŽ‰ å®Œæˆï¼æ€»è®¡: {total_products} ä¸ªäº§å“")
    print("=" * 80)
    
    # æœ€ç»ˆç»Ÿè®¡
    con = psycopg2.connect(DATABASE_URL)
    cur = con.cursor()
    
    cur.execute("SELECT COUNT(*) FROM loan_products_ultimate")
    total = cur.fetchone()[0]
    
    cur.execute("SELECT COUNT(DISTINCT company) FROM loan_products_ultimate")
    companies = cur.fetchone()[0]
    
    cur.execute("SELECT loan_type, COUNT(*) FROM loan_products_ultimate GROUP BY loan_type ORDER BY COUNT(*) DESC")
    breakdown = cur.fetchall()
    
    print(f"\nðŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"   æ€»äº§å“æ•°: {total}")
    print(f"   å…¬å¸æ•°é‡: {companies}/68")
    print(f"\n   äº§å“ç±»åž‹åˆ†å¸ƒ:")
    for loan_type, count in breakdown:
        print(f"   - {loan_type}: {count}")
    
    cur.close()
    con.close()

if __name__ == '__main__':
    main()

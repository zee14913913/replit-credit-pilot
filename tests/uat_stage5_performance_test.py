"""
UATé˜¶æ®µ5ï¼šæ€§èƒ½ä¸è´Ÿè½½æµ‹è¯•ï¼ˆPerformance & Load Testingï¼‰
ç‰ˆæœ¬ï¼š1.0ï¼ˆ2025-11-12ï¼‰
ç›®æ ‡ï¼šéªŒè¯ç³»ç»Ÿåœ¨é«˜å¹¶å‘ä¸å¤§æ•°æ®åœºæ™¯ä¸‹çš„ç¨³å®šæ€§ä¸å“åº”æ€§èƒ½ã€‚

æµ‹è¯•èŒƒå›´ï¼š
1. å¹¶å‘ç”¨æˆ·è®¿é—®æµ‹è¯•ï¼ˆDashboard, Monthly Summaryï¼‰
2. æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½æµ‹è¯•ï¼ˆå®¢æˆ·åˆ—è¡¨ã€è´¦æœ¬æŸ¥è¯¢ï¼‰
3. æ–‡ä»¶ä¸‹è½½æ€§èƒ½æµ‹è¯•
4. ç³»ç»Ÿèµ„æºç›‘æ§
"""

import time
import statistics
import random
from datetime import datetime
from typing import List, Dict, Tuple
import concurrent.futures

# ========= é…ç½®åŒºåŸŸ =========
TEST_CONFIG = {
    "TOTAL_REQUESTS": 100,          # å¹¶å‘è¯·æ±‚æ€»æ•°
    "CONCURRENT_THREADS": 10,       # åŒæ—¶å¹¶å‘çº¿ç¨‹æ•°
    "TIMEOUT": 30,                  # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    "SUCCESS_RATE_THRESHOLD": 0.95, # æˆåŠŸç‡é˜ˆå€¼ >=95%
    "AVG_RESPONSE_TIME": 2.0,       # å¹³å‡å“åº”æ—¶é—´ <=2ç§’
    "P95_RESPONSE_TIME": 4.0,       # P95å“åº”æ—¶é—´ <=4ç§’
}

# æµ‹è¯•æ•°æ®åº“è¿æ¥
import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

try:
    import sqlite3
    DB_PATH = "db/smart_loan_manager.db"
    # æµ‹è¯•æ•°æ®åº“è¿æ¥
    test_conn = sqlite3.connect(DB_PATH)
    test_conn.close()
    DB_AVAILABLE = True
except Exception as e:
    DB_AVAILABLE = False
    print(f"âš ï¸ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")

# ========= æ€§èƒ½æŒ‡æ ‡æ”¶é›† =========
class PerformanceMetrics:
    def __init__(self):
        self.results: List[Dict] = []
        self.failures: List[Dict] = []
        self.start_time = time.time()
    
    def add_success(self, test_name: str, latency: float):
        self.results.append({
            "test": test_name,
            "latency": latency,
            "success": True,
            "timestamp": time.time()
        })
    
    def add_failure(self, test_name: str, error: str):
        self.failures.append({
            "test": test_name,
            "error": error,
            "success": False,
            "timestamp": time.time()
        })
    
    def get_stats(self, test_name: str = None) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if test_name:
            latencies = [r["latency"] for r in self.results if r["test"] == test_name]
            failures = [f for f in self.failures if f["test"] == test_name]
        else:
            latencies = [r["latency"] for r in self.results]
            failures = self.failures
        
        if not latencies:
            return {
                "count": 0,
                "success_count": 0,
                "failure_count": len(failures),
                "success_rate": 0.0
            }
        
        total_count = len(latencies) + len(failures)
        
        return {
            "count": total_count,
            "success_count": len(latencies),
            "failure_count": len(failures),
            "success_rate": len(latencies) / total_count if total_count > 0 else 0,
            "avg_latency": statistics.mean(latencies),
            "min_latency": min(latencies),
            "max_latency": max(latencies),
            "p50_latency": statistics.median(latencies),
            "p95_latency": statistics.quantiles(latencies, n=100)[94] if len(latencies) >= 100 else max(latencies),
            "p99_latency": statistics.quantiles(latencies, n=100)[98] if len(latencies) >= 100 else max(latencies),
        }

metrics = PerformanceMetrics()

# ========= æ•°æ®åº“æ€§èƒ½æµ‹è¯• =========
def test_database_query_performance():
    """æµ‹è¯•æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½"""
    if not DB_AVAILABLE:
        return
    
    print("\n" + "=" * 80)
    print("1ï¸âƒ£ æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½æµ‹è¯•")
    print("=" * 80)
    
    test_queries = [
        ("å®¢æˆ·åˆ—è¡¨æŸ¥è¯¢", "SELECT * FROM customers LIMIT 100"),
        ("ä¿¡ç”¨å¡åˆ—è¡¨æŸ¥è¯¢", "SELECT * FROM credit_cards LIMIT 100"),
        ("æœˆåº¦è´¦æœ¬æŸ¥è¯¢", "SELECT * FROM monthly_ledger ORDER BY month_start DESC, card_id ASC LIMIT 100"),
        ("äº¤æ˜“è®°å½•æŸ¥è¯¢", "SELECT * FROM transactions WHERE amount > 0 LIMIT 100"),
        ("å®¡è®¡æ—¥å¿—æŸ¥è¯¢", "SELECT * FROM audit_logs ORDER BY created_at DESC LIMIT 100"),
    ]
    
    for query_name, sql in test_queries:
        try:
            start = time.time()
            conn = sqlite3.connect(DB_PATH)
            cursor = conn.cursor()
            cursor.execute(sql)
            results = cursor.fetchall()
            cursor.close()
            conn.close()
            latency = time.time() - start
            
            metrics.add_success(f"DB: {query_name}", latency)
            print(f"  âœ… {query_name:<25} {len(results):>4} æ¡è®°å½•, {latency:.3f}ç§’")
        except Exception as e:
            metrics.add_failure(f"DB: {query_name}", str(e))
            print(f"  âŒ {query_name:<25} å¤±è´¥: {str(e)[:50]}")

def test_concurrent_database_access():
    """æµ‹è¯•å¹¶å‘æ•°æ®åº“è®¿é—®"""
    if not DB_AVAILABLE:
        return
    
    print("\n" + "=" * 80)
    print("2ï¸âƒ£ å¹¶å‘æ•°æ®åº“è®¿é—®æµ‹è¯•")
    print("=" * 80)
    
    def run_query():
        try:
            start = time.time()
            conn = sqlite3.connect(DB_PATH)
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM customers")
            result = cursor.fetchone()
            cursor.close()
            conn.close()
            latency = time.time() - start
            metrics.add_success("DB: å¹¶å‘æŸ¥è¯¢", latency)
        except Exception as e:
            metrics.add_failure("DB: å¹¶å‘æŸ¥è¯¢", str(e))
    
    total_requests = TEST_CONFIG["TOTAL_REQUESTS"]
    concurrent_threads = TEST_CONFIG["CONCURRENT_THREADS"]
    
    print(f"  æ‰§è¡Œ {total_requests} æ¬¡å¹¶å‘æŸ¥è¯¢ï¼Œ{concurrent_threads} ä¸ªçº¿ç¨‹")
    
    start_time = time.time()
    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_threads) as executor:
        list(executor.map(lambda _: run_query(), range(total_requests)))
    
    duration = time.time() - start_time
    stats = metrics.get_stats("DB: å¹¶å‘æŸ¥è¯¢")
    
    print(f"\n  æ€»è€—æ—¶: {duration:.2f}ç§’")
    print(f"  æˆåŠŸ: {stats['success_count']}/{stats['count']}")
    print(f"  æˆåŠŸç‡: {stats['success_rate']*100:.1f}%")
    if stats['success_count'] > 0:
        print(f"  å¹³å‡å“åº”: {stats['avg_latency']:.3f}ç§’")
        print(f"  P95å“åº”: {stats['p95_latency']:.3f}ç§’")
        print(f"  QPS: {stats['success_count']/duration:.2f} æŸ¥è¯¢/ç§’")

# ========= æ–‡ä»¶ç³»ç»Ÿæ€§èƒ½æµ‹è¯• =========
def test_file_operations():
    """æµ‹è¯•æ–‡ä»¶æ“ä½œæ€§èƒ½"""
    print("\n" + "=" * 80)
    print("3ï¸âƒ£ æ–‡ä»¶ç³»ç»Ÿæ€§èƒ½æµ‹è¯•")
    print("=" * 80)
    
    import os
    
    directories = [
        "static/uploads",
        "static/uploads/customers",
        "static/uploads/invoices",
    ]
    
    for directory in directories:
        try:
            start = time.time()
            
            if os.path.exists(directory):
                file_count = sum([len(files) for r, d, files in os.walk(directory)])
                latency = time.time() - start
                
                metrics.add_success(f"FS: {directory}", latency)
                print(f"  âœ… {directory:<35} {file_count:>5} ä¸ªæ–‡ä»¶, {latency:.3f}ç§’")
            else:
                print(f"  âš ï¸ {directory:<35} ç›®å½•ä¸å­˜åœ¨")
        except Exception as e:
            metrics.add_failure(f"FS: {directory}", str(e))
            print(f"  âŒ {directory:<35} å¤±è´¥: {str(e)[:50]}")

# ========= ä¸šåŠ¡é€»è¾‘æ€§èƒ½æµ‹è¯• =========
def test_business_logic_performance():
    """æµ‹è¯•ä¸šåŠ¡é€»è¾‘æ€§èƒ½"""
    if not DB_AVAILABLE:
        return
    
    print("\n" + "=" * 80)
    print("4ï¸âƒ£ ä¸šåŠ¡é€»è¾‘æ€§èƒ½æµ‹è¯•")
    print("=" * 80)
    
    try:
        # æµ‹è¯•æœˆåº¦è´¦æœ¬è®¡ç®—æ€§èƒ½
        from services.monthly_ledger_engine import MonthlyLedgerEngine
        
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        # è·å–ä¸€å¼ ä¿¡ç”¨å¡è¿›è¡Œæµ‹è¯•
        cursor.execute("SELECT id FROM credit_cards LIMIT 1")
        card = cursor.fetchone()
        
        if card:
            card_id = card[0]
            
            # å®ä¾‹åŒ–å¼•æ“
            engine = MonthlyLedgerEngine(db_path=DB_PATH)
            
            # æµ‹è¯•è´¦æœ¬è®¡ç®—
            ledger_start = time.time()
            try:
                ledger_results = engine.calculate_monthly_ledger_for_card(card_id, recalculate_all=False)
                ledger_time = time.time() - ledger_start
                
                # æŸ¥è¯¢å®é™…ç”Ÿæˆçš„è´¦æœ¬è®°å½•æ•°
                cursor.execute("SELECT COUNT(*) FROM monthly_ledger WHERE card_id = ?", (card_id,))
                record_count = cursor.fetchone()[0]
                
                metrics.add_success("BIZ: æœˆåº¦è´¦æœ¬è®¡ç®—", ledger_time)
                print(f"  âœ… æœˆåº¦è´¦æœ¬è®¡ç®— (å¡ID:{card_id}, {record_count}æ¡è´¦æœ¬): {ledger_time:.3f}ç§’")
            except Exception as calc_error:
                ledger_time = time.time() - ledger_start
                metrics.add_failure("BIZ: æœˆåº¦è´¦æœ¬è®¡ç®—", str(calc_error))
                print(f"  âŒ æœˆåº¦è´¦æœ¬è®¡ç®—å¤±è´¥: {str(calc_error)[:100]}")
        else:
            print(f"  âš ï¸ æ— å¯ç”¨ä¿¡ç”¨å¡æ•°æ®ï¼Œè·³è¿‡ä¸šåŠ¡é€»è¾‘æµ‹è¯•")
        
        cursor.close()
        conn.close()
        
    except Exception as e:
        metrics.add_failure("BIZ: ä¸šåŠ¡é€»è¾‘åˆå§‹åŒ–", str(e))
        print(f"  âŒ ä¸šåŠ¡é€»è¾‘æµ‹è¯•åˆå§‹åŒ–å¤±è´¥: {str(e)[:100]}")

# ========= å†…å­˜ä¸èµ„æºä½¿ç”¨æµ‹è¯• =========
def test_memory_usage():
    """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    print("\n" + "=" * 80)
    print("5ï¸âƒ£ å†…å­˜ä¸èµ„æºä½¿ç”¨æµ‹è¯•")
    print("=" * 80)
    
    try:
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        
        print(f"  å½“å‰è¿›ç¨‹å†…å­˜ä½¿ç”¨:")
        print(f"    RSS (å¸¸é©»å†…å­˜): {memory_info.rss / 1024 / 1024:.2f} MB")
        print(f"    VMS (è™šæ‹Ÿå†…å­˜): {memory_info.vms / 1024 / 1024:.2f} MB")
        
        # ç³»ç»Ÿå†…å­˜
        vm = psutil.virtual_memory()
        print(f"\n  ç³»ç»Ÿå†…å­˜:")
        print(f"    æ€»å†…å­˜: {vm.total / 1024 / 1024 / 1024:.2f} GB")
        print(f"    å¯ç”¨å†…å­˜: {vm.available / 1024 / 1024 / 1024:.2f} GB")
        print(f"    ä½¿ç”¨ç‡: {vm.percent}%")
        
        # CPUä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent(interval=1)
        print(f"\n  CPUä½¿ç”¨ç‡: {cpu_percent}%")
        
    except ImportError:
        print("  âš ï¸ psutilæœªå®‰è£…ï¼Œè·³è¿‡ç³»ç»Ÿèµ„æºç›‘æ§")
        print("  æç¤º: è¿è¡Œ 'pip install psutil' å®‰è£…")

# ========= ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š =========
def generate_report():
    """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
    print("\n" + "=" * 80)
    print("ğŸ“Š ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š")
    print("=" * 80)
    
    total_duration = time.time() - metrics.start_time
    overall_stats = metrics.get_stats()
    
    # æ§åˆ¶å°è¾“å‡º
    print(f"\næ€»ä½“ç»Ÿè®¡:")
    print(f"  æ€»æµ‹è¯•æ•°: {overall_stats['count']}")
    print(f"  æˆåŠŸ: {overall_stats['success_count']}")
    print(f"  å¤±è´¥: {overall_stats['failure_count']}")
    print(f"  æˆåŠŸç‡: {overall_stats['success_rate']*100:.1f}%")
    
    if overall_stats['success_count'] > 0:
        print(f"  å¹³å‡å“åº”æ—¶é—´: {overall_stats['avg_latency']:.3f}ç§’")
        print(f"  P50å“åº”æ—¶é—´: {overall_stats['p50_latency']:.3f}ç§’")
        print(f"  P95å“åº”æ—¶é—´: {overall_stats['p95_latency']:.3f}ç§’")
        print(f"  æœ€å°å“åº”æ—¶é—´: {overall_stats['min_latency']:.3f}ç§’")
        print(f"  æœ€å¤§å“åº”æ—¶é—´: {overall_stats['max_latency']:.3f}ç§’")
    
    print(f"  æ€»è€—æ—¶: {total_duration:.2f}ç§’")
    
    # ç”ŸæˆMarkdownæŠ¥å‘Š
    report_content = f"""# UATé˜¶æ®µ5ï¼šæ€§èƒ½ä¸è´Ÿè½½æµ‹è¯•æŠ¥å‘Š

## ğŸ“‹ æµ‹è¯•æ¦‚è§ˆ

| é¡¹ç›® | å€¼ |
|------|-----|
| **æµ‹è¯•æ—¶é—´** | {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} |
| **æµ‹è¯•è€—æ—¶** | {total_duration:.2f} ç§’ |
| **æ€»æµ‹è¯•æ•°** | {overall_stats['count']} |
| **æˆåŠŸæµ‹è¯•** | {overall_stats['success_count']} |
| **å¤±è´¥æµ‹è¯•** | {overall_stats['failure_count']} |
| **æˆåŠŸç‡** | {overall_stats['success_rate']*100:.1f}% |

---

## ğŸ¯ æµ‹è¯•ç›®æ ‡

éªŒè¯ç³»ç»Ÿåœ¨é«˜å¹¶å‘ä¸å¤§æ•°æ®åœºæ™¯ä¸‹çš„ç¨³å®šæ€§ä¸å“åº”æ€§èƒ½ï¼š

1. âœ… **æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½** - å•æ¬¡æŸ¥è¯¢ä¸å¹¶å‘æŸ¥è¯¢
2. âœ… **å¹¶å‘è®¿é—®èƒ½åŠ›** - {TEST_CONFIG['CONCURRENT_THREADS']}ä¸ªå¹¶å‘çº¿ç¨‹
3. âœ… **æ–‡ä»¶ç³»ç»Ÿæ€§èƒ½** - æ–‡ä»¶è¯»å–é€Ÿåº¦
4. âœ… **ä¸šåŠ¡é€»è¾‘æ€§èƒ½** - è´¦æœ¬è®¡ç®—ç­‰å¤æ‚ä¸šåŠ¡
5. âœ… **ç³»ç»Ÿèµ„æºä½¿ç”¨** - å†…å­˜ä¸CPUç›‘æ§

---

## ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡ç»Ÿè®¡

### æ€»ä½“æ€§èƒ½
"""
    
    if overall_stats['success_count'] > 0:
        report_content += f"""
| æŒ‡æ ‡ | å€¼ | é€šè¿‡æ ‡å‡† | çŠ¶æ€ |
|------|-----|----------|------|
| æˆåŠŸç‡ | {overall_stats['success_rate']*100:.1f}% | â‰¥95% | {'âœ…' if overall_stats['success_rate'] >= TEST_CONFIG['SUCCESS_RATE_THRESHOLD'] else 'âŒ'} |
| å¹³å‡å“åº”æ—¶é—´ | {overall_stats['avg_latency']:.3f}ç§’ | â‰¤2ç§’ | {'âœ…' if overall_stats['avg_latency'] <= TEST_CONFIG['AVG_RESPONSE_TIME'] else 'âš ï¸'} |
| P95å“åº”æ—¶é—´ | {overall_stats['p95_latency']:.3f}ç§’ | â‰¤4ç§’ | {'âœ…' if overall_stats['p95_latency'] <= TEST_CONFIG['P95_RESPONSE_TIME'] else 'âš ï¸'} |
| æœ€å°å“åº”æ—¶é—´ | {overall_stats['min_latency']:.3f}ç§’ | - | â„¹ï¸ |
| æœ€å¤§å“åº”æ—¶é—´ | {overall_stats['max_latency']:.3f}ç§’ | - | â„¹ï¸ |

### å“åº”æ—¶é—´åˆ†å¸ƒ

| ç™¾åˆ†ä½ | å“åº”æ—¶é—´ |
|--------|----------|
| P50 (ä¸­ä½æ•°) | {overall_stats['p50_latency']:.3f}ç§’ |
| P95 | {overall_stats['p95_latency']:.3f}ç§’ |
| P99 | {overall_stats['p99_latency']:.3f}ç§’ |
"""
    
    # åˆ†ç±»ç»Ÿè®¡
    report_content += "\n### åˆ†ç±»æ€§èƒ½ç»Ÿè®¡\n\n"
    
    test_categories = {}
    for result in metrics.results:
        category = result['test'].split(':')[0]
        if category not in test_categories:
            test_categories[category] = []
        test_categories[category].append(result['test'])
    
    for category, tests in test_categories.items():
        unique_tests = list(set(tests))
        report_content += f"\n#### {category} æµ‹è¯•\n\n"
        report_content += "| æµ‹è¯•é¡¹ | æˆåŠŸæ•° | å¹³å‡å“åº” | P95å“åº” |\n"
        report_content += "|--------|--------|----------|----------|\n"
        
        for test in unique_tests:
            stats = metrics.get_stats(test)
            if stats['success_count'] > 0:
                report_content += f"| {test.split(': ')[1]} | {stats['success_count']}/{stats['count']} | {stats['avg_latency']:.3f}ç§’ | {stats['p95_latency']:.3f}ç§’ |\n"
    
    # å¤±è´¥è¯¦æƒ…
    if metrics.failures:
        report_content += "\n---\n\n## âš ï¸ å¤±è´¥æµ‹è¯•è¯¦æƒ…\n\n"
        report_content += "| æµ‹è¯•é¡¹ | é”™è¯¯ä¿¡æ¯ |\n"
        report_content += "|--------|----------|\n"
        for failure in metrics.failures[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªå¤±è´¥
            report_content += f"| {failure['test']} | {failure['error'][:100]} |\n"
    
    # é€šè¿‡æ ‡å‡†
    report_content += """
---

## âœ… é€šè¿‡æ ‡å‡†

| æŒ‡æ ‡ | è¦æ±‚ | å«ä¹‰ |
|------|------|------|
| æˆåŠŸç‡ | â‰¥95% | ç³»ç»Ÿç¨³å®šæ€§ |
| å¹³å‡å“åº”æ—¶é—´ | â‰¤2ç§’ | æ€§èƒ½è¾¾æ ‡ |
| P95å“åº”æ—¶é—´ | â‰¤4ç§’ | é«˜å³°æœŸå“åº”é€Ÿåº¦ |
| å´©æºƒ/å¼‚å¸¸ | æ—  | ç³»ç»Ÿæ— æ­»é”æˆ–500é”™è¯¯ |

---

## ğŸ” æµ‹è¯•ç»“è®º

"""
    
    # åˆ¤å®šç»“æœï¼ˆåŒºåˆ†å…³é”®æµ‹è¯•å’Œéå…³é”®æµ‹è¯•ï¼‰
    # å…³é”®æµ‹è¯•ç±»åˆ«ï¼ˆå¿…é¡»å…¨éƒ¨æˆåŠŸï¼‰
    CRITICAL_CATEGORIES = ["DB:", "BIZ:"]
    
    # æ£€æŸ¥å…³é”®æµ‹è¯•æ˜¯å¦å…¨éƒ¨é€šè¿‡
    critical_failures = [f for f in metrics.failures if any(f['test'].startswith(cat) for cat in CRITICAL_CATEGORIES)]
    has_critical_failure = len(critical_failures) > 0
    
    # æ€»ä½“é€šè¿‡æ ‡å‡†ï¼šå…³é”®æµ‹è¯•0å¤±è´¥ + æˆåŠŸç‡â‰¥95% + æ€§èƒ½è¾¾æ ‡
    passed = (
        not has_critical_failure and
        overall_stats['success_rate'] >= TEST_CONFIG['SUCCESS_RATE_THRESHOLD'] and
        (overall_stats['avg_latency'] <= TEST_CONFIG['AVG_RESPONSE_TIME'] if overall_stats['success_count'] > 0 else False) and
        (overall_stats['p95_latency'] <= TEST_CONFIG['P95_RESPONSE_TIME'] if overall_stats['success_count'] > 0 else False)
    )
    
    if passed:
        report_content += """### âœ… **æµ‹è¯•é€šè¿‡ï¼**

**ç³»ç»Ÿæ€§èƒ½è¾¾åˆ°ä¼ä¸šçº§ç”Ÿäº§æ ‡å‡†ï¼š**
- âœ… å…³é”®æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼ˆDBæŸ¥è¯¢ã€å¹¶å‘ã€ä¸šåŠ¡é€»è¾‘ï¼‰
- âœ… æˆåŠŸç‡è¾¾æ ‡ï¼ˆâ‰¥95%ï¼‰
- âœ… å¹³å‡å“åº”æ—¶é—´è¾¾æ ‡ï¼ˆâ‰¤2ç§’ï¼‰
- âœ… P95å“åº”æ—¶é—´è¾¾æ ‡ï¼ˆâ‰¤4ç§’ï¼‰
- âœ… ç³»ç»Ÿç¨³å®šæ— å´©æºƒ

**ç³»ç»ŸçŠ¶æ€ï¼š** ğŸ‰ **å¯æŠ•å…¥ç”Ÿäº§ç¯å¢ƒä½¿ç”¨**

---

## ğŸš€ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. âœ… ç³»ç»Ÿå·²é€šè¿‡UATé˜¶æ®µ1-5å…¨éƒ¨æµ‹è¯•
2. âœ… å¯æ­£å¼æ ‡è®°ä¸º **Production Liveï¼ˆæ­£å¼ç”Ÿäº§ç¯å¢ƒï¼‰**
3. å»ºè®®æ‰§è¡Œæœ€ç»ˆæ•°æ®åº“è¿ç§»é”å®šï¼š`python db/migrations_v5_1_final.py`ï¼ˆå¦‚å­˜åœ¨ï¼‰
4. é…ç½®ç”Ÿäº§ç¯å¢ƒç›‘æ§ä¸å‘Šè­¦
5. å‡†å¤‡ä¸Šçº¿éƒ¨ç½²

"""
    else:
        report_content += """### âŒ **æµ‹è¯•æœªé€šè¿‡**

**å¤±è´¥åŸå› ï¼š**
"""
        if has_critical_failure:
            report_content += f"\n#### ğŸš¨ å…³é”®æµ‹è¯•å¤±è´¥ï¼ˆ{len(critical_failures)}ä¸ªï¼‰\n\n"
            for failure in critical_failures:
                report_content += f"- âŒ **{failure['test']}**: {failure['error'][:150]}\n"
            report_content += "\n**å…³é”®æµ‹è¯•å¿…é¡»å…¨éƒ¨é€šè¿‡æ‰èƒ½æŠ•å…¥ç”Ÿäº§ï¼**\n\n"
        
        if overall_stats['success_rate'] < TEST_CONFIG['SUCCESS_RATE_THRESHOLD']:
            report_content += f"- âŒ æˆåŠŸç‡ä¸è¾¾æ ‡ï¼ˆ{overall_stats['success_rate']*100:.1f}% < 95%ï¼‰\n"
        if overall_stats['success_count'] > 0 and overall_stats['avg_latency'] > TEST_CONFIG['AVG_RESPONSE_TIME']:
            report_content += f"- âš ï¸ å¹³å‡å“åº”æ—¶é—´åé«˜ï¼ˆ{overall_stats['avg_latency']:.3f}ç§’ > 2ç§’ï¼‰\n"
        if overall_stats['success_count'] > 0 and overall_stats['p95_latency'] > TEST_CONFIG['P95_RESPONSE_TIME']:
            report_content += f"- âš ï¸ P95å“åº”æ—¶é—´åé«˜ï¼ˆ{overall_stats['p95_latency']:.3f}ç§’ > 4ç§’ï¼‰\n"
        
        report_content += """
**å»ºè®®ä¼˜åŒ–æªæ–½ï¼š**
1. ä¿®å¤æ‰€æœ‰å…³é”®æµ‹è¯•å¤±è´¥ï¼ˆDBæŸ¥è¯¢ã€ä¸šåŠ¡é€»è¾‘ï¼‰
2. ä¼˜åŒ–æ•°æ®åº“ç´¢å¼•ï¼ˆé«˜é¢‘æŸ¥è¯¢å­—æ®µï¼‰
3. å¯ç”¨æŸ¥è¯¢ç»“æœç¼“å­˜ï¼ˆRedisï¼‰
4. å®æ–½å¼‚æ­¥å¤„ç†é˜Ÿåˆ—ï¼ˆCelery/RQï¼‰
5. å¢åŠ æ•°æ®åº“è¿æ¥æ± é…ç½®
6. ä¼˜åŒ–æ…¢æŸ¥è¯¢SQLè¯­å¥

"""
    
    report_content += """---

**æµ‹è¯•æ‰§è¡Œè€…ï¼š** UATè‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬  
**æŠ¥å‘Šç‰ˆæœ¬ï¼š** 1.0  
**æµ‹è¯•é…ç½®ï¼š**
- æ€»è¯·æ±‚æ•°: {total_requests}
- å¹¶å‘çº¿ç¨‹: {concurrent_threads}
- è¶…æ—¶æ—¶é—´: {timeout}ç§’
""".format(
        total_requests=TEST_CONFIG['TOTAL_REQUESTS'],
        concurrent_threads=TEST_CONFIG['CONCURRENT_THREADS'],
        timeout=TEST_CONFIG['TIMEOUT']
    )
    
    # å†™å…¥æ–‡ä»¶
    with open("UAT_STAGE5_REPORT.md", "w", encoding="utf-8") as f:
        f.write(report_content)
    
    print(f"\nâœ… æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆï¼šUAT_STAGE5_REPORT.md")
    
    # è¿”å›æµ‹è¯•æ˜¯å¦é€šè¿‡
    return passed

# ========= ä¸»æµ‹è¯•æµç¨‹ =========
def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("=" * 80)
    print("ğŸ§ª UATé˜¶æ®µ5ï¼šæ€§èƒ½ä¸è´Ÿè½½æµ‹è¯•")
    print("=" * 80)
    print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"é…ç½®: {TEST_CONFIG['TOTAL_REQUESTS']}æ¬¡è¯·æ±‚, {TEST_CONFIG['CONCURRENT_THREADS']}å¹¶å‘")
    
    # æ‰§è¡Œæµ‹è¯•
    test_database_query_performance()
    test_concurrent_database_access()
    test_file_operations()
    test_business_logic_performance()
    test_memory_usage()
    
    # ç”ŸæˆæŠ¥å‘Š
    passed = generate_report()
    
    print("\n" + "=" * 80)
    if passed:
        print("ğŸ‰ UATé˜¶æ®µ5æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿå¯æŠ•å…¥ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ã€‚")
    else:
        print("âš ï¸ UATé˜¶æ®µ5æµ‹è¯•æœªå®Œå…¨é€šè¿‡ï¼Œå»ºè®®ä¼˜åŒ–åé‡æ–°æµ‹è¯•ã€‚")
    print("=" * 80)

if __name__ == "__main__":
    main()

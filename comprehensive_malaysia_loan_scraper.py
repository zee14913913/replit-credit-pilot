"""
é©¬æ¥è¥¿äºš68å®¶é‡‘èæœºæ„å…¨é¢è´·æ¬¾äº§å“çˆ¬è™«
æŒ‰CSVæ–‡ä»¶é¡ºåºçˆ¬å–æ‰€æœ‰è´·æ¬¾äº§å“ï¼ˆä¿¡ç”¨å¡ã€æˆ¿è´·ã€ä¸ªäººè´·æ¬¾ã€refinanceã€å€ºåŠ¡æ•´åˆã€è½¦è´·ã€ä¼ä¸šè´·æ¬¾ã€SMEè´·æ¬¾ç­‰ï¼‰
ç”Ÿæˆ12åˆ—ç²¾è‡´è¡¨æ ¼
"""
import sys
sys.path.insert(0, '/home/runner/workspace')

import csv
import logging
import sqlite3
import time
import requests
from typing import List, Dict, Any
from datetime import datetime
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
from concurrent.futures import ThreadPoolExecutor, as_completed

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# æ•°æ®åº“è·¯å¾„
DB_PATH = "/home/runner/loans_comprehensive.db"
CSV_INPUT = "/home/runner/workspace/attached_assets/New é©¬æ¥è¥¿äºšè´·æ¬¾æœºæ„ä¸å¹³å°å…¨å®˜ç½‘_å®Œæ•´ç‰ˆ.csv_1762664297188.csv"
CSV_OUTPUT = "/home/runner/malaysia_loans_complete_table.csv"


class ComprehensiveLoanScraper:
    """å…¨é¢è´·æ¬¾äº§å“çˆ¬è™«"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,ms;q=0.8,zh;q=0.7',
        })
        
        # æ‰€æœ‰è´·æ¬¾äº§å“ç±»å‹å…³é”®è¯
        self.loan_keywords = {
            'credit_card': ['credit card', 'credit-card', 'kad kredit', 'visa', 'mastercard', 'amex'],
            'home_loan': ['home loan', 'housing loan', 'mortgage', 'property loan', 'rumah', 'housing finance'],
            'refinance': ['refinance', 'refinancing', 'loan refinance'],
            'personal_loan': ['personal loan', 'cash loan', 'personal financing', 'pinjaman peribadi'],
            'debt_consolidation': ['debt consolidation', 'consolidation loan', 'debt management'],
            'car_loan': ['car loan', 'auto loan', 'vehicle loan', 'hire purchase', 'kereta'],
            'business_loan': ['business loan', 'business financing', 'commercial loan'],
            'sme_loan': ['sme loan', 'sme financing', 'small business', 'enterprise loan'],
            'other': ['loan', 'financing', 'pinjaman', 'pembiayaan', 'credit']
        }
    
    def classify_loan_type(self, text: str) -> str:
        """æ™ºèƒ½åˆ†ç±»è´·æ¬¾ç±»å‹"""
        text_lower = text.lower()
        
        # æŒ‰ä¼˜å…ˆçº§æ£€æŸ¥
        if any(kw in text_lower for kw in self.loan_keywords['credit_card']):
            return 'CREDIT_CARD'
        elif any(kw in text_lower for kw in self.loan_keywords['home_loan']):
            return 'HOME_LOAN'
        elif any(kw in text_lower for kw in self.loan_keywords['refinance']):
            return 'REFINANCE'
        elif any(kw in text_lower for kw in self.loan_keywords['debt_consolidation']):
            return 'DEBT_CONSOLIDATION'
        elif any(kw in text_lower for kw in self.loan_keywords['car_loan']):
            return 'CAR_LOAN'
        elif any(kw in text_lower for kw in self.loan_keywords['sme_loan']):
            return 'SME_LOAN'
        elif any(kw in text_lower for kw in self.loan_keywords['business_loan']):
            return 'BUSINESS_LOAN'
        elif any(kw in text_lower for kw in self.loan_keywords['personal_loan']):
            return 'PERSONAL_LOAN'
        else:
            return 'OTHER'
    
    def find_all_loan_pages(self, base_url: str, company_name: str) -> List[str]:
        """
        æŸ¥æ‰¾æ‰€æœ‰è´·æ¬¾ç›¸å…³é¡µé¢
        åŒ…æ‹¬ä¿¡ç”¨å¡ã€æˆ¿è´·ã€ä¸ªäººè´·æ¬¾ç­‰æ‰€æœ‰ç±»å‹
        """
        logger.info(f"  ğŸ” æ¢ç´¢ {company_name} çš„æ‰€æœ‰è´·æ¬¾é¡µé¢...")
        
        loan_pages = set()
        
        try:
            # è®¿é—®é¦–é¡µ
            response = self.session.get(base_url, timeout=20, allow_redirects=True)
            
            if response.status_code != 200:
                logger.warning(f"    é¦–é¡µè¿”å› {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
            all_links = soup.find_all('a', href=True)
            
            for link in all_links:
                href = link.get('href', '')
                text = link.get_text(strip=True).lower()
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«è´·æ¬¾å…³é”®è¯
                is_loan_link = False
                for category, keywords in self.loan_keywords.items():
                    if any(kw in text or kw in href.lower() for kw in keywords):
                        is_loan_link = True
                        break
                
                if is_loan_link:
                    full_url = urljoin(base_url, href)
                    if self._is_valid_url(full_url, base_url):
                        loan_pages.add(full_url)
            
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•å¸¸è§è·¯å¾„
            if not loan_pages:
                loan_pages = self._try_common_loan_paths(base_url)
            
            logger.info(f"    æ‰¾åˆ° {len(loan_pages)} ä¸ªè´·æ¬¾é¡µé¢")
            return list(loan_pages)
            
        except Exception as e:
            logger.error(f"    âŒ æ¢ç´¢å¤±è´¥: {e}")
            return []
    
    def _is_valid_url(self, url: str, base_url: str) -> bool:
        """éªŒè¯URL"""
        try:
            parsed_url = urlparse(url)
            parsed_base = urlparse(base_url)
            
            if parsed_url.netloc != parsed_base.netloc:
                return False
            
            invalid = ['login', 'logout', 'signin', 'register', 'mailto:', 'tel:', 'javascript:', '#']
            if any(inv in url.lower() for inv in invalid):
                return False
            
            return True
        except:
            return False
    
    def _try_common_loan_paths(self, base_url: str) -> set:
        """å°è¯•å¸¸è§è·¯å¾„"""
        common_paths = [
            '/personal/loans', '/personal/financing', '/loans', '/financing',
            '/personal/credit-cards', '/cards', '/credit-cards',
            '/home-loans', '/mortgage', '/personal-loans',
            '/business-loans', '/sme', '/business/financing',
            '/products/loans', '/en/loans', '/en/personal/loans'
        ]
        
        valid_urls = set()
        for path in common_paths:
            url = urljoin(base_url, path)
            try:
                response = self.session.head(url, timeout=5, allow_redirects=True)
                if response.status_code == 200:
                    valid_urls.add(url)
            except:
                pass
        
        return valid_urls
    
    def extract_products_from_page(self, url: str, company_name: str) -> List[Dict[str, Any]]:
        """ä»é¡µé¢æå–äº§å“ï¼ˆ12ä¸ªå­—æ®µï¼‰"""
        products = []
        
        try:
            response = self.session.get(url, timeout=20)
            if response.status_code != 200:
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            text = soup.get_text()
            
            # åˆ¤æ–­è´·æ¬¾ç±»å‹
            loan_type = self.classify_loan_type(url + ' ' + text)
            
            # æå–äº§å“åç§°
            product_name = None
            for tag in ['h1', 'h2', 'title']:
                heading = soup.find(tag)
                if heading:
                    product_name = heading.get_text(strip=True)
                    if len(product_name) > 5:
                        break
            
            if not product_name:
                return []
            
            # æå–åˆ©ç‡
            rate = self._extract_rate(text)
            
            # æå–æœŸé™
            tenure = self._extract_tenure(text)
            
            # æå–ç‰¹ç‚¹ï¼ˆä»åˆ—è¡¨é¡¹ï¼‰
            features = []
            feature_sections = soup.find_all(['ul', 'ol'])
            for section in feature_sections[:3]:
                items = section.find_all('li')
                for item in items[:5]:
                    item_text = item.get_text(strip=True)
                    if 10 < len(item_text) < 150:
                        features.append(item_text)
            
            # æŸ¥æ‰¾PDFé“¾æ¥
            app_form = self._find_pdf(soup, ['application', 'apply', 'form'])
            disclosure = self._find_pdf(soup, ['disclosure', 'pds', 'product disclosure'])
            terms = self._find_pdf(soup, ['terms', 'conditions', 'tnc', 't&c'])
            
            # åˆ¤æ–­å®¢æˆ·åå¥½
            customer_pref = self._determine_customer_type(text, product_name)
            
            product = {
                'company': company_name,
                'loan_type': loan_type,
                'product_name': product_name,
                'required_doc': 'è¯·è”ç³»é“¶è¡Œäº†è§£æ‰€éœ€æ–‡ä»¶',
                'features': ' | '.join(features[:5]) if features else 'è¯·è®¿é—®é“¶è¡Œå®˜ç½‘äº†è§£äº§å“ç‰¹ç‚¹',
                'benefits': 'è¯·è®¿é—®é“¶è¡Œå®˜ç½‘äº†è§£äº§å“ä¼˜åŠ¿',
                'fees_charges': 'è¯·è”ç³»é“¶è¡Œäº†è§£è´¹ç”¨è¯¦æƒ…',
                'tenure': tenure,
                'rate': rate,
                'application_form_url': app_form or '',
                'product_disclosure_url': disclosure or '',
                'terms_conditions_url': terms or '',
                'preferred_customer_type': customer_pref,
                'source_url': url,
                'scraped_at': datetime.now().isoformat()
            }
            
            products.append(product)
            
        except Exception as e:
            logger.error(f"    âŒ é¡µé¢æå–å¤±è´¥ {url}: {e}")
        
        return products
    
    def _extract_rate(self, text: str) -> str:
        """æå–åˆ©ç‡"""
        patterns = [
            r'(\d+\.?\d*)\s*%\s*(p\.a\.|per\s+annum)?',
            r'(BR|BLR|SBR)\s*[\+\-]\s*(\d+\.?\d*)\s*%?',
            r'from\s+(\d+\.?\d*)\s*%',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(0)
        
        return 'è¯·è”ç³»é“¶è¡Œ'
    
    def _extract_tenure(self, text: str) -> str:
        """æå–æœŸé™"""
        patterns = [
            r'up\s+to\s+(\d+)\s*(years?|months?)',
            r'(\d+)\s*(years?|tahun)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(0)
        
        return 'è¯·è”ç³»é“¶è¡Œ'
    
    def _find_pdf(self, soup: BeautifulSoup, keywords: List[str]) -> str:
        """æŸ¥æ‰¾PDFé“¾æ¥"""
        links = soup.find_all('a', href=re.compile(r'\.pdf$', re.I))
        
        for link in links:
            text = link.get_text().lower()
            href = link.get('href', '').lower()
            if any(kw in text or kw in href for kw in keywords):
                return link.get('href')
        
        return ''
    
    def _determine_customer_type(self, text: str, product_name: str) -> str:
        """åˆ¤æ–­å®¢æˆ·ç±»å‹"""
        combined = (text + ' ' + product_name).lower()
        
        business_score = sum(1 for kw in ['business', 'sme', 'enterprise', 'self-employed'] if kw in combined)
        salaried_score = sum(1 for kw in ['salaried', 'employee', 'payslip'] if kw in combined)
        
        if business_score > salaried_score:
            return 'ä¼ä¸šå®¢æˆ· (Business/Self-Employed)'
        elif salaried_score > 0:
            return 'æ‰“å·¥æ— (Salaried)'
        else:
            return 'æ‰€æœ‰å®¢æˆ· (All)'
    
    def scrape_institution(self, company_name: str, website: str) -> List[Dict[str, Any]]:
        """çˆ¬å–å•ä¸ªæœºæ„çš„æ‰€æœ‰è´·æ¬¾äº§å“"""
        logger.info(f"ğŸ¦ å¼€å§‹çˆ¬å–: {company_name}")
        
        all_products = []
        
        # æŸ¥æ‰¾æ‰€æœ‰è´·æ¬¾é¡µé¢
        loan_pages = self.find_all_loan_pages(website, company_name)
        
        if not loan_pages:
            logger.warning(f"  âš ï¸ {company_name}: æœªæ‰¾åˆ°è´·æ¬¾é¡µé¢")
            return []
        
        # æå–æ¯ä¸ªé¡µé¢çš„äº§å“
        for page_url in loan_pages[:10]:  # é™åˆ¶æœ€å¤š10ä¸ªé¡µé¢
            logger.info(f"    ğŸ“„ è®¿é—®: {page_url}")
            products = self.extract_products_from_page(page_url, company_name)
            
            if products:
                logger.info(f"      âœ… æ‰¾åˆ° {len(products)} ä¸ªäº§å“")
                all_products.extend(products)
            
            time.sleep(0.5)  # ç¤¼è²Œå»¶è¿Ÿ
        
        logger.info(f"  âœ… {company_name}: å…± {len(all_products)} ä¸ªäº§å“")
        return all_products


def init_database():
    """åˆå§‹åŒ–æ•°æ®åº“"""
    con = sqlite3.connect(DB_PATH)
    cur = con.cursor()
    
    cur.execute("""
        CREATE TABLE IF NOT EXISTS loan_products_complete(
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            company TEXT,
            loan_type TEXT,
            product_name TEXT,
            required_doc TEXT,
            features TEXT,
            benefits TEXT,
            fees_charges TEXT,
            tenure TEXT,
            rate TEXT,
            application_form_url TEXT,
            product_disclosure_url TEXT,
            terms_conditions_url TEXT,
            preferred_customer_type TEXT,
            source_url TEXT,
            scraped_at TEXT
        )
    """)
    
    con.commit()
    con.close()
    logger.info("âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")


def load_institutions_from_csv(csv_path: str) -> List[Dict[str, str]]:
    """ä»CSVåŠ è½½æœºæ„åˆ—è¡¨"""
    institutions = []
    
    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        next(reader)  # è·³è¿‡æ ‡é¢˜è¡Œ
        
        for row in reader:
            if len(row) >= 2:
                institutions.append({
                    'name': row[0],
                    'website': row[1]
                })
    
    logger.info(f"ğŸ“‹ åŠ è½½äº† {len(institutions)} å®¶é‡‘èæœºæ„")
    return institutions


def save_to_database(products: List[Dict[str, Any]]):
    """ä¿å­˜åˆ°æ•°æ®åº“"""
    if not products:
        return
    
    con = sqlite3.connect(DB_PATH)
    cur = con.cursor()
    
    insert_sql = """
        INSERT INTO loan_products_complete(
            company, loan_type, product_name, required_doc, features, benefits,
            fees_charges, tenure, rate, application_form_url, product_disclosure_url,
            terms_conditions_url, preferred_customer_type, source_url, scraped_at
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    """
    
    items = [
        (
            p['company'], p['loan_type'], p['product_name'], p['required_doc'],
            p['features'], p['benefits'], p['fees_charges'], p['tenure'],
            p['rate'], p.get('application_form_url', ''), p.get('product_disclosure_url', ''),
            p.get('terms_conditions_url', ''), p['preferred_customer_type'],
            p.get('source_url', ''), p.get('scraped_at', '')
        )
        for p in products
    ]
    
    cur.executemany(insert_sql, items)
    con.commit()
    con.close()


def export_to_csv_table(output_path: str):
    """å¯¼å‡ºä¸ºç²¾è‡´è¡¨æ ¼ï¼ˆ12åˆ—ï¼‰"""
    con = sqlite3.connect(DB_PATH)
    con.row_factory = sqlite3.Row
    cur = con.cursor()
    
    cur.execute("""
        SELECT 
            company AS 'COMPANY',
            loan_type AS 'LOAN TYPE',
            required_doc AS 'REQUIRED DOC',
            features AS 'FEATURES',
            benefits AS 'BENEFITS',
            fees_charges AS 'FEES & CHARGES',
            tenure AS 'TENURE',
            rate AS 'RATE',
            application_form_url AS 'APPLICATION FORM',
            product_disclosure_url AS 'PRODUCT DISCLOSURE',
            terms_conditions_url AS 'TERMS & CONDITIONS',
            preferred_customer_type AS 'å®¢æˆ·åå¥½'
        FROM loan_products_complete
        ORDER BY company, loan_type
    """)
    
    rows = cur.fetchall()
    con.close()
    
    # å†™å…¥CSV
    with open(output_path, 'w', newline='', encoding='utf-8-sig') as f:
        if rows:
            writer = csv.DictWriter(f, fieldnames=rows[0].keys())
            writer.writeheader()
            for row in rows:
                writer.writerow(dict(row))
    
    logger.info(f"ğŸ“¤ è¡¨æ ¼å·²å¯¼å‡º: {output_path}")
    logger.info(f"   æ€»è®¡: {len(rows)} ä¸ªäº§å“")


def main():
    """ä¸»æµç¨‹"""
    logger.info("")
    logger.info("=" * 100)
    logger.info("ğŸš€ é©¬æ¥è¥¿äºš68å®¶é‡‘èæœºæ„å…¨é¢è´·æ¬¾äº§å“çˆ¬è™«")
    logger.info("=" * 100)
    logger.info("")
    
    start_time = datetime.now()
    
    # åˆå§‹åŒ–
    init_database()
    
    # åŠ è½½æœºæ„åˆ—è¡¨
    institutions = load_institutions_from_csv(CSV_INPUT)
    
    # åˆ›å»ºçˆ¬è™«
    scraper = ComprehensiveLoanScraper()
    
    # é€ä¸ªçˆ¬å–ï¼ˆæŒ‰CSVé¡ºåºï¼‰
    all_products = []
    
    for idx, inst in enumerate(institutions, 1):
        logger.info(f"\nè¿›åº¦: {idx}/{len(institutions)}")
        logger.info("-" * 100)
        
        try:
            products = scraper.scrape_institution(inst['name'], inst['website'])
            if products:
                all_products.extend(products)
                # å³æ—¶ä¿å­˜
                save_to_database(products)
        except Exception as e:
            logger.error(f"âŒ {inst['name']} çˆ¬å–å¤±è´¥: {e}")
        
        time.sleep(1)  # æœºæ„é—´å»¶è¿Ÿ
    
    # å¯¼å‡ºè¡¨æ ¼
    logger.info("")
    logger.info("=" * 100)
    logger.info("ğŸ“Š å¯¼å‡ºç²¾è‡´è¡¨æ ¼ï¼ˆ12åˆ—ï¼‰")
    logger.info("=" * 100)
    export_to_csv_table(CSV_OUTPUT)
    
    # ç»Ÿè®¡
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    logger.info("")
    logger.info("=" * 100)
    logger.info("ğŸ‰ çˆ¬å–å®Œæˆï¼")
    logger.info("=" * 100)
    logger.info(f"æ€»è€—æ—¶: {duration:.1f} ç§’ ({duration/60:.1f} åˆ†é’Ÿ)")
    logger.info(f"å¤„ç†æœºæ„: {len(institutions)} å®¶")
    logger.info(f"äº§å“æ€»æ•°: {len(all_products)}")
    logger.info(f"æ•°æ®åº“: {DB_PATH}")
    logger.info(f"è¡¨æ ¼æ–‡ä»¶: {CSV_OUTPUT}")
    logger.info("")


if __name__ == '__main__':
    main()

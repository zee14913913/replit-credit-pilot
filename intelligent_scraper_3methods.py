#!/usr/bin/env python
"""
æ™ºèƒ½çˆ¬è™« - ç»“åˆ3ç§æ–¹æ³•100%æå–å®Œæ•´äº§å“è¯¦æƒ…
æ–¹æ³•1: Footerå¯¼èˆª â†’ Personal/Business/SME/Corporate â†’ äº§å“ç±»åˆ« â†’ Learn More/Apply
æ–¹æ³•2: SearchåŠŸèƒ½ â†’ æœç´¢å…³é”®è¯ â†’ äº§å“ç±»åˆ« â†’ Learn More/Apply
æ–¹æ³•3: URLæ‹¼æŽ¥ â†’ ç›´æŽ¥è®¿é—® â†’ äº§å“ç±»åˆ« â†’ Learn More/Apply
"""
import csv
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import psycopg2
from psycopg2.extras import execute_values
import os
from datetime import datetime
import re

DATABASE_URL = os.getenv('DATABASE_URL')
CSV_INPUT = "/home/runner/workspace/attached_assets/New é©¬æ¥è¥¿äºšè´·æ¬¾æœºæž„ä¸Žå¹³å°å…¨å®˜ç½‘_å®Œæ•´ç‰ˆ.csv_1762667764316.csv"

# ä¸šåŠ¡åˆ†ç±»
BUSINESS_CATEGORIES = ['personal', 'business', 'sme', 'corporate']

# äº§å“ç±»åž‹è·¯å¾„
PRODUCT_PATHS = {
    'credit_card': ['credit-card', 'cards', 'credit-cards', 'card'],
    'loan': ['loan', 'loans', 'lending', 'borrowing'],
    'financing': ['financing', 'finance'],
    'mortgage': ['mortgage', 'home-loan', 'housing-loan', 'property-loan', 'home-financing'],
    'fixed_deposit': ['fixed-deposit', 'fd', 'deposit', 'deposits', 'time-deposit'],
    'overdraft': ['overdraft', 'od'],
    'banking': ['banking', 'products']
}

def load_institutions_in_order():
    """åŠ è½½æ‰€æœ‰æœºæž„ï¼ˆä¸¥æ ¼CSVé¡ºåºï¼‰"""
    institutions = []
    with open(CSV_INPUT, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        next(reader)
        for idx, row in enumerate(reader, 1):
            if len(row) >= 2:
                institutions.append({
                    'order': idx,
                    'name': row[0].strip(),
                    'website': row[1].strip()
                })
    return institutions

def get_completed_companies():
    """èŽ·å–å·²å®Œæˆçš„å…¬å¸åˆ—è¡¨"""
    con = psycopg2.connect(DATABASE_URL)
    cur = con.cursor()
    cur.execute("SELECT DISTINCT company FROM loan_products_ultimate")
    completed = set(row[0] for row in cur.fetchall())
    cur.close()
    con.close()
    return completed

def classify_loan_type(text):
    """æ™ºèƒ½åˆ†ç±»äº§å“ç±»åž‹"""
    text_lower = text.lower()
    
    if any(kw in text_lower for kw in ['credit card', 'debit card', 'kad kredit', 'visa', 'mastercard', 'amex']):
        return 'CREDIT_CARD'
    elif any(kw in text_lower for kw in ['home loan', 'housing', 'mortgage', 'property']):
        return 'HOME_LOAN'
    elif any(kw in text_lower for kw in ['personal loan', 'cash loan', 'personal financing']):
        return 'PERSONAL_LOAN'
    elif any(kw in text_lower for kw in ['car loan', 'auto', 'vehicle', 'hire purchase']):
        return 'CAR_LOAN'
    elif any(kw in text_lower for kw in ['business', 'sme', 'commercial']):
        return 'SME_LOAN'
    elif any(kw in text_lower for kw in ['fixed deposit', 'fd', 'time deposit']):
        return 'FIXED_DEPOSIT'
    elif any(kw in text_lower for kw in ['overdraft', 'od']):
        return 'OVERDRAFT'
    elif any(kw in text_lower for kw in ['refinanc', 'debt consolidation']):
        return 'REFINANCE'
    else:
        return 'OTHER'

def extract_detailed_fields(soup, url):
    """æå–è¯¦ç»†çš„12å­—æ®µä¿¡æ¯"""
    text = soup.get_text()
    
    # æå–åˆ©çŽ‡
    rate = 'è¯·è”ç³»é“¶è¡Œ'
    rate_patterns = [
        r'(\d+\.?\d*)\s*%\s*p\.?a\.?',
        r'rate[:\s]+(\d+\.?\d*)\s*%',
        r'interest[:\s]+(\d+\.?\d*)\s*%',
        r'(\d+\.?\d*)\s*%'
    ]
    for pattern in rate_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            rate = match.group(0)
            break
    
    # æå–æœŸé™
    tenure = 'è¯·è”ç³»é“¶è¡Œ'
    tenure_patterns = [
        r'(\d+)\s*(?:year|tahun|month|bulan)',
        r'tenure[:\s]+(\d+)',
        r'term[:\s]+(\d+)'
    ]
    for pattern in tenure_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            tenure = match.group(0)
            break
    
    # æå–ç‰¹ç‚¹ï¼ˆFeaturesï¼‰
    features = 'è¯·è®¿é—®é“¶è¡Œå®˜ç½‘'
    features_section = soup.find(['div', 'section'], text=re.compile(r'feature|benefit|highlight', re.I))
    if features_section:
        features_list = features_section.find_all(['li', 'p'])
        if features_list:
            features = ' | '.join([li.get_text(strip=True)[:100] for li in features_list[:3]])
    
    # æå–è´¹ç”¨
    fees = 'è¯·è”ç³»é“¶è¡Œ'
    if re.search(r'annual fee|fee|charge', text, re.I):
        fee_match = re.search(r'RM\s*\d+', text)
        if fee_match:
            fees = fee_match.group(0)
    
    # æŸ¥æ‰¾ç”³è¯·è¡¨é“¾æŽ¥
    application_url = ''
    apply_link = soup.find('a', text=re.compile(r'apply|application', re.I))
    if apply_link and apply_link.get('href'):
        application_url = urljoin(url, apply_link.get('href'))
    
    # æŸ¥æ‰¾äº§å“æŠ«éœ²é“¾æŽ¥
    disclosure_url = ''
    disclosure_link = soup.find('a', text=re.compile(r'disclosure|product.*sheet|brochure', re.I))
    if disclosure_link and disclosure_link.get('href'):
        disclosure_url = urljoin(url, disclosure_link.get('href'))
    
    # æŸ¥æ‰¾æ¡æ¬¾é“¾æŽ¥
    terms_url = ''
    terms_link = soup.find('a', text=re.compile(r'term|condition|t&c|tnc', re.I))
    if terms_link and terms_link.get('href'):
        terms_url = urljoin(url, terms_link.get('href'))
    
    return {
        'rate': rate,
        'tenure': tenure,
        'features': features,
        'fees_charges': fees,
        'application_form_url': application_url,
        'product_disclosure_url': disclosure_url,
        'terms_conditions_url': terms_url
    }

def method1_footer_navigation(session, base_url):
    """æ–¹æ³•1: ä»ŽFooterå¯¼èˆªæŸ¥æ‰¾äº§å“"""
    print("   ðŸ“ æ–¹æ³•1: Footerå¯¼èˆª")
    products = []
    
    try:
        response = session.get(base_url, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æŸ¥æ‰¾Footer
        footer = soup.find(['footer', 'div'], class_=lambda x: x and 'footer' in str(x).lower())
        if not footer:
            footer = soup.find_all(['footer', 'div'])[-1] if soup.find_all(['footer', 'div']) else soup
        
        # åœ¨Footerä¸­æŸ¥æ‰¾ä¸šåŠ¡åˆ†ç±»é“¾æŽ¥
        for category in BUSINESS_CATEGORIES:
            links = footer.find_all('a', text=re.compile(category, re.I))
            for link in links[:2]:  # æ¯ä¸ªåˆ†ç±»æœ€å¤š2ä¸ªé“¾æŽ¥
                if link.get('href'):
                    category_url = urljoin(base_url, link.get('href'))
                    print(f"      â†’ {category.upper()}: {category_url}")
                    
                    # è®¿é—®åˆ†ç±»é¡µé¢
                    try:
                        cat_response = session.get(category_url, timeout=10)
                        cat_soup = BeautifulSoup(cat_response.text, 'html.parser')
                        
                        # æŸ¥æ‰¾äº§å“ç±»åž‹é“¾æŽ¥
                        for product_type, keywords in PRODUCT_PATHS.items():
                            for keyword in keywords:
                                product_links = cat_soup.find_all('a', href=re.compile(keyword, re.I))
                                for prod_link in product_links[:5]:  # æ¯ç§äº§å“ç±»åž‹æœ€å¤š5ä¸ª
                                    if prod_link.get('href'):
                                        product_url = urljoin(category_url, prod_link.get('href'))
                                        products.append(product_url)
                        
                        time.sleep(0.3)
                    except:
                        pass
    except:
        pass
    
    return list(set(products))

def method2_search_function(session, base_url):
    """æ–¹æ³•2: ä½¿ç”¨SearchåŠŸèƒ½"""
    print("   ðŸ” æ–¹æ³•2: SearchåŠŸèƒ½")
    products = []
    
    try:
        response = session.get(base_url, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æŸ¥æ‰¾æœç´¢æ¡†
        search_input = soup.find(['input'], {'type': 'search'}) or soup.find(['input'], {'name': re.compile('search', re.I)})
        
        # æŸ¥æ‰¾æœç´¢é“¾æŽ¥
        search_links = soup.find_all('a', text=re.compile(r'search', re.I)) + soup.find_all('a', href=re.compile(r'search', re.I))
        
        for search_link in search_links[:1]:
            if search_link.get('href'):
                search_url = urljoin(base_url, search_link.get('href'))
                print(f"      â†’ Search URL: {search_url}")
                products.append(search_url)
    except:
        pass
    
    return products

def method3_url_append(base_url):
    """æ–¹æ³•3: URLç›´æŽ¥æ‹¼æŽ¥"""
    print("   ðŸ”— æ–¹æ³•3: URLæ‹¼æŽ¥")
    products = []
    
    # ç»„åˆ: /category/product
    for category in BUSINESS_CATEGORIES:
        for product_type, keywords in PRODUCT_PATHS.items():
            for keyword in keywords[:2]:  # æ¯ç§äº§å“æœ€å¤š2ä¸ªå…³é”®è¯
                # /personal/credit-cards
                url1 = urljoin(base_url, f"/{category}/{keyword}")
                products.append(url1)
                
                # /personal/cards
                url2 = urljoin(base_url, f"/{category}/{keyword}s" if not keyword.endswith('s') else keyword)
                products.append(url2)
    
    return list(set(products))

def extract_product_from_detail_page(session, url, company_name):
    """ä»Žäº§å“è¯¦æƒ…é¡µæå–å®Œæ•´ä¿¡æ¯"""
    try:
        response = session.get(url, timeout=10)
        if response.status_code != 200:
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        text = soup.get_text()
        
        # æŸ¥æ‰¾äº§å“åç§°
        product_name = None
        for tag in ['h1', 'h2', 'h3']:
            heading = soup.find(tag)
            if heading:
                product_name = heading.get_text(strip=True)
                if len(product_name) > 3 and len(product_name) < 150:
                    break
        
        if not product_name:
            return None
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºäº§å“é¡µï¼ˆå¿…é¡»åŒ…å«äº§å“å…³é”®è¯ï¼‰
        if not any(kw in text.lower() for kw in ['card', 'loan', 'financing', 'deposit', 'overdraft']):
            return None
        
        # æå–è¯¦ç»†å­—æ®µ
        details = extract_detailed_fields(soup, url)
        loan_type = classify_loan_type(product_name + ' ' + url)
        
        return {
            'company': company_name,
            'product_name': product_name,
            'loan_type': loan_type,
            'rate': details['rate'],
            'tenure': details['tenure'],
            'features': details['features'],
            'fees_charges': details['fees_charges'],
            'application_form_url': details['application_form_url'],
            'product_disclosure_url': details['product_disclosure_url'],
            'terms_conditions_url': details['terms_conditions_url'],
            'required_doc': 'è¯·è®¿é—®é“¶è¡Œå®˜ç½‘',
            'benefits': 'è¯·è®¿é—®é“¶è¡Œå®˜ç½‘',
            'preferred_customer_type': 'æ‰€æœ‰å®¢æˆ·',
            'source_url': url
        }
    except:
        return None

def scrape_company_with_3methods(order, company_name, base_url):
    """ä½¿ç”¨3ç§æ–¹æ³•ç»¼åˆçˆ¬å–å•ä¸ªå…¬å¸"""
    print(f"\n{'='*80}")
    print(f"[{order}/68] ðŸ¦ {company_name}")
    print(f"{'='*80}")
    
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0'
    })
    
    all_product_urls = []
    
    # æ–¹æ³•1: Footerå¯¼èˆª
    try:
        urls1 = method1_footer_navigation(session, base_url)
        all_product_urls.extend(urls1)
        print(f"      æ–¹æ³•1æ‰¾åˆ°: {len(urls1)} ä¸ªé“¾æŽ¥")
    except Exception as e:
        print(f"      æ–¹æ³•1å¤±è´¥: {str(e)[:50]}")
    
    # æ–¹æ³•2: SearchåŠŸèƒ½
    try:
        urls2 = method2_search_function(session, base_url)
        all_product_urls.extend(urls2)
        print(f"      æ–¹æ³•2æ‰¾åˆ°: {len(urls2)} ä¸ªé“¾æŽ¥")
    except Exception as e:
        print(f"      æ–¹æ³•2å¤±è´¥: {str(e)[:50]}")
    
    # æ–¹æ³•3: URLæ‹¼æŽ¥
    try:
        urls3 = method3_url_append(base_url)
        all_product_urls.extend(urls3)
        print(f"      æ–¹æ³•3æ‰¾åˆ°: {len(urls3)} ä¸ªé“¾æŽ¥")
    except Exception as e:
        print(f"      æ–¹æ³•3å¤±è´¥: {str(e)[:50]}")
    
    # åŽ»é‡
    all_product_urls = list(set(all_product_urls))
    print(f"   ðŸ“¦ æ€»é“¾æŽ¥æ•°: {len(all_product_urls)}")
    
    # è®¿é—®æ¯ä¸ªé“¾æŽ¥å¹¶æå–äº§å“è¯¦æƒ…
    products = []
    for idx, url in enumerate(all_product_urls[:100], 1):  # é™åˆ¶æ¯å®¶å…¬å¸100ä¸ªé“¾æŽ¥
        product = extract_product_from_detail_page(session, url, company_name)
        if product:
            products.append(product)
            if idx % 10 == 0:
                print(f"      å·²å¤„ç†: {idx}/{min(len(all_product_urls), 100)}, æ‰¾åˆ°: {len(products)} ä¸ªäº§å“")
        time.sleep(0.2)
    
    # åŽ»é‡
    seen = set()
    unique_products = []
    for p in products:
        key = f"{p['company']}_{p['product_name']}"
        if key not in seen:
            seen.add(key)
            unique_products.append(p)
    
    print(f"   âœ… æœ€ç»ˆäº§å“æ•°: {len(unique_products)}")
    return unique_products

def save_to_db(products):
    """ä¿å­˜åˆ°æ•°æ®åº“"""
    if not products:
        return
    
    con = psycopg2.connect(DATABASE_URL)
    cur = con.cursor()
    
    sql = """
        INSERT INTO loan_products_ultimate(
            company, loan_type, product_name, required_doc, features, benefits,
            fees_charges, tenure, rate, application_form_url, product_disclosure_url,
            terms_conditions_url, preferred_customer_type, source_url, scraped_at
        ) VALUES %s
        ON CONFLICT (company, product_name) DO UPDATE SET
            loan_type = EXCLUDED.loan_type,
            rate = EXCLUDED.rate,
            tenure = EXCLUDED.tenure,
            features = EXCLUDED.features,
            fees_charges = EXCLUDED.fees_charges,
            application_form_url = EXCLUDED.application_form_url,
            product_disclosure_url = EXCLUDED.product_disclosure_url,
            terms_conditions_url = EXCLUDED.terms_conditions_url,
            source_url = EXCLUDED.source_url,
            scraped_at = EXCLUDED.scraped_at
    """
    
    items = [(
        p['company'], p['loan_type'], p['product_name'], p['required_doc'],
        p['features'], p['benefits'], p['fees_charges'], p['tenure'], p['rate'],
        p['application_form_url'], p['product_disclosure_url'], p['terms_conditions_url'],
        p['preferred_customer_type'], p['source_url'], datetime.now()
    ) for p in products]
    
    execute_values(cur, sql, items)
    con.commit()
    cur.close()
    con.close()

def main():
    print("=" * 80)
    print("æ™ºèƒ½çˆ¬è™« - 3ç§æ–¹æ³•ç»¼åˆç­–ç•¥")
    print("ä»Žç¬¬9å®¶å…¬å¸ç»§ç»­ - ä¿ç•™å·²æœ‰750ä¸ªäº§å“")
    print("=" * 80)
    
    # èŽ·å–å·²å®Œæˆçš„å…¬å¸
    completed = get_completed_companies()
    print(f"\nâœ… å·²å®Œæˆ: {len(completed)} å®¶å…¬å¸")
    for c in sorted(completed):
        print(f"   - {c}")
    
    # åŠ è½½æ‰€æœ‰æœºæž„
    institutions = load_institutions_in_order()
    print(f"\nðŸ“‹ æ€»æœºæž„æ•°: {len(institutions)}")
    
    # è¿‡æ»¤å‡ºæœªå®Œæˆçš„
    remaining = [inst for inst in institutions if inst['name'] not in completed]
    print(f"ðŸ“Œ å‰©ä½™: {len(remaining)} å®¶æœºæž„\n")
    
    total_new_products = 0
    
    # æŒ‰é¡ºåºçˆ¬å–å‰©ä½™çš„
    for inst in remaining:
        try:
            products = scrape_company_with_3methods(inst['order'], inst['name'], inst['website'])
            if products:
                total_new_products += len(products)
                save_to_db(products)
                print(f"   ðŸ’¾ å·²ä¿å­˜ {len(products)} ä¸ªæ–°äº§å“")
            else:
                print(f"   âš ï¸  æœªæ‰¾åˆ°äº§å“")
        except Exception as e:
            print(f"   âŒ é”™è¯¯: {str(e)[:80]}")
        
        time.sleep(2)
    
    print("\n" + "=" * 80)
    print(f"ðŸŽ‰ å®Œæˆï¼æ–°å¢ž: {total_new_products} ä¸ªäº§å“")
    print("=" * 80)
    
    # æœ€ç»ˆç»Ÿè®¡
    con = psycopg2.connect(DATABASE_URL)
    cur = con.cursor()
    
    cur.execute("SELECT COUNT(*) FROM loan_products_ultimate")
    total = cur.fetchone()[0]
    
    cur.execute("SELECT COUNT(DISTINCT company) FROM loan_products_ultimate")
    companies = cur.fetchone()[0]
    
    print(f"\nðŸ“Š æ•°æ®åº“æ€»ç»Ÿè®¡:")
    print(f"   æ€»äº§å“æ•°: {total}")
    print(f"   å…¬å¸æ•°é‡: {companies}/68")
    
    cur.close()
    con.close()

if __name__ == '__main__':
    main()

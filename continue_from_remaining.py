#!/usr/bin/env python
"""
ä»å‰©ä½™æœºæ„ç»§ç»­çˆ¬å– - ä½¿ç”¨3ç§æ™ºèƒ½æ–¹æ³•
å·²å®Œæˆ: 8å®¶ (750ä¸ªäº§å“)
å‰©ä½™: 59å®¶æœºæ„
"""
import csv
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import psycopg2
from psycopg2.extras import execute_values
import os
from datetime import datetime
import re

DATABASE_URL = os.getenv('DATABASE_URL')
CSV_INPUT = "/home/runner/workspace/attached_assets/New é©¬æ¥è¥¿äºšè´·æ¬¾æœºæ„ä¸å¹³å°å…¨å®˜ç½‘_å®Œæ•´ç‰ˆ.csv_1762667764316.csv"

# äº§å“åˆ†ç±»å…³é”®è¯
BUSINESS_CATS = ['personal', 'business', 'sme', 'corporate']
PRODUCT_TYPES = ['credit-card', 'cards', 'loan', 'loans', 'financing', 'mortgage', 
                 'home-loan', 'housing-loan', 'fixed-deposit', 'fd', 'deposit', 'overdraft', 'od']

def get_completed_companies():
    """è·å–å·²å®Œæˆçš„å…¬å¸"""
    con = psycopg2.connect(DATABASE_URL)
    cur = con.cursor()
    cur.execute("SELECT DISTINCT company FROM loan_products_ultimate")
    completed = set(row[0] for row in cur.fetchall())
    cur.close()
    con.close()
    return completed

def load_all_institutions():
    """åŠ è½½æ‰€æœ‰æœºæ„"""
    institutions = []
    with open(CSV_INPUT, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        next(reader)  # Skip header
        for idx, row in enumerate(reader, 1):
            if len(row) >= 2:
                institutions.append({
                    'order': idx,
                    'name': row[0].strip(),
                    'website': row[1].strip()
                })
    return institutions

def classify_product(text):
    """åˆ†ç±»äº§å“"""
    text_lower = text.lower()
    if any(kw in text_lower for kw in ['credit card', 'kad kredit', 'visa', 'mastercard']):
        return 'CREDIT_CARD'
    elif any(kw in text_lower for kw in ['home', 'housing', 'mortgage', 'property']):
        return 'HOME_LOAN'
    elif any(kw in text_lower for kw in ['personal loan', 'cash loan']):
        return 'PERSONAL_LOAN'
    elif any(kw in text_lower for kw in ['car', 'auto', 'vehicle']):
        return 'CAR_LOAN'
    elif any(kw in text_lower for kw in ['business', 'sme', 'commercial']):
        return 'SME_LOAN'
    elif any(kw in text_lower for kw in ['fixed deposit', 'fd', 'time deposit']):
        return 'FIXED_DEPOSIT'
    elif any(kw in text_lower for kw in ['overdraft', 'od']):
        return 'OVERDRAFT'
    else:
        return 'OTHER'

def extract_rate(text):
    """æå–åˆ©ç‡"""
    match = re.search(r'(\d+\.?\d*)\s*%', text)
    return match.group(0) if match else 'è¯·è”ç³»é“¶è¡Œ'

def scrape_single_company(order, name, website):
    """çˆ¬å–å•ä¸ªå…¬å¸ - ç®€åŒ–ç‰ˆ"""
    print(f"\n[{order}/67] ğŸ¦ {name}")
    print(f"   {website}")
    
    session = requests.Session()
    session.headers.update({'User-Agent': 'Mozilla/5.0'})
    
    products = []
    visited = set()
    
    # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„URL (æ–¹æ³•3: URLæ‹¼æ¥)
    urls_to_try = []
    
    # æ ¹URL
    urls_to_try.append(website)
    
    # /personal, /business, /sme, /corporate
    for cat in BUSINESS_CATS:
        urls_to_try.append(urljoin(website, f'/{cat}'))
        urls_to_try.append(urljoin(website, f'/en/{cat}'))
        
        # /personal/cards, /personal/loans, etc.
        for prod in PRODUCT_TYPES:
            urls_to_try.append(urljoin(website, f'/{cat}/{prod}'))
            urls_to_try.append(urljoin(website, f'/en/{cat}/{prod}'))
    
    # ç›´æ¥äº§å“URL
    for prod in PRODUCT_TYPES:
        urls_to_try.append(urljoin(website, f'/{prod}'))
        urls_to_try.append(urljoin(website, f'/en/{prod}'))
    
    # å»é‡
    urls_to_try = list(set(urls_to_try))
    print(f"   å°è¯• {len(urls_to_try)} ä¸ªURL")
    
    # è®¿é—®æ¯ä¸ªURL
    for idx, url in enumerate(urls_to_try[:80], 1):  # é™åˆ¶80ä¸ª
        if url in visited:
            continue
        visited.add(url)
        
        try:
            response = session.get(url, timeout=8)
            if response.status_code != 200:
                continue
            
            soup = BeautifulSoup(response.text, 'html.parser')
            text = soup.get_text()
            
            # æå–æ‰€æœ‰æ ‡é¢˜
            for tag in ['h1', 'h2', 'h3', 'h4']:
                for heading in soup.find_all(tag):
                    h_text = heading.get_text(strip=True)
                    
                    # å¿…é¡»åŒ…å«äº§å“å…³é”®è¯
                    if 5 < len(h_text) < 150 and any(kw in h_text.lower() for kw in [
                        'card', 'loan', 'financing', 'deposit', 'overdraft', 'mortgage'
                    ]):
                        products.append({
                            'company': name,
                            'product_name': h_text,
                            'loan_type': classify_product(h_text),
                            'rate': extract_rate(text[:3000]),
                            'source_url': url
                        })
            
            if idx % 20 == 0:
                print(f"   è¿›åº¦: {idx}/{min(len(urls_to_try), 80)}, æ‰¾åˆ°: {len(products)}")
            
            time.sleep(0.2)
            
        except:
            pass
    
    # å»é‡
    seen = set()
    unique = []
    for p in products:
        key = f"{p['company']}_{p['product_name']}"
        if key not in seen:
            seen.add(key)
            unique.append(p)
    
    print(f"   âœ… æ‰¾åˆ° {len(unique)} ä¸ªäº§å“")
    return unique

def save_products(products):
    """ä¿å­˜äº§å“"""
    if not products:
        return
    
    con = psycopg2.connect(DATABASE_URL)
    cur = con.cursor()
    
    sql = """
        INSERT INTO loan_products_ultimate(
            company, loan_type, product_name, required_doc, features, benefits,
            fees_charges, tenure, rate, application_form_url, product_disclosure_url,
            terms_conditions_url, preferred_customer_type, source_url, scraped_at
        ) VALUES %s
        ON CONFLICT (company, product_name) DO NOTHING
    """
    
    items = [(
        p['company'], p['loan_type'], p['product_name'], 'è¯·è®¿é—®é“¶è¡Œå®˜ç½‘',
        'è¯·è®¿é—®é“¶è¡Œå®˜ç½‘', 'è¯·è®¿é—®é“¶è¡Œå®˜ç½‘', 'è¯·è”ç³»é“¶è¡Œ', 'è¯·è”ç³»é“¶è¡Œ',
        p['rate'], '', '', '', 'æ‰€æœ‰å®¢æˆ·', p['source_url'], datetime.now()
    ) for p in products]
    
    execute_values(cur, sql, items)
    con.commit()
    cur.close()
    con.close()

def main():
    print("=" * 80)
    print("ç»§ç»­çˆ¬å– - ä»å‰©ä½™æœºæ„å¼€å§‹")
    print("=" * 80)
    
    # è·å–å·²å®Œæˆå’Œæ‰€æœ‰æœºæ„
    completed = get_completed_companies()
    all_inst = load_all_institutions()
    
    print(f"\nâœ… å·²å®Œæˆ: {len(completed)} å®¶")
    print(f"ğŸ“‹ æ€»æœºæ„: {len(all_inst)} å®¶")
    
    # è¿‡æ»¤å‰©ä½™
    remaining = [inst for inst in all_inst if inst['name'] not in completed]
    print(f"ğŸ“Œ å‰©ä½™: {len(remaining)} å®¶\n")
    
    total_new = 0
    
    # é€ä¸ªçˆ¬å–
    for inst in remaining:
        try:
            products = scrape_single_company(inst['order'], inst['name'], inst['website'])
            if products:
                save_products(products)
                total_new += len(products)
                print(f"   ğŸ’¾ å·²ä¿å­˜")
            else:
                print(f"   âš ï¸  æ— äº§å“")
        except Exception as e:
            print(f"   âŒ é”™è¯¯: {str(e)[:50]}")
        
        time.sleep(1)
    
    print("\n" + "=" * 80)
    print(f"ğŸ‰ å®Œæˆï¼æ–°å¢ {total_new} ä¸ªäº§å“")
    
    # æœ€ç»ˆç»Ÿè®¡
    con = psycopg2.connect(DATABASE_URL)
    cur = con.cursor()
    cur.execute("SELECT COUNT(*), COUNT(DISTINCT company) FROM loan_products_ultimate")
    result = cur.fetchone()
    if result:
        total, companies = result
        print(f"ğŸ“Š æ•°æ®åº“: {total} ä¸ªäº§å“ï¼Œ{companies} å®¶å…¬å¸")
    cur.close()
    con.close()

if __name__ == '__main__':
    main()

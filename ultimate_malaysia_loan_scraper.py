"""
ğŸš€ CreditPilot - é©¬æ¥è¥¿äºš68å®¶é‡‘èæœºæ„æ·±åº¦çˆ¬è™«ç³»ç»Ÿ (Ultimate Edition)
ä¸‰å±‚æ¶æ„è®¾è®¡ï¼š
- Layer 0: CSVé©±åŠ¨çš„orchestratorï¼ˆæŒ‰é¡ºåºå¤„ç†68å®¶æœºæ„ï¼‰
- Layer 1: æ— é™åˆ¶é“¾æ¥æ¢ç´¢ + è‡ªé€‚åº”åˆ†é¡µå¤„ç†
- Layer 2: äº§å“è¯¦æƒ…é¡µæ·±åº¦æå–ï¼ˆ12ä¸ªå­—æ®µï¼‰
- Layer 3: æ•°æ®éªŒè¯å’Œè´¨é‡ä¿è¯

ç›®æ ‡ï¼š3000-5000ä¸ªäº§å“ï¼Œ100%å‡†ç¡®æ€§
"""
import sys
sys.path.insert(0, '/home/runner/workspace')

import csv
import logging
import time
import requests
from typing import List, Dict, Any, Set, Optional
from datetime import datetime
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs, urlencode, urlunparse
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict
import hashlib
import os
import psycopg2
from psycopg2.extras import execute_values

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# è·¯å¾„é…ç½®
DATABASE_URL = os.getenv('DATABASE_URL')
CSV_INPUT = "/home/runner/workspace/attached_assets/New é©¬æ¥è¥¿äºšè´·æ¬¾æœºæ„ä¸å¹³å°å…¨å®˜ç½‘_å®Œæ•´ç‰ˆ.csv_1762667764316.csv"
CSV_OUTPUT = "/home/runner/malaysia_loans_ultimate_complete.csv"

# æ¯”ä»·å¹³å°åˆ—è¡¨ï¼ˆéœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
COMPARISON_PLATFORMS = [
    'imoney', 'ringgitplus', 'loanstreet', 'bankbazaar', 
    'credit malaysia', 'propertyguru', 'mystartr', 'getfinancial',
    'finfolio', 'smartloans', 'kreditgo', '1bank'
]


class PaginationHandler:
    """è‡ªé€‚åº”åˆ†é¡µå¤„ç†å™¨ - æ”¯æŒæ‰€æœ‰åˆ†é¡µç±»å‹"""
    
    @staticmethod
    def find_next_page(soup: BeautifulSoup, current_url: str) -> Optional[str]:
        """
        æ™ºèƒ½æ£€æµ‹ä¸‹ä¸€é¡µé“¾æ¥
        æ”¯æŒï¼šnumbered links, rel='next', JS tokens, API offsets
        """
        base_url = urlparse(current_url)._replace(query='', fragment='').geturl()
        
        # æ–¹æ³•1: æŸ¥æ‰¾rel="next"
        next_link = soup.find('a', rel='next')
        if next_link and next_link.get('href'):
            return urljoin(current_url, next_link['href'])
        
        # æ–¹æ³•2: æŸ¥æ‰¾"Next"æŒ‰é’®æˆ–é“¾æ¥
        next_patterns = ['next', 'next page', 'â€º', 'Â»', 'seterusnya', 'berikutnya']
        for link in soup.find_all('a', href=True):
            text = link.get_text(strip=True).lower()
            if any(pattern in text for pattern in next_patterns):
                href = link.get('href')
                if href and not href.startswith('#'):
                    return urljoin(current_url, href)
        
        # æ–¹æ³•3: æŸ¥æ‰¾åˆ†é¡µæ•°å­—ï¼ˆå½“å‰é¡µ+1ï¼‰
        parsed = urlparse(current_url)
        query_params = parse_qs(parsed.query)
        
        # æ£€æŸ¥pageå‚æ•°
        if 'page' in query_params:
            try:
                current_page = int(query_params['page'][0])
                query_params['page'] = [str(current_page + 1)]
                new_query = urlencode(query_params, doseq=True)
                return urlunparse(parsed._replace(query=new_query))
            except:
                pass
        
        # æ–¹æ³•4: æ£€æŸ¥offsetå‚æ•°
        if 'offset' in query_params:
            try:
                current_offset = int(query_params['offset'][0])
                limit = int(query_params.get('limit', [20])[0])
                query_params['offset'] = [str(current_offset + limit)]
                new_query = urlencode(query_params, doseq=True)
                return urlunparse(parsed._replace(query=new_query))
            except:
                pass
        
        return None
    
    @staticmethod
    def detect_max_pages(soup: BeautifulSoup) -> int:
        """æ£€æµ‹æœ€å¤§é¡µæ•°"""
        max_page = 1
        
        # æŸ¥æ‰¾åˆ†é¡µé“¾æ¥ä¸­çš„æœ€å¤§æ•°å­—
        for link in soup.find_all('a', href=True):
            text = link.get_text(strip=True)
            if text.isdigit():
                page_num = int(text)
                max_page = max(max_page, page_num)
        
        return max_page


class ProductExtractor:
    """äº§å“å­—æ®µæå–å™¨ - æå–å®Œæ•´12ä¸ªå­—æ®µ"""
    
    @staticmethod
    def extract_rate(text: str, soup: BeautifulSoup = None) -> str:
        """æå–åˆ©ç‡"""
        patterns = [
            r'(\d+\.?\d*)\s*%\s*(p\.a\.|per\s+annum|pa)?',
            r'(BR|BLR|SBR|OPR)\s*[\+\-]\s*(\d+\.?\d*)\s*%?',
            r'from\s+(\d+\.?\d*)\s*%',
            r'as\s+low\s+as\s+(\d+\.?\d*)\s*%',
            r'starting\s+from\s+(\d+\.?\d*)\s*%',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(0).strip()
        
        # æŸ¥æ‰¾è¡¨æ ¼ä¸­çš„åˆ©ç‡
        if soup:
            for th in soup.find_all(['th', 'td', 'strong', 'b']):
                th_text = th.get_text().lower()
                if 'rate' in th_text or 'interest' in th_text or 'kadar' in th_text:
                    # æŸ¥æ‰¾ç›¸é‚»å•å…ƒæ ¼
                    sibling = th.find_next_sibling(['td', 'span', 'div'])
                    if sibling:
                        sibling_text = sibling.get_text(strip=True)
                        for pattern in patterns:
                            match = re.search(pattern, sibling_text, re.IGNORECASE)
                            if match:
                                return match.group(0).strip()
        
        return 'è¯·è”ç³»é“¶è¡Œ'
    
    @staticmethod
    def extract_tenure(text: str, soup: BeautifulSoup = None) -> str:
        """æå–æœŸé™"""
        patterns = [
            r'up\s+to\s+(\d+)\s*(years?|months?|tahun|bulan)',
            r'(\d+)\s*[-â€“]\s*(\d+)\s*(years?|months?|tahun|bulan)',
            r'(\d+)\s*(years?|months?|tahun|bulan)',
            r'maximum\s+(\d+)\s*(years?|months?)',
            r'tenure.*?(\d+)\s*(years?|months?)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(0).strip()
        
        return 'è¯·è”ç³»é“¶è¡Œ'
    
    @staticmethod
    def extract_fees(text: str, soup: BeautifulSoup = None) -> str:
        """æå–è´¹ç”¨"""
        fees = []
        
        # å¸¸è§è´¹ç”¨å…³é”®è¯
        fee_patterns = [
            (r'annual\s+fee.*?RM\s*(\d+[,\d]*)', 'Annual Fee'),
            (r'processing\s+fee.*?(\d+\.?\d*)\s*%', 'Processing Fee'),
            (r'stamp\s+duty.*?RM\s*(\d+[,\d]*)', 'Stamp Duty'),
            (r'legal\s+fee.*?RM\s*(\d+[,\d]*)', 'Legal Fee'),
            (r'late\s+payment.*?RM\s*(\d+[,\d]*)', 'Late Payment'),
            (r'service\s+tax.*?(\d+\.?\d*)\s*%', 'Service Tax'),
        ]
        
        for pattern, fee_type in fee_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                fees.append(f"{fee_type}: {match.group(0)}")
        
        if fees:
            return ' | '.join(fees)
        
        return 'è¯·è”ç³»é“¶è¡Œäº†è§£è´¹ç”¨è¯¦æƒ…'
    
    @staticmethod
    def extract_features(soup: BeautifulSoup, text: str) -> str:
        """æå–äº§å“ç‰¹ç‚¹"""
        features = []
        
        # æŸ¥æ‰¾åˆ—è¡¨é¡¹
        for ul in soup.find_all(['ul', 'ol']):
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç‰¹ç‚¹åˆ—è¡¨
            parent_text = ''
            parent = ul.find_parent(['div', 'section'])
            if parent:
                heading = parent.find(['h2', 'h3', 'h4', 'strong'])
                if heading:
                    parent_text = heading.get_text().lower()
            
            if any(kw in parent_text for kw in ['feature', 'highlight', 'benefit', 'advantage', 'ciri', 'kelebihan']):
                items = ul.find_all('li')
                for item in items[:5]:
                    item_text = item.get_text(strip=True)
                    if 10 < len(item_text) < 200:
                        features.append(item_text)
        
        if features:
            return ' | '.join(features[:5])
        
        return 'è¯·è®¿é—®é“¶è¡Œå®˜ç½‘äº†è§£äº§å“ç‰¹ç‚¹'
    
    @staticmethod
    def extract_benefits(soup: BeautifulSoup, text: str) -> str:
        """æå–äº§å“ä¼˜åŠ¿"""
        benefits = []
        
        # æŸ¥æ‰¾åŒ…å«"benefits"æˆ–"rewards"çš„éƒ¨åˆ†
        for section in soup.find_all(['div', 'section', 'article']):
            heading = section.find(['h2', 'h3', 'h4', 'strong', 'b'])
            if heading:
                heading_text = heading.get_text().lower()
                if any(kw in heading_text for kw in ['benefit', 'reward', 'advantage', 'perks', 'ganjaran', 'manfaat']):
                    # æå–æ­¤éƒ¨åˆ†çš„åˆ—è¡¨
                    items = section.find_all('li')
                    for item in items[:5]:
                        item_text = item.get_text(strip=True)
                        if 10 < len(item_text) < 200:
                            benefits.append(item_text)
        
        if benefits:
            return ' | '.join(benefits[:5])
        
        return 'è¯·è®¿é—®é“¶è¡Œå®˜ç½‘äº†è§£äº§å“ä¼˜åŠ¿'
    
    @staticmethod
    def extract_required_docs(soup: BeautifulSoup, text: str) -> str:
        """æå–æ‰€éœ€æ–‡ä»¶"""
        docs = []
        
        # æŸ¥æ‰¾åŒ…å«"document"æˆ–"requirement"çš„éƒ¨åˆ†
        for section in soup.find_all(['div', 'section', 'article']):
            heading = section.find(['h2', 'h3', 'h4', 'strong', 'b'])
            if heading:
                heading_text = heading.get_text().lower()
                if any(kw in heading_text for kw in ['document', 'requirement', 'eligibility', 'dokumen', 'syarat']):
                    items = section.find_all('li')
                    for item in items[:5]:
                        item_text = item.get_text(strip=True)
                        if any(kw in item_text.lower() for kw in ['ic', 'nric', 'passport', 'payslip', 'statement', 'form', 'proof']):
                            docs.append(item_text)
        
        if docs:
            return ' | '.join(docs[:5])
        
        return 'è¯·è”ç³»é“¶è¡Œäº†è§£æ‰€éœ€æ–‡ä»¶'
    
    @staticmethod
    def find_pdf_links(soup: BeautifulSoup, keywords: List[str]) -> str:
        """æŸ¥æ‰¾PDFé“¾æ¥"""
        # æŸ¥æ‰¾æ‰€æœ‰PDFé“¾æ¥
        pdf_links = soup.find_all('a', href=re.compile(r'\.pdf$', re.I))
        
        for link in pdf_links:
            text = link.get_text().lower()
            href = link.get('href', '').lower()
            
            if any(kw in text or kw in href for kw in keywords):
                full_url = link.get('href')
                if full_url:
                    return full_url
        
        return ''
    
    @staticmethod
    def determine_customer_type(text: str, product_name: str) -> str:
        """åˆ¤æ–­å®¢æˆ·ç±»å‹åå¥½"""
        combined = (text + ' ' + product_name).lower()
        
        business_keywords = ['business', 'sme', 'enterprise', 'self-employed', 'entrepreneur', 'perniagaan']
        salaried_keywords = ['salaried', 'employee', 'payslip', 'gaji', 'pekerja']
        
        business_score = sum(1 for kw in business_keywords if kw in combined)
        salaried_score = sum(1 for kw in salaried_keywords if kw in combined)
        
        if business_score > salaried_score:
            return 'ä¼ä¸šå®¢æˆ· (Business/Self-Employed)'
        elif salaried_score > 0:
            return 'æ‰“å·¥æ— (Salaried)'
        else:
            return 'æ‰€æœ‰å®¢æˆ· (All)'


class UltimateLoanScraper:
    """ç»ˆææ·±åº¦çˆ¬è™« - ä¸‰å±‚æ¶æ„"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,ms;q=0.8,zh;q=0.7',
        })
        
        self.pagination_handler = PaginationHandler()
        self.extractor = ProductExtractor()
        
        # å·²è®¿é—®URLï¼ˆå»é‡ï¼‰
        self.visited_urls: Set[str] = set()
        self.product_hashes: Set[str] = set()
        
        # è´·æ¬¾ç±»å‹å…³é”®è¯
        self.loan_keywords = {
            'credit_card': ['credit card', 'credit-card', 'kad kredit', 'visa', 'mastercard', 'amex', 'cards/'],
            'home_loan': ['home loan', 'housing loan', 'mortgage', 'property loan', 'rumah', 'housing finance', 'home-loan'],
            'refinance': ['refinance', 'refinancing', 'loan refinance', 'refi'],
            'personal_loan': ['personal loan', 'cash loan', 'personal financing', 'pinjaman peribadi', 'personal-loan'],
            'debt_consolidation': ['debt consolidation', 'consolidation loan', 'debt management'],
            'car_loan': ['car loan', 'auto loan', 'vehicle loan', 'hire purchase', 'kereta', 'auto-loan'],
            'business_loan': ['business loan', 'business financing', 'commercial loan', 'business-loan'],
            'sme_loan': ['sme loan', 'sme financing', 'small business', 'enterprise loan', 'sme/'],
            'other': ['loan', 'financing', 'pinjaman', 'pembiayaan', 'credit']
        }
    
    def classify_loan_type(self, text: str) -> str:
        """æ™ºèƒ½åˆ†ç±»è´·æ¬¾ç±»å‹"""
        text_lower = text.lower()
        
        # æŒ‰ä¼˜å…ˆçº§æ£€æŸ¥
        for loan_type, keywords in self.loan_keywords.items():
            if any(kw in text_lower for kw in keywords):
                return loan_type.upper()
        
        return 'OTHER'
    
    def is_comparison_platform(self, company_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ¯”ä»·å¹³å°"""
        return any(platform in company_name.lower() for platform in COMPARISON_PLATFORMS)
    
    # ===== LAYER 1: æ— é™åˆ¶å‘ç° + è‡ªé€‚åº”åˆ†é¡µ =====
    
    def discover_all_loan_pages(self, base_url: str, company_name: str, max_pages: int = 100) -> List[str]:
        """
        Layer 1: æ— é™åˆ¶é“¾æ¥æ¢ç´¢
        ç§»é™¤æ‰€æœ‰é¡µé¢é™åˆ¶ï¼Œä½¿ç”¨åˆ†é¡µå¤„ç†å™¨çˆ¬å–æ‰€æœ‰é¡µé¢
        """
        logger.info(f"  ğŸ” Layer 1: æ·±åº¦æ¢ç´¢ {company_name} çš„æ‰€æœ‰è´·æ¬¾é¡µé¢...")
        
        all_loan_pages = set()
        is_comparison = self.is_comparison_platform(company_name)
        
        try:
            # è®¿é—®é¦–é¡µ
            response = self.session.get(base_url, timeout=20, allow_redirects=True)
            
            if response.status_code != 200:
                logger.warning(f"    é¦–é¡µè¿”å› {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰è´·æ¬¾ç›¸å…³é“¾æ¥
            loan_links = self._find_loan_links(soup, base_url)
            all_loan_pages.update(loan_links)
            
            # å¦‚æœæ˜¯æ¯”ä»·å¹³å°ï¼Œä½¿ç”¨ç‰¹æ®Šç­–ç•¥
            if is_comparison:
                comparison_pages = self._scrape_comparison_platform(base_url, company_name, soup)
                all_loan_pages.update(comparison_pages)
            else:
                # æ™®é€šé“¶è¡Œï¼šå°è¯•å¸¸è§è·¯å¾„
                common_pages = self._try_common_paths(base_url)
                all_loan_pages.update(common_pages)
            
            # å¤„ç†åˆ†é¡µï¼ˆé’ˆå¯¹åˆ—è¡¨é¡µï¼‰
            paginated_pages = self._handle_pagination(list(all_loan_pages)[:10], max_pages)
            all_loan_pages.update(paginated_pages)
            
            logger.info(f"    âœ… æ‰¾åˆ° {len(all_loan_pages)} ä¸ªè´·æ¬¾é¡µé¢ï¼ˆæ— é™åˆ¶ï¼‰")
            return list(all_loan_pages)
            
        except Exception as e:
            logger.error(f"    âŒ æ¢ç´¢å¤±è´¥: {e}")
            return []
    
    def _find_loan_links(self, soup: BeautifulSoup, base_url: str) -> Set[str]:
        """æŸ¥æ‰¾æ‰€æœ‰è´·æ¬¾ç›¸å…³é“¾æ¥"""
        loan_links = set()
        
        all_links = soup.find_all('a', href=True)
        
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True).lower()
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«è´·æ¬¾å…³é”®è¯
            is_loan_link = False
            for category, keywords in self.loan_keywords.items():
                if any(kw in text or kw in href.lower() for kw in keywords):
                    is_loan_link = True
                    break
            
            if is_loan_link:
                full_url = urljoin(base_url, href)
                if self._is_valid_url(full_url, base_url):
                    loan_links.add(full_url)
        
        return loan_links
    
    def _is_valid_url(self, url: str, base_url: str) -> bool:
        """éªŒè¯URL"""
        try:
            parsed_url = urlparse(url)
            parsed_base = urlparse(base_url)
            
            # å¿…é¡»æ˜¯åŒä¸€åŸŸå
            if parsed_url.netloc != parsed_base.netloc:
                return False
            
            # æ’é™¤æ— æ•ˆé“¾æ¥
            invalid = ['login', 'logout', 'signin', 'register', 'mailto:', 'tel:', 'javascript:', '#']
            if any(inv in url.lower() for inv in invalid):
                return False
            
            return True
        except:
            return False
    
    def _try_common_paths(self, base_url: str) -> Set[str]:
        """å°è¯•å¸¸è§è´·æ¬¾è·¯å¾„"""
        common_paths = [
            # Credit Cards
            '/personal/credit-cards', '/credit-cards', '/cards', '/en/cards',
            '/personal/cards', '/islamic/cards',
            # Home Loans
            '/personal/home-loans', '/home-loans', '/mortgage', '/housing-loan',
            '/personal/property', '/islamic/home-financing',
            # Personal Loans
            '/personal/loans', '/personal-loans', '/cash-loan', '/personal/financing',
            '/islamic/personal-financing',
            # Car Loans
            '/personal/car-loan', '/auto-loan', '/hire-purchase', '/vehicle-financing',
            # Business & SME
            '/business/loans', '/sme', '/business/financing', '/sme/loans',
            '/business/sme-loans', '/islamic/business-financing',
            # General
            '/loans', '/financing', '/products/loans', '/en/loans',
        ]
        
        valid_urls = set()
        for path in common_paths:
            url = urljoin(base_url, path)
            try:
                response = self.session.head(url, timeout=5, allow_redirects=True)
                if response.status_code == 200:
                    valid_urls.add(url)
                    logger.info(f"      âœ… æ‰¾åˆ°è·¯å¾„: {path}")
            except:
                pass
        
        return valid_urls
    
    def _scrape_comparison_platform(self, base_url: str, company_name: str, soup: BeautifulSoup) -> Set[str]:
        """æ¯”ä»·å¹³å°ä¸“ç”¨çˆ¬è™«ç­–ç•¥"""
        logger.info(f"      ğŸ¯ æ£€æµ‹åˆ°æ¯”ä»·å¹³å°ï¼Œä½¿ç”¨ä¸“ç”¨ç­–ç•¥")
        
        comparison_pages = set()
        
        # ç­–ç•¥1: æŸ¥æ‰¾æ‰€æœ‰äº§å“åˆ—è¡¨é¡µ
        product_list_keywords = ['credit-card', 'personal-loan', 'home-loan', 'car-loan', 
                                  'business-loan', 'compare', 'products', 'list']
        
        for link in soup.find_all('a', href=True):
            href = link.get('href', '').lower()
            if any(kw in href for kw in product_list_keywords):
                full_url = urljoin(base_url, link['href'])
                if self._is_valid_url(full_url, base_url):
                    comparison_pages.add(full_url)
        
        # ç­–ç•¥2: å°è¯•ç›´æ¥è·¯å¾„
        comparison_paths = [
            '/credit-cards', '/credit-cards/all',
            '/personal-loans', '/personal-loans/all',
            '/home-loans', '/home-loans/all',
            '/car-loans', '/car-loans/all',
            '/business-loans', '/business-loans/all',
            '/compare/credit-cards', '/compare/personal-loans',
        ]
        
        for path in comparison_paths:
            url = urljoin(base_url, path)
            try:
                response = self.session.head(url, timeout=5, allow_redirects=True)
                if response.status_code == 200:
                    comparison_pages.add(url)
            except:
                pass
        
        return comparison_pages
    
    def _handle_pagination(self, seed_urls: List[str], max_pages: int = 100) -> Set[str]:
        """å¤„ç†åˆ†é¡µ - çˆ¬å–æ‰€æœ‰é¡µé¢ç›´åˆ°æ²¡æœ‰ä¸‹ä¸€é¡µ"""
        logger.info(f"      ğŸ“‘ å¼€å§‹å¤„ç†åˆ†é¡µï¼ˆæœ€å¤š{max_pages}é¡µï¼‰...")
        
        all_pages = set()
        
        for seed_url in seed_urls:
            if seed_url in self.visited_urls:
                continue
            
            current_url = seed_url
            page_count = 0
            
            while current_url and page_count < max_pages:
                if current_url in self.visited_urls:
                    break
                
                try:
                    response = self.session.get(current_url, timeout=15)
                    if response.status_code != 200:
                        break
                    
                    self.visited_urls.add(current_url)
                    all_pages.add(current_url)
                    page_count += 1
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # æŸ¥æ‰¾ä¸‹ä¸€é¡µ
                    next_url = self.pagination_handler.find_next_page(soup, current_url)
                    
                    if not next_url or next_url == current_url:
                        break
                    
                    current_url = next_url
                    time.sleep(0.5)
                    
                except Exception as e:
                    logger.debug(f"        åˆ†é¡µå¤„ç†é”™è¯¯: {e}")
                    break
            
            if page_count > 1:
                logger.info(f"        âœ… {seed_url} - æ‰¾åˆ° {page_count} é¡µ")
        
        return all_pages
    
    # ===== LAYER 2: äº§å“è¯¦æƒ…æ·±åº¦æå– =====
    
    def extract_products_from_page(self, url: str, company_name: str) -> List[Dict[str, Any]]:
        """
        Layer 2: äº§å“è¯¦æƒ…é¡µæ·±åº¦æå–
        æå–å®Œæ•´12ä¸ªå­—æ®µ
        """
        products = []
        
        try:
            response = self.session.get(url, timeout=20)
            if response.status_code != 200:
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            text = soup.get_text(separator=' ', strip=True)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯äº§å“åˆ—è¡¨é¡µè¿˜æ˜¯è¯¦æƒ…é¡µ
            is_listing_page = self._is_listing_page(soup)
            
            if is_listing_page:
                # åˆ—è¡¨é¡µï¼šæå–æ‰€æœ‰äº§å“é“¾æ¥ï¼Œç„¶åè®¿é—®è¯¦æƒ…é¡µ
                product_links = self._extract_product_links(soup, url)
                logger.info(f"      ğŸ“‹ åˆ—è¡¨é¡µï¼Œæ‰¾åˆ° {len(product_links)} ä¸ªäº§å“é“¾æ¥")
                
                for product_url in product_links[:50]:  # é™åˆ¶æ¯é¡µæœ€å¤š50ä¸ªäº§å“
                    product = self._extract_single_product(product_url, company_name)
                    if product:
                        products.append(product)
                    time.sleep(0.3)
            else:
                # è¯¦æƒ…é¡µï¼šç›´æ¥æå–
                product = self._extract_single_product(url, company_name)
                if product:
                    products.append(product)
            
        except Exception as e:
            logger.error(f"      âŒ é¡µé¢æå–å¤±è´¥ {url}: {e}")
        
        return products
    
    def _is_listing_page(self, soup: BeautifulSoup) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºäº§å“åˆ—è¡¨é¡µ"""
        # æŸ¥æ‰¾å¤šä¸ªäº§å“å¡ç‰‡æˆ–åˆ—è¡¨é¡¹
        product_cards = soup.find_all(['div', 'article'], class_=re.compile(r'(product|card|item)', re.I))
        
        if len(product_cards) > 3:
            return True
        
        # æŸ¥æ‰¾è¡¨æ ¼è¡Œ
        table_rows = soup.find_all('tr')
        if len(table_rows) > 5:
            return True
        
        return False
    
    def _extract_product_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """ä»åˆ—è¡¨é¡µæå–æ‰€æœ‰äº§å“é“¾æ¥"""
        product_links = []
        
        # æŸ¥æ‰¾äº§å“å¡ç‰‡ä¸­çš„é“¾æ¥
        for card in soup.find_all(['div', 'article'], class_=re.compile(r'(product|card|item)', re.I)):
            link = card.find('a', href=True)
            if link:
                full_url = urljoin(base_url, link['href'])
                if self._is_valid_url(full_url, base_url):
                    product_links.append(full_url)
        
        # æŸ¥æ‰¾è¡¨æ ¼ä¸­çš„é“¾æ¥
        for row in soup.find_all('tr'):
            link = row.find('a', href=True)
            if link:
                href = link['href']
                # æ’é™¤åˆ†é¡µé“¾æ¥
                if not any(kw in href.lower() for kw in ['page=', 'next', 'prev']):
                    full_url = urljoin(base_url, href)
                    if self._is_valid_url(full_url, base_url):
                        product_links.append(full_url)
        
        return list(set(product_links))  # å»é‡
    
    def _extract_single_product(self, url: str, company_name: str) -> Optional[Dict[str, Any]]:
        """æå–å•ä¸ªäº§å“çš„å®Œæ•´12ä¸ªå­—æ®µ"""
        try:
            response = self.session.get(url, timeout=15)
            if response.status_code != 200:
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            text = soup.get_text(separator=' ', strip=True)
            
            # æå–äº§å“åç§°
            product_name = self._extract_product_name(soup)
            if not product_name or len(product_name) < 3:
                return None
            
            # å»é‡æ£€æŸ¥
            product_hash = hashlib.md5(f"{company_name}_{product_name}".encode()).hexdigest()
            if product_hash in self.product_hashes:
                return None
            self.product_hashes.add(product_hash)
            
            # åˆ¤æ–­è´·æ¬¾ç±»å‹
            loan_type = self.classify_loan_type(url + ' ' + text + ' ' + product_name)
            
            # æå–12ä¸ªå­—æ®µ
            product = {
                'company': company_name,
                'loan_type': loan_type,
                'product_name': product_name,
                'required_doc': self.extractor.extract_required_docs(soup, text),
                'features': self.extractor.extract_features(soup, text),
                'benefits': self.extractor.extract_benefits(soup, text),
                'fees_charges': self.extractor.extract_fees(text, soup),
                'tenure': self.extractor.extract_tenure(text, soup),
                'rate': self.extractor.extract_rate(text, soup),
                'application_form_url': self.extractor.find_pdf_links(soup, ['application', 'apply', 'form']),
                'product_disclosure_url': self.extractor.find_pdf_links(soup, ['disclosure', 'pds', 'product disclosure']),
                'terms_conditions_url': self.extractor.find_pdf_links(soup, ['terms', 'conditions', 'tnc', 't&c']),
                'preferred_customer_type': self.extractor.determine_customer_type(text, product_name),
                'source_url': url,
                'scraped_at': datetime.now().isoformat()
            }
            
            return product
            
        except Exception as e:
            logger.debug(f"        æå–äº§å“å¤±è´¥ {url}: {e}")
            return None
    
    def _extract_product_name(self, soup: BeautifulSoup) -> str:
        """æå–äº§å“åç§°"""
        # ä¼˜å…ˆçº§1: h1
        h1 = soup.find('h1')
        if h1:
            name = h1.get_text(strip=True)
            if 5 < len(name) < 150:
                return name
        
        # ä¼˜å…ˆçº§2: title
        title = soup.find('title')
        if title:
            name = title.get_text(strip=True)
            # æ¸…ç†titleï¼ˆç§»é™¤ç«™ç‚¹åç§°ï¼‰
            name = re.split(r'[|â€“-]', name)[0].strip()
            if 5 < len(name) < 150:
                return name
        
        # ä¼˜å…ˆçº§3: h2
        h2 = soup.find('h2')
        if h2:
            name = h2.get_text(strip=True)
            if 5 < len(name) < 150:
                return name
        
        return ''
    
    # ===== LAYER 0: CSV Orchestrator =====
    
    def scrape_institution(self, company_name: str, website: str) -> List[Dict[str, Any]]:
        """
        Layer 0: çˆ¬å–å•ä¸ªæœºæ„çš„æ‰€æœ‰è´·æ¬¾äº§å“
        orchestrator - åè°ƒLayer 1å’ŒLayer 2
        """
        logger.info(f"ğŸ¦ å¼€å§‹çˆ¬å–: {company_name}")
        logger.info(f"   ç½‘å€: {website}")
        
        all_products = []
        
        # Layer 1: å‘ç°æ‰€æœ‰è´·æ¬¾é¡µé¢ï¼ˆæ— é™åˆ¶ï¼‰
        loan_pages = self.discover_all_loan_pages(website, company_name, max_pages=100)
        
        if not loan_pages:
            logger.warning(f"  âš ï¸ {company_name}: æœªæ‰¾åˆ°è´·æ¬¾é¡µé¢")
            return []
        
        logger.info(f"  ğŸ“„ Layer 2: æå–äº§å“è¯¦æƒ…ï¼ˆ{len(loan_pages)} ä¸ªé¡µé¢ï¼‰...")
        
        # Layer 2: æå–æ¯ä¸ªé¡µé¢çš„äº§å“ï¼ˆæ— é¡µé¢é™åˆ¶ï¼‰
        for idx, page_url in enumerate(loan_pages, 1):
            logger.info(f"    [{idx}/{len(loan_pages)}] {page_url}")
            
            products = self.extract_products_from_page(page_url, company_name)
            
            if products:
                logger.info(f"      âœ… æ‰¾åˆ° {len(products)} ä¸ªäº§å“")
                all_products.extend(products)
            
            time.sleep(0.5)  # ç¤¼è²Œå»¶è¿Ÿ
        
        logger.info(f"  âœ… {company_name}: å…± {len(all_products)} ä¸ªäº§å“")
        return all_products


# ===== LAYER 3: æ•°æ®éªŒè¯å’ŒQA =====

def validate_product(product: Dict[str, Any]) -> bool:
    """éªŒè¯äº§å“æ•°æ®è´¨é‡"""
    required_fields = ['company', 'loan_type', 'product_name']
    
    # å¿…é¡»å­—æ®µæ£€æŸ¥
    for field in required_fields:
        if not product.get(field):
            return False
    
    # è‡³å°‘10ä¸ªéç©ºå­—æ®µ
    non_empty_count = sum(1 for v in product.values() if v and str(v).strip())
    if non_empty_count < 10:
        return False
    
    return True


def init_database():
    """åˆå§‹åŒ–PostgreSQLæ•°æ®åº“"""
    con = psycopg2.connect(DATABASE_URL)
    cur = con.cursor()
    
    cur.execute("""
        CREATE TABLE IF NOT EXISTS loan_products_ultimate(
            id SERIAL PRIMARY KEY,
            company TEXT,
            loan_type TEXT,
            product_name TEXT,
            required_doc TEXT,
            features TEXT,
            benefits TEXT,
            fees_charges TEXT,
            tenure TEXT,
            rate TEXT,
            application_form_url TEXT,
            product_disclosure_url TEXT,
            terms_conditions_url TEXT,
            preferred_customer_type TEXT,
            source_url TEXT,
            scraped_at TIMESTAMP,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(company, product_name)
        )
    """)
    
    con.commit()
    cur.close()
    con.close()
    logger.info("âœ… PostgreSQLæ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")


def load_institutions_from_csv(csv_path: str) -> List[Dict[str, str]]:
    """ä»CSVåŠ è½½æœºæ„åˆ—è¡¨ï¼ˆæŒ‰é¡ºåºï¼‰"""
    institutions = []
    
    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        next(reader)  # è·³è¿‡æ ‡é¢˜è¡Œ
        
        for row in reader:
            if len(row) >= 2:
                institutions.append({
                    'name': row[0],
                    'website': row[1]
                })
    
    logger.info(f"ğŸ“‹ åŠ è½½äº† {len(institutions)} å®¶é‡‘èæœºæ„ï¼ˆæŒ‰CSVé¡ºåºï¼‰")
    return institutions


def save_to_database(products: List[Dict[str, Any]]):
    """ä¿å­˜åˆ°PostgreSQLæ•°æ®åº“ï¼ˆå»é‡ï¼‰"""
    if not products:
        return
    
    con = psycopg2.connect(DATABASE_URL)
    cur = con.cursor()
    
    insert_sql = """
        INSERT INTO loan_products_ultimate(
            company, loan_type, product_name, required_doc, features, benefits,
            fees_charges, tenure, rate, application_form_url, product_disclosure_url,
            terms_conditions_url, preferred_customer_type, source_url, scraped_at
        ) VALUES %s
        ON CONFLICT (company, product_name) DO NOTHING
    """
    
    items = [
        (
            p['company'], p['loan_type'], p['product_name'], p['required_doc'],
            p['features'], p['benefits'], p['fees_charges'], p['tenure'],
            p['rate'], p.get('application_form_url', ''), p.get('product_disclosure_url', ''),
            p.get('terms_conditions_url', ''), p['preferred_customer_type'],
            p.get('source_url', ''), p.get('scraped_at', '')
        )
        for p in products
        if validate_product(p)  # Layer 3: éªŒè¯
    ]
    
    if items:
        execute_values(cur, insert_sql, items)
        con.commit()
        logger.info(f"    ğŸ’¾ ä¿å­˜äº† {len(items)} ä¸ªæœ‰æ•ˆäº§å“åˆ°æ•°æ®åº“")
    
    cur.close()
    con.close()


def export_to_csv_table(output_path: str):
    """å¯¼å‡ºä¸ºç²¾è‡´è¡¨æ ¼ï¼ˆ12åˆ—ï¼‰"""
    con = psycopg2.connect(DATABASE_URL)
    cur = con.cursor()
    
    cur.execute("""
        SELECT 
            company AS "COMPANY",
            loan_type AS "LOAN TYPE",
            required_doc AS "REQUIRED DOC",
            features AS "FEATURES",
            benefits AS "BENEFITS",
            fees_charges AS "FEES & CHARGES",
            tenure AS "TENURE",
            rate AS "RATE",
            application_form_url AS "APPLICATION FORM",
            product_disclosure_url AS "PRODUCT DISCLOSURE",
            terms_conditions_url AS "TERMS & CONDITIONS",
            preferred_customer_type AS "å®¢æˆ·åå¥½"
        FROM loan_products_ultimate
        ORDER BY 
            CASE company
                WHEN 'Affin Bank Berhad' THEN 1
                WHEN 'Alliance Bank Malaysia Berhad' THEN 2
                WHEN 'AmBank (M) Berhad' THEN 3
                ELSE 999
            END,
            loan_type,
            product_name
    """)
    
    rows = cur.fetchall()
    columns = [desc[0] for desc in cur.description]
    
    cur.close()
    con.close()
    
    # å†™å…¥CSV
    with open(output_path, 'w', newline='', encoding='utf-8-sig') as f:
        if rows:
            writer = csv.DictWriter(f, fieldnames=columns)
            writer.writeheader()
            for row in rows:
                writer.writerow(dict(zip(columns, row)))
    
    logger.info(f"ğŸ“¤ è¡¨æ ¼å·²å¯¼å‡º: {output_path}")
    logger.info(f"   æ€»è®¡: {len(rows)} ä¸ªäº§å“")


def generate_statistics():
    """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
    con = psycopg2.connect(DATABASE_URL)
    cur = con.cursor()
    
    # æ€»äº§å“æ•°
    cur.execute("SELECT COUNT(*) FROM loan_products_ultimate")
    total_products = cur.fetchone()[0]
    
    # æŒ‰å…¬å¸ç»Ÿè®¡
    cur.execute("""
        SELECT company, COUNT(*) as count 
        FROM loan_products_ultimate 
        GROUP BY company 
        ORDER BY count DESC
    """)
    by_company = cur.fetchall()
    
    # æŒ‰è´·æ¬¾ç±»å‹ç»Ÿè®¡
    cur.execute("""
        SELECT loan_type, COUNT(*) as count 
        FROM loan_products_ultimate 
        GROUP BY loan_type 
        ORDER BY count DESC
    """)
    by_loan_type = cur.fetchall()
    
    cur.close()
    con.close()
    
    logger.info("")
    logger.info("=" * 100)
    logger.info("ğŸ“Š ç»Ÿè®¡æŠ¥å‘Š")
    logger.info("=" * 100)
    logger.info(f"æ€»äº§å“æ•°: {total_products}")
    logger.info("")
    logger.info("æŒ‰å…¬å¸åˆ†å¸ƒï¼ˆTop 10ï¼‰:")
    for company, count in by_company[:10]:
        logger.info(f"  {company}: {count} ä¸ªäº§å“")
    logger.info("")
    logger.info("æŒ‰è´·æ¬¾ç±»å‹åˆ†å¸ƒ:")
    for loan_type, count in by_loan_type:
        logger.info(f"  {loan_type}: {count} ä¸ªäº§å“")
    logger.info("")


def main():
    """ä¸»æµç¨‹"""
    logger.info("")
    logger.info("=" * 100)
    logger.info("ğŸš€ CreditPilot - é©¬æ¥è¥¿äºš68å®¶é‡‘èæœºæ„æ·±åº¦çˆ¬è™«ç³»ç»Ÿ (Ultimate Edition)")
    logger.info("=" * 100)
    logger.info("ä¸‰å±‚æ¶æ„: Layer 0 (Orchestrator) + Layer 1 (Discovery) + Layer 2 (Extraction) + Layer 3 (QA)")
    logger.info("ç›®æ ‡: 3000-5000ä¸ªäº§å“ï¼Œ100%å‡†ç¡®æ€§")
    logger.info("=" * 100)
    logger.info("")
    
    start_time = datetime.now()
    
    # åˆå§‹åŒ–
    init_database()
    
    # åŠ è½½æœºæ„åˆ—è¡¨ï¼ˆæŒ‰CSVé¡ºåºï¼‰
    institutions = load_institutions_from_csv(CSV_INPUT)
    
    # åˆ›å»ºçˆ¬è™«
    scraper = UltimateLoanScraper()
    
    # é€ä¸ªçˆ¬å–ï¼ˆæŒ‰CSVé¡ºåºï¼‰
    all_products = []
    
    for idx, inst in enumerate(institutions, 1):
        logger.info(f"\nğŸ“ è¿›åº¦: {idx}/{len(institutions)}")
        logger.info("-" * 100)
        
        try:
            products = scraper.scrape_institution(inst['name'], inst['website'])
            if products:
                all_products.extend(products)
                # å³æ—¶ä¿å­˜ï¼ˆLayer 3: éªŒè¯ï¼‰
                save_to_database(products)
        except Exception as e:
            logger.error(f"âŒ {inst['name']} çˆ¬å–å¤±è´¥: {e}")
        
        time.sleep(2)  # æœºæ„é—´å»¶è¿Ÿ
    
    # å¯¼å‡ºè¡¨æ ¼
    logger.info("")
    logger.info("=" * 100)
    logger.info("ğŸ“Š å¯¼å‡ºç²¾è‡´è¡¨æ ¼ï¼ˆ12åˆ—ï¼‰")
    logger.info("=" * 100)
    export_to_csv_table(CSV_OUTPUT)
    
    # ç»Ÿè®¡
    generate_statistics()
    
    # æ€»ç»“
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    logger.info("")
    logger.info("=" * 100)
    logger.info("ğŸ‰ çˆ¬å–å®Œæˆï¼")
    logger.info("=" * 100)
    logger.info(f"æ€»è€—æ—¶: {duration:.1f} ç§’ ({duration/60:.1f} åˆ†é’Ÿ)")
    logger.info(f"å¤„ç†æœºæ„: {len(institutions)} å®¶")
    logger.info(f"äº§å“æ€»æ•°: {len(all_products)}")
    logger.info(f"æ•°æ®åº“: PostgreSQL (persistent)")
    logger.info(f"è¡¨æ ¼æ–‡ä»¶: {CSV_OUTPUT}")
    logger.info("")
    logger.info("ğŸ’¡ æç¤º: è¯·éšæœºéªŒè¯å‡ å®¶å…¬å¸çš„æ•°æ®ï¼Œç¡®ä¿100%å‡†ç¡®æ€§ï¼")
    logger.info("")


if __name__ == '__main__':
    main()

"""
å¢å¼ºç‰ˆé“¶è¡Œçˆ¬è™« - æ™ºèƒ½æ¢ç´¢å¯¼èˆªèœå•
ä¸éœ€è¦ç™»å½•ä¹Ÿèƒ½æ‰¾åˆ°å®Œæ•´çš„è´·æ¬¾äº§å“ä¿¡æ¯
"""
import os
import json
import logging
import requests
from typing import List, Dict, Any, Optional, Set
from bs4 import BeautifulSoup
from datetime import datetime
import re
import time
from urllib.parse import urljoin, urlparse

logger = logging.getLogger(__name__)


class EnhancedBankScraper:
    """
    å¢å¼ºç‰ˆçˆ¬è™«ï¼šæ™ºèƒ½æ¢ç´¢ç½‘ç«™å¯¼èˆª
    
    ç­–ç•¥ï¼š
    1. é¦–é¡µ â†’ æ¢ç´¢æ‰€æœ‰å¯¼èˆªèœå•é“¾æ¥
    2. æŸ¥æ‰¾ä¾§è¾¹æ ã€é¡µè„šçš„è´·æ¬¾ç›¸å…³é“¾æ¥
    3. å°è¯•å¤šç§URLæ¨¡å¼
    4. é€’å½’æ¢ç´¢å­é¡µé¢
    """
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,ms;q=0.8,zh;q=0.7',
        })
        
        # è´·æ¬¾ç›¸å…³å…³é”®è¯ï¼ˆè‹±æ–‡/é©¬æ¥æ–‡/ä¸­æ–‡ï¼‰
        self.loan_keywords = [
            # è‹±æ–‡
            'loan', 'loans', 'financing', 'finance', 'credit', 'mortgage',
            'borrow', 'lending', 'personal loan', 'home loan', 'business loan',
            'sme loan', 'car loan', 'education loan', 'refinance',
            
            # é©¬æ¥æ–‡
            'pinjaman', 'pembiayaan', 'kredit',
            
            # å¸¸è§äº§å“å
            'flexi', 'cash', 'easy loan', 'quick loan',
        ]
    
    def find_loan_links(self, soup: BeautifulSoup, base_url: str) -> Set[str]:
        """
        æ™ºèƒ½æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„è´·æ¬¾ç›¸å…³é“¾æ¥
        
        æŸ¥æ‰¾ä½ç½®ï¼š
        1. é¡¶éƒ¨å¯¼èˆªæ ï¼ˆnav, headerï¼‰
        2. ä¾§è¾¹æ ï¼ˆsidebar, asideï¼‰
        3. é¡µè„šï¼ˆfooterï¼‰
        4. ä¸‹æ‹‰èœå•ï¼ˆdropdown, mega-menuï¼‰
        5. ä¸»å†…å®¹åŒºåŸŸçš„é“¾æ¥
        """
        loan_links = set()
        
        # 1. æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
        all_links = soup.find_all('a', href=True)
        
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True).lower()
            
            # æ£€æŸ¥é“¾æ¥æ–‡æœ¬æˆ–URLæ˜¯å¦åŒ…å«è´·æ¬¾å…³é”®è¯
            if any(keyword in text or keyword in href.lower() for keyword in self.loan_keywords):
                full_url = urljoin(base_url, href)
                
                # è¿‡æ»¤å¤–éƒ¨é“¾æ¥å’Œæ— æ•ˆé“¾æ¥
                if self._is_valid_url(full_url, base_url):
                    loan_links.add(full_url)
        
        logger.info(f"   æ‰¾åˆ° {len(loan_links)} ä¸ªè´·æ¬¾ç›¸å…³é“¾æ¥")
        return loan_links
    
    def _is_valid_url(self, url: str, base_url: str) -> bool:
        """éªŒè¯URLæ˜¯å¦æœ‰æ•ˆ"""
        try:
            parsed_url = urlparse(url)
            parsed_base = urlparse(base_url)
            
            # å¿…é¡»æ˜¯åŒä¸€ä¸ªåŸŸå
            if parsed_url.netloc != parsed_base.netloc:
                return False
            
            # æ’é™¤ç™»å½•ã€ä¸‹è½½ã€å¤–éƒ¨é“¾æ¥
            invalid_patterns = [
                'login', 'logout', 'signin', 'signup', 'register',
                'download', 'pdf', 'doc', 'xls',
                'mailto:', 'tel:', 'javascript:',
                '#', 'void(0)'
            ]
            
            if any(pattern in url.lower() for pattern in invalid_patterns):
                return False
            
            return True
        except:
            return False
    
    def explore_navigation_menu(self, bank_name: str, website: str) -> List[str]:
        """
        æ¢ç´¢ç½‘ç«™å¯¼èˆªèœå•ï¼Œæ‰¾åˆ°æ‰€æœ‰è´·æ¬¾äº§å“é¡µé¢
        
        Args:
            bank_name: é“¶è¡Œåç§°
            website: é“¶è¡Œç½‘ç«™URL
        
        Returns:
            æ‰€æœ‰è´·æ¬¾äº§å“é¡µé¢çš„URLåˆ—è¡¨
        """
        logger.info(f"ğŸ” æ¢ç´¢ {bank_name} çš„å¯¼èˆªèœå•...")
        
        try:
            # è®¿é—®é¦–é¡µ
            response = self.session.get(website, timeout=15, allow_redirects=True)
            
            if response.status_code != 200:
                logger.warning(f"   é¦–é¡µè¿”å› {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰è´·æ¬¾ç›¸å…³é“¾æ¥
            loan_links = self.find_loan_links(soup, website)
            
            if not loan_links:
                logger.warning(f"   âš ï¸ æœªæ‰¾åˆ°è´·æ¬¾é“¾æ¥ï¼Œå°è¯•å¸¸è§URLæ¨¡å¼...")
                loan_links = self._try_common_patterns(website)
            
            return list(loan_links)
            
        except Exception as e:
            logger.error(f"   âŒ æ¢ç´¢å¤±è´¥: {e}")
            return []
    
    def _try_common_patterns(self, base_url: str) -> Set[str]:
        """å°è¯•å¸¸è§çš„è´·æ¬¾é¡µé¢URLæ¨¡å¼"""
        common_paths = [
            # è‹±æ–‡è·¯å¾„
            '/personal/loans',
            '/personal/financing',
            '/loans',
            '/financing',
            '/products/loans',
            '/products/financing',
            '/personal/products/loans',
            '/personal-loans',
            '/home-loans',
            '/business-loans',
            '/sme-financing',
            
            # é©¬æ¥æ–‡è·¯å¾„
            '/pinjaman',
            '/pembiayaan',
            
            # å¸¸è§å­è·¯å¾„
            '/en/personal/loans',
            '/en/loans',
            '/my/personal/loans',
        ]
        
        valid_urls = set()
        
        for path in common_paths:
            url = urljoin(base_url, path)
            try:
                response = self.session.head(url, timeout=5, allow_redirects=True)
                if response.status_code == 200:
                    valid_urls.add(url)
                    logger.info(f"   âœ… æ‰¾åˆ°: {path}")
            except:
                pass
        
        return valid_urls
    
    def extract_product_details_from_page(
        self,
        url: str,
        bank_name: str,
        institution_type: str
    ) -> List[Dict[str, Any]]:
        """
        ä»å•ä¸ªé¡µé¢æå–æ‰€æœ‰è´·æ¬¾äº§å“è¯¦æƒ…
        
        è¿”å›12ä¸ªå­—æ®µçš„äº§å“åˆ—è¡¨
        """
        products = []
        
        try:
            response = self.session.get(url, timeout=15)
            
            if response.status_code != 200:
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç­–ç•¥1: æŸ¥æ‰¾äº§å“å¡ç‰‡æˆ–åˆ—è¡¨é¡¹
            product_containers = self._find_product_containers(soup)
            
            if not product_containers:
                # ç­–ç•¥2: æ•´ä¸ªé¡µé¢ä½œä¸ºä¸€ä¸ªäº§å“
                product = self._extract_single_product(soup, url, bank_name, institution_type)
                if product:
                    products.append(product)
            else:
                # æå–æ¯ä¸ªäº§å“
                for container in product_containers[:10]:  # é™åˆ¶æœ€å¤š10ä¸ª
                    product = self._extract_product_from_container(
                        container, url, bank_name, institution_type, soup
                    )
                    if product:
                        products.append(product)
            
        except Exception as e:
            logger.error(f"   âŒ é¡µé¢è§£æå¤±è´¥ {url}: {e}")
        
        return products
    
    def _find_product_containers(self, soup: BeautifulSoup) -> List:
        """æŸ¥æ‰¾äº§å“å®¹å™¨ï¼ˆå¡ç‰‡ã€åˆ—è¡¨é¡¹ç­‰ï¼‰"""
        containers = []
        
        # å¸¸è§äº§å“å®¹å™¨çš„CSSç±»åå’Œæ ‡ç­¾
        selectors = [
            {'class': re.compile(r'product.*card', re.I)},
            {'class': re.compile(r'loan.*card', re.I)},
            {'class': re.compile(r'product.*item', re.I)},
            {'class': re.compile(r'financing.*card', re.I)},
        ]
        
        for selector in selectors:
            found = soup.find_all('div', selector)
            if found:
                containers.extend(found)
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾åˆ—è¡¨é¡¹
        if not containers:
            # æŸ¥æ‰¾åŒ…å«è´·æ¬¾å…³é”®è¯çš„section
            sections = soup.find_all('section')
            for section in sections:
                text = section.get_text().lower()
                if any(kw in text for kw in ['loan', 'financing', 'pinjaman']):
                    items = section.find_all('li')
                    if items:
                        containers.extend(items)
        
        return containers
    
    def _extract_product_from_container(
        self,
        container,
        page_url: str,
        bank_name: str,
        institution_type: str,
        full_soup: BeautifulSoup
    ) -> Optional[Dict[str, Any]]:
        """ä»äº§å“å®¹å™¨ä¸­æå–12ä¸ªå­—æ®µ"""
        
        # äº§å“åç§°
        product_name = None
        for tag in ['h1', 'h2', 'h3', 'h4', 'h5']:
            heading = container.find(tag)
            if heading:
                product_name = heading.get_text(strip=True)
                break
        
        if not product_name or len(product_name) < 3:
            return None
        
        # æå–åˆ©ç‡
        text = container.get_text()
        rate = self._extract_rate(text)
        
        # æå–æœŸé™
        tenure = self._extract_tenure(text)
        
        # åˆ¤æ–­è´·æ¬¾ç±»å‹
        loan_type = self._classify_loan_type(product_name + ' ' + text)
        
        # æå–ç‰¹ç‚¹å’Œä¼˜åŠ¿ï¼ˆä»åˆ—è¡¨é¡¹ï¼‰
        features = []
        benefits = []
        
        lists = container.find_all('li')
        for li in lists[:5]:
            item_text = li.get_text(strip=True)
            if len(item_text) > 10 and len(item_text) < 150:
                # ç®€å•åˆ¤æ–­æ˜¯ç‰¹ç‚¹è¿˜æ˜¯ä¼˜åŠ¿
                if any(kw in item_text.lower() for kw in ['benefit', 'advantage', 'why']):
                    benefits.append(item_text)
                else:
                    features.append(item_text)
        
        # æŸ¥æ‰¾PDFé“¾æ¥
        application_form = self._find_pdf_link(container, ['apply', 'application'])
        disclosure = self._find_pdf_link(container, ['disclosure', 'pds'])
        terms = self._find_pdf_link(container, ['terms', 'conditions', 'tnc'])
        
        # åˆ¤æ–­å®¢æˆ·åå¥½
        preferred_customer = self._determine_customer_preference(text, product_name)
        
        return {
            'company': bank_name,
            'loan_type': loan_type,
            'product_name': product_name,
            'required_doc': "è¯·è”ç³»é“¶è¡Œäº†è§£æ‰€éœ€æ–‡ä»¶",
            'features': ' | '.join(features) if features else "è¯·è®¿é—®é“¶è¡Œå®˜ç½‘äº†è§£äº§å“ç‰¹ç‚¹",
            'benefits': ' | '.join(benefits) if benefits else "è¯·è®¿é—®é“¶è¡Œå®˜ç½‘äº†è§£äº§å“ä¼˜åŠ¿",
            'fees_charges': "è¯·è”ç³»é“¶è¡Œäº†è§£è´¹ç”¨è¯¦æƒ…",
            'tenure': tenure,
            'rate': rate,
            'application_form_url': application_form,
            'product_disclosure_url': disclosure,
            'terms_conditions_url': terms,
            'preferred_customer_type': preferred_customer,
            'institution_type': institution_type,
            'source_url': page_url,
            'pulled_at': datetime.now().isoformat()
        }
    
    def _extract_single_product(
        self,
        soup: BeautifulSoup,
        url: str,
        bank_name: str,
        institution_type: str
    ) -> Optional[Dict[str, Any]]:
        """å°†æ•´ä¸ªé¡µé¢ä½œä¸ºä¸€ä¸ªäº§å“æå–"""
        
        # äº§å“åç§°ï¼ˆé¡µé¢æ ‡é¢˜ï¼‰
        product_name = None
        h1 = soup.find('h1')
        if h1:
            product_name = h1.get_text(strip=True)
        else:
            title = soup.find('title')
            if title:
                product_name = title.get_text(strip=True)
        
        if not product_name:
            return None
        
        text = soup.get_text()
        
        return {
            'company': bank_name,
            'loan_type': self._classify_loan_type(product_name + ' ' + text),
            'product_name': product_name,
            'required_doc': "è¯·è”ç³»é“¶è¡Œäº†è§£æ‰€éœ€æ–‡ä»¶",
            'features': "è¯·è®¿é—®é“¶è¡Œå®˜ç½‘äº†è§£äº§å“ç‰¹ç‚¹",
            'benefits': "è¯·è®¿é—®é“¶è¡Œå®˜ç½‘äº†è§£äº§å“ä¼˜åŠ¿",
            'fees_charges': "è¯·è”ç³»é“¶è¡Œäº†è§£è´¹ç”¨è¯¦æƒ…",
            'tenure': self._extract_tenure(text),
            'rate': self._extract_rate(text),
            'application_form_url': None,
            'product_disclosure_url': None,
            'terms_conditions_url': None,
            'preferred_customer_type': self._determine_customer_preference(text, product_name),
            'institution_type': institution_type,
            'source_url': url,
            'pulled_at': datetime.now().isoformat()
        }
    
    def _extract_rate(self, text: str) -> str:
        """æå–åˆ©ç‡"""
        patterns = [
            r'(\d+\.?\d*)\s*%\s*(p\.a\.|per\s+annum)?',
            r'(BR|BLR|SBR)\s*[\+\-]\s*(\d+\.?\d*)\s*%?',
            r'from\s+(\d+\.?\d*)\s*%',
            r'as\s+low\s+as\s+(\d+\.?\d*)\s*%',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(0)
        
        return "Contact Bank"
    
    def _extract_tenure(self, text: str) -> str:
        """æå–æœŸé™"""
        patterns = [
            r'up\s+to\s+(\d+)\s*(years?|months?)',
            r'(\d+)\s*(years?|tahun)',
            r'(\d+)\s*-\s*(\d+)\s*(years?)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(0)
        
        return "Contact Bank"
    
    def _classify_loan_type(self, text: str) -> str:
        """åˆ†ç±»è´·æ¬¾ç±»å‹"""
        text_lower = text.lower()
        
        if any(kw in text_lower for kw in ['home', 'housing', 'mortgage', 'property', 'rumah']):
            return 'HOME'
        elif any(kw in text_lower for kw in ['refinance', 'refinancing']):
            return 'REFINANCE'
        elif any(kw in text_lower for kw in ['debt consolidation', 'consolidation']):
            return 'DEBT_CONSOLIDATION'
        elif any(kw in text_lower for kw in ['business', 'sme', 'commercial', 'enterprise']):
            return 'BUSINESS'
        elif any(kw in text_lower for kw in ['personal', 'cash', 'peribadi']):
            return 'PERSONAL'
        else:
            return 'OTHER'
    
    def _find_pdf_link(self, container, keywords: List[str]) -> Optional[str]:
        """æŸ¥æ‰¾PDFé“¾æ¥"""
        links = container.find_all('a', href=re.compile(r'\.pdf$', re.I))
        
        for link in links:
            text = link.get_text().lower()
            href = link.get('href', '').lower()
            
            if any(kw in text or kw in href for kw in keywords):
                return link.get('href')
        
        return None
    
    def _determine_customer_preference(self, text: str, product_name: str) -> str:
        """åˆ¤æ–­å®¢æˆ·åå¥½"""
        text_lower = (text + ' ' + product_name).lower()
        
        business_score = sum(1 for kw in ['business', 'sme', 'entrepreneur', 'self-employed'] if kw in text_lower)
        salaried_score = sum(1 for kw in ['salaried', 'employee', 'fixed income', 'payslip'] if kw in text_lower)
        
        if business_score > salaried_score:
            return "ä¼ä¸šå®¢æˆ· (Business/Self-Employed)"
        elif salaried_score > 0:
            return "æ‰“å·¥æ—/å›ºå®šæ”¶å…¥å®¢æˆ· (Salaried/Fixed Income)"
        else:
            return "æ‰€æœ‰å®¢æˆ·ç±»å‹ (All Customer Types)"
    
    def scrape_bank_comprehensive(
        self,
        bank_name: str,
        website: str,
        institution_type: str
    ) -> List[Dict[str, Any]]:
        """
        å®Œæ•´çˆ¬å–å•ä¸ªé“¶è¡Œçš„æ‰€æœ‰è´·æ¬¾äº§å“
        
        æµç¨‹ï¼š
        1. æ¢ç´¢å¯¼èˆªèœå•ï¼Œæ‰¾åˆ°æ‰€æœ‰è´·æ¬¾é¡µé¢
        2. è®¿é—®æ¯ä¸ªé¡µé¢ï¼Œæå–äº§å“è¯¦æƒ…
        3. è¿”å›æ‰€æœ‰äº§å“
        """
        logger.info(f"ğŸ¦ å¼€å§‹çˆ¬å–: {bank_name}")
        
        all_products = []
        
        # æ­¥éª¤1: æ¢ç´¢å¯¼èˆªï¼Œæ‰¾åˆ°æ‰€æœ‰è´·æ¬¾é¡µé¢
        loan_pages = self.explore_navigation_menu(bank_name, website)
        
        if not loan_pages:
            logger.warning(f"   âš ï¸ {bank_name}: æœªæ‰¾åˆ°è´·æ¬¾é¡µé¢")
            return []
        
        logger.info(f"   æ‰¾åˆ° {len(loan_pages)} ä¸ªè´·æ¬¾é¡µé¢")
        
        # æ­¥éª¤2: è®¿é—®æ¯ä¸ªé¡µé¢ï¼Œæå–äº§å“
        for page_url in loan_pages[:5]:  # é™åˆ¶æœ€å¤š5ä¸ªé¡µé¢
            logger.info(f"   ğŸ“„ è®¿é—®: {page_url}")
            
            products = self.extract_product_details_from_page(
                page_url, bank_name, institution_type
            )
            
            if products:
                logger.info(f"      âœ… æ‰¾åˆ° {len(products)} ä¸ªäº§å“")
                all_products.extend(products)
            
            time.sleep(1)  # ç¤¼è²Œæ€§å»¶è¿Ÿ
        
        logger.info(f"âœ… {bank_name}: å…±è·å– {len(all_products)} ä¸ªäº§å“")
        return all_products


# å…¨å±€å•ä¾‹
enhanced_scraper = EnhancedBankScraper()

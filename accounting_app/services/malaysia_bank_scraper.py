"""
é©¬æ¥è¥¿äºšé“¶è¡Œè´·æ¬¾æ•°æ®é‡‡é›†å™¨ - ä½¿ç”¨ScrapingDog API
"""
import os
import json
import logging
import time
import csv
from typing import List, Dict, Any
from datetime import datetime
from bs4 import BeautifulSoup
import re

logger = logging.getLogger(__name__)

class MalaysiaBankScraper:
    """ä½¿ç”¨ScrapingDogé‡‡é›†é©¬æ¥è¥¿äºš64å®¶é“¶è¡Œçš„è´·æ¬¾äº§å“æ•°æ®"""
    
    def __init__(self):
        from accounting_app.services.scrapingdog_client import scrapingdog_client
        self.client = scrapingdog_client
        
        config_path = os.path.join(
            os.path.dirname(__file__),
            '../data/malaysia_financial_institutions.json'
        )
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        
        self.institutions = []
        for category in ['commercial_banks', 'islamic_banks', 'development_banks', 
                        'digital_banks', 'p2p_platforms', 'non_bank_credit']:
            self.institutions.extend(self.config.get(category, []))
        
        logger.info(f"âœ… å·²åŠ è½½ {len(self.institutions)} å®¶é‡‘èæœºæ„")
    
    def scrape_bank(self, bank: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        çˆ¬å–å•ä¸ªé“¶è¡Œçš„æ‰€æœ‰è´·æ¬¾äº§å“
        
        Returns:
            äº§å“åˆ—è¡¨ï¼Œæ¯ä¸ªäº§å“åŒ…å«12ä¸ªå­—æ®µ
        """
        products = []
        bank_name = bank['name']
        website = bank['website']
        
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ¦ å¼€å§‹çˆ¬å–: {bank_name}")
        logger.info(f"ğŸŒ ç½‘ç«™: {website}")
        
        loan_paths = [
            '/personal/loans',
            '/loans',
            '/financing',
            '/products/loans',
            '/en/personal/loans',
            '/personal-loan',
            '/home-loan',
            '/business-loan',
        ]
        
        for path in loan_paths:
            url = f"{website}{path}"
            
            html = self.client.scrape_url(url, dynamic=False)
            
            if html and len(html) > 1000:
                logger.info(f"  âœ… è·å–åˆ°å†…å®¹: {len(html)} å­—ç¬¦")
                
                extracted = self._extract_products_from_html(html, bank, url)
                products.extend(extracted)
                
                if len(products) > 0:
                    logger.info(f"  ğŸ“Š æ‰¾åˆ° {len(extracted)} ä¸ªäº§å“")
                    break
            else:
                logger.info(f"  â­ï¸  è·³è¿‡: {url}")
            
            time.sleep(2)
        
        if len(products) == 0:
            logger.warning(f"  âš ï¸  {bank_name}: æœªæ‰¾åˆ°è´·æ¬¾äº§å“")
        else:
            logger.info(f"  âœ… {bank_name}: å…±æ‰¾åˆ° {len(products)} ä¸ªäº§å“")
        
        return products
    
    def _extract_products_from_html(self, html: str, bank: Dict, source_url: str) -> List[Dict[str, Any]]:
        """ä»HTMLä¸­æå–äº§å“æ•°æ®"""
        soup = BeautifulSoup(html, 'html.parser')
        products = []
        
        title_tags = soup.find_all(['h1', 'h2', 'h3', 'h4'])
        
        loan_keywords = {
            'personal': ['personal loan', 'personal financing', 'cash'],
            'home': ['home loan', 'housing', 'mortgage', 'property'],
            'business': ['business loan', 'SME', 'corporate', 'commercial'],
            'auto': ['car loan', 'auto', 'vehicle'],
            'other': ['debt consolidation', 'refinance', 'education']
        }
        
        text_blocks = soup.find_all(['div', 'section', 'article'], class_=re.compile(r'product|loan|financing', re.I))
        
        for block in text_blocks[:10]:
            text = block.get_text().lower()
            
            for loan_type, keywords in loan_keywords.items():
                if any(kw in text for kw in keywords):
                    
                    rate_match = re.search(r'(\d+\.?\d*)\s*%', text)
                    amount_match = re.search(r'RM\s*([\d,]+)', text)
                    tenure_match = re.search(r'(\d+)\s*(?:year|tahun)', text)
                    
                    product = {
                        'company': bank['name'],
                        'loan_type': loan_type.upper(),
                        'product_name': self._clean_text(block.find(['h1', 'h2', 'h3', 'h4']).get_text() if block.find(['h1', 'h2', 'h3', 'h4']) else f"{loan_type} Loan"),
                        'required_doc': self._extract_documents(text),
                        'features': self._extract_features(block),
                        'benefits': self._extract_benefits(block),
                        'fees_charges': self._extract_fees(text),
                        'tenure': f"{tenure_match.group(1)} years" if tenure_match else "æŸ¥è¯¢é“¶è¡Œ",
                        'rate': f"{rate_match.group(1)}% p.a." if rate_match else "æŸ¥è¯¢é“¶è¡Œ",
                        'application_form_url': self._find_link(block, ['apply', 'application']),
                        'product_disclosure_url': self._find_link(block, ['pds', 'disclosure', 'product disclosure']),
                        'terms_conditions_url': self._find_link(block, ['terms', 'conditions', 't&c', 'tnc']),
                        'preferred_customer_type': self._extract_eligibility(text),
                        'institution_type': bank['type'],
                        'source_url': source_url,
                        'pulled_at': datetime.now().isoformat()
                    }
                    
                    products.append(product)
                    break
        
        return products[:5]
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        return ' '.join(text.split()).strip()
    
    def _extract_documents(self, text: str) -> str:
        """æå–æ‰€éœ€æ–‡ä»¶"""
        docs = []
        doc_keywords = ['MyKad', 'NRIC', 'IC', 'payslip', 'salary', 'EPF', 'bank statement', 'income']
        for keyword in doc_keywords:
            if keyword.lower() in text:
                docs.append(keyword)
        return ', '.join(docs) if docs else "è¯·è”ç³»é“¶è¡Œ"
    
    def _extract_features(self, block) -> str:
        """æå–ç‰¹ç‚¹"""
        features = []
        feature_keywords = ['flexible', 'fast', 'easy', 'competitive', 'low rate', 'no fee']
        text = block.get_text().lower()
        for kw in feature_keywords:
            if kw in text:
                features.append(kw.title())
        return ', '.join(features[:3]) if features else "æŸ¥è¯¢é“¶è¡Œ"
    
    def _extract_benefits(self, block) -> str:
        """æå–ä¼˜åŠ¿"""
        benefits = []
        ul_tags = block.find_all('ul')
        for ul in ul_tags:
            items = ul.find_all('li')
            benefits.extend([self._clean_text(li.get_text()) for li in items[:3]])
        return ' | '.join(benefits[:3]) if benefits else "æŸ¥è¯¢é“¶è¡Œ"
    
    def _extract_fees(self, text: str) -> str:
        """æå–è´¹ç”¨"""
        fee_match = re.search(r'(?:fee|charge|stamp duty).*?(\d+\.?\d*%?)', text, re.I)
        return fee_match.group(0) if fee_match else "æŸ¥è¯¢é“¶è¡Œ"
    
    def _extract_eligibility(self, text: str) -> str:
        """æå–å€Ÿè´·äººå–œå¥½"""
        conditions = []
        if 'monthly income' in text or 'rm' in text:
            income_match = re.search(r'RM\s*([\d,]+)', text)
            if income_match:
                conditions.append(f"Min income RM{income_match.group(1)}")
        if 'age' in text:
            conditions.append("Age requirements apply")
        if 'employed' in text or 'salary' in text:
            conditions.append("Salaried employees")
        return ', '.join(conditions) if conditions else "æŸ¥è¯¢é“¶è¡Œ"
    
    def _find_link(self, block, keywords: List[str]) -> str:
        """æŸ¥æ‰¾ç›¸å…³é“¾æ¥"""
        links = block.find_all('a', href=True)
        for link in links:
            link_text = link.get_text().lower()
            href = link['href']
            if any(kw in link_text or kw in href.lower() for kw in keywords):
                if href.startswith('http'):
                    return href
                elif href.startswith('/'):
                    return f"{block.find_parent().get('data-url', '')}{href}"
        return ""
    
    def scrape_top_banks(self, limit: int = 5) -> List[Dict[str, Any]]:
        """çˆ¬å–å‰Nå®¶é“¶è¡Œ"""
        all_products = []
        
        for i, bank in enumerate(self.institutions[:limit], 1):
            logger.info(f"\nğŸ“ è¿›åº¦: {i}/{limit}")
            products = self.scrape_bank(bank)
            all_products.extend(products)
            
            time.sleep(3)
        
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ‰ é‡‡é›†å®Œæˆï¼å…±è·å– {len(all_products)} ä¸ªäº§å“")
        
        return all_products
    
    def export_to_csv(self, products: List[Dict[str, Any]], filename: str):
        """å¯¼å‡ºä¸ºCSV"""
        if not products:
            logger.warning("æ²¡æœ‰äº§å“æ•°æ®å¯å¯¼å‡º")
            return
        
        fieldnames = [
            'company', 'loan_type', 'product_name', 'required_doc',
            'features', 'benefits', 'fees_charges', 'tenure', 'rate',
            'application_form_url', 'product_disclosure_url', 
            'terms_conditions_url', 'preferred_customer_type',
            'institution_type', 'source_url', 'pulled_at'
        ]
        
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(products)
        
        logger.info(f"âœ… æ•°æ®å·²å¯¼å‡ºåˆ°: {filename}")

malaysia_scraper = MalaysiaBankScraper()

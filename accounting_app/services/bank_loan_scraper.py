"""
é©¬æ¥è¥¿äºšä¸»è¦é“¶è¡Œè´·æ¬¾äº§å“çˆ¬è™«
æŠ“å–çœŸå®çš„è´·æ¬¾äº§å“ä¿¡æ¯
"""
import requests
import logging
from typing import List, Dict, Any, Optional
from bs4 import BeautifulSoup
from datetime import datetime
import re

logger = logging.getLogger(__name__)


class BankLoanScraper:
    """é“¶è¡Œè´·æ¬¾äº§å“çˆ¬è™«åŸºç±»"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
    
    def extract_rate(self, text: str) -> Optional[str]:
        """ä»æ–‡æœ¬ä¸­æå–åˆ©ç‡æ•°å­—"""
        match = re.search(r'(\d+\.?\d*)%', text)
        return match.group(0) if match else None
    
    def scrape(self) -> List[Dict[str, Any]]:
        """çˆ¬å–è´·æ¬¾äº§å“ï¼ˆå­ç±»å®ç°ï¼‰"""
        raise NotImplementedError


class MaybankScraper(BankLoanScraper):
    """Maybank è´·æ¬¾äº§å“çˆ¬è™«"""
    
    LOAN_PAGES = {
        'personal': 'https://www.maybank2u.com.my/en/personal/loans/personal-financing.page',
        'home': 'https://www.maybank2u.com.my/en/personal/loans/home-financing.page',
        'auto': 'https://www.maybank2u.com.my/en/personal/loans/hire-purchase.page',
    }
    
    def scrape(self) -> List[Dict[str, Any]]:
        """çˆ¬å–Maybankè´·æ¬¾äº§å“"""
        products = []
        
        for loan_type, url in self.LOAN_PAGES.items():
            try:
                logger.info(f"æ­£åœ¨çˆ¬å– Maybank {loan_type} loan...")
                response = self.session.get(url, timeout=15)
                
                if response.status_code != 200:
                    logger.warning(f"âš ï¸ Maybank {loan_type} é¡µé¢è¿”å› {response.status_code}")
                    continue
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ç¤ºä¾‹è§£æé€»è¾‘ï¼ˆéœ€æ ¹æ®å®é™…ç½‘é¡µç»“æ„è°ƒæ•´ï¼‰
                # è¿™é‡Œæä¾›ä¸€ä¸ªé€šç”¨æ¡†æ¶
                product_cards = soup.find_all('div', class_=re.compile('product|loan|card'))
                
                for card in product_cards[:5]:  # é™åˆ¶æ¯ç±»æœ€å¤š5ä¸ªäº§å“
                    title = card.find(['h2', 'h3', 'h4'])
                    rate_elem = card.find(text=re.compile(r'\d+\.?\d*%'))
                    
                    if title:
                        product = {
                            'source': 'maybank',
                            'bank': 'Maybank',
                            'product': title.get_text(strip=True),
                            'type': loan_type.upper(),
                            'rate': self.extract_rate(rate_elem) if rate_elem else 'Contact Bank',
                            'summary': 'è¯·è®¿é—® Maybank å®˜ç½‘äº†è§£è¯¦æƒ…',
                            'url': url,
                            'pulled_at': datetime.now().isoformat()
                        }
                        products.append(product)
                
                logger.info(f"âœ… Maybank {loan_type}: æ‰¾åˆ° {len(products)} ä¸ªäº§å“")
                
            except Exception as e:
                logger.error(f"âŒ Maybank {loan_type} çˆ¬å–å¤±è´¥: {e}")
        
        return products


class CIMBScraper(BankLoanScraper):
    """CIMB è´·æ¬¾äº§å“çˆ¬è™«"""
    
    LOAN_PAGES = {
        'personal': 'https://www.cimb.com.my/en/personal/products/loans/personal-loans.html',
        'home': 'https://www.cimb.com.my/en/personal/products/loans/home-loans.html',
    }
    
    def scrape(self) -> List[Dict[str, Any]]:
        """çˆ¬å–CIMBè´·æ¬¾äº§å“"""
        products = []
        
        for loan_type, url in self.LOAN_PAGES.items():
            try:
                logger.info(f"æ­£åœ¨çˆ¬å– CIMB {loan_type} loan...")
                response = self.session.get(url, timeout=15)
                
                if response.status_code != 200:
                    logger.warning(f"âš ï¸ CIMB {loan_type} é¡µé¢è¿”å› {response.status_code}")
                    continue
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # è§£æCIMBç½‘é¡µç»“æ„
                product_sections = soup.find_all(['div', 'section'], class_=re.compile('product|loan'))
                
                for section in product_sections[:5]:
                    title = section.find(['h2', 'h3', 'h4'])
                    
                    if title:
                        product = {
                            'source': 'cimb',
                            'bank': 'CIMB Bank',
                            'product': title.get_text(strip=True),
                            'type': loan_type.upper(),
                            'rate': 'Contact Bank',
                            'summary': 'è¯·è®¿é—® CIMB å®˜ç½‘äº†è§£è¯¦æƒ…',
                            'url': url,
                            'pulled_at': datetime.now().isoformat()
                        }
                        products.append(product)
                
                logger.info(f"âœ… CIMB {loan_type}: æ‰¾åˆ° {len(products)} ä¸ªäº§å“")
                
            except Exception as e:
                logger.error(f"âŒ CIMB {loan_type} çˆ¬å–å¤±è´¥: {e}")
        
        return products


class PublicBankScraper(BankLoanScraper):
    """Public Bank è´·æ¬¾äº§å“çˆ¬è™«"""
    
    LOAN_PAGES = {
        'personal': 'https://www.pbebank.com/personal/loans/personal-loans.html',
        'home': 'https://www.pbebank.com/personal/loans/home-loans.html',
    }
    
    def scrape(self) -> List[Dict[str, Any]]:
        """çˆ¬å–Public Bankè´·æ¬¾äº§å“"""
        products = []
        
        for loan_type, url in self.LOAN_PAGES.items():
            try:
                logger.info(f"æ­£åœ¨çˆ¬å– Public Bank {loan_type} loan...")
                response = self.session.get(url, timeout=15)
                
                if response.status_code != 200:
                    logger.warning(f"âš ï¸ Public Bank {loan_type} é¡µé¢è¿”å› {response.status_code}")
                    continue
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # è§£æPublic Bankç½‘é¡µç»“æ„
                titles = soup.find_all(['h2', 'h3'], class_=re.compile('product|title|loan'))
                
                for title in titles[:5]:
                    product = {
                        'source': 'public-bank',
                        'bank': 'Public Bank',
                        'product': title.get_text(strip=True),
                        'type': loan_type.upper(),
                        'rate': 'Contact Bank',
                        'summary': 'è¯·è®¿é—® Public Bank å®˜ç½‘äº†è§£è¯¦æƒ…',
                        'url': url,
                        'pulled_at': datetime.now().isoformat()
                    }
                    products.append(product)
                
                logger.info(f"âœ… Public Bank {loan_type}: æ‰¾åˆ° {len(products)} ä¸ªäº§å“")
                
            except Exception as e:
                logger.error(f"âŒ Public Bank {loan_type} çˆ¬å–å¤±è´¥: {e}")
        
        return products


class BankLoanAggregator:
    """é“¶è¡Œè´·æ¬¾æ•°æ®èšåˆå™¨"""
    
    def __init__(self):
        self.scrapers = [
            MaybankScraper(),
            CIMBScraper(),
            PublicBankScraper(),
        ]
    
    def scrape_all_banks(self) -> List[Dict[str, Any]]:
        """
        çˆ¬å–æ‰€æœ‰é“¶è¡Œçš„è´·æ¬¾äº§å“
        
        Returns:
            [
                {
                    'source': 'maybank',
                    'bank': 'Maybank',
                    'product': 'Personal Loan-i',
                    'type': 'PERSONAL',
                    'rate': '6.88%',
                    'summary': '...',
                    'url': 'https://...',
                    'pulled_at': '2025-11-09T04:00:00'
                },
                ...
            ]
        """
        all_products = []
        
        for scraper in self.scrapers:
            try:
                products = scraper.scrape()
                all_products.extend(products)
                logger.info(f"âœ… {scraper.__class__.__name__}: æˆåŠŸè·å– {len(products)} ä¸ªäº§å“")
            except Exception as e:
                logger.error(f"âŒ {scraper.__class__.__name__} å¤±è´¥: {e}")
        
        logger.info(f"ğŸ‰ æ€»è®¡çˆ¬å– {len(all_products)} ä¸ªé“¶è¡Œè´·æ¬¾äº§å“")
        return all_products
    
    def validate_products(self, products: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        éªŒè¯äº§å“æ•°æ®å®Œæ•´æ€§
        
        Args:
            products: åŸå§‹äº§å“åˆ—è¡¨
        
        Returns:
            éªŒè¯é€šè¿‡çš„äº§å“åˆ—è¡¨
        """
        valid_products = []
        required_fields = ['source', 'bank', 'product', 'type']
        
        for product in products:
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            if all(product.get(field) for field in required_fields):
                # æ ‡å‡†åŒ–åˆ©ç‡æ ¼å¼
                if product.get('rate') and '%' not in product['rate']:
                    product['rate'] = f"{product['rate']}%"
                
                # ç¡®ä¿æœ‰summary
                if not product.get('summary'):
                    product['summary'] = f"è¯·è®¿é—® {product['bank']} å®˜ç½‘äº†è§£è¯¦æƒ…"
                
                valid_products.append(product)
            else:
                logger.warning(f"âš ï¸ äº§å“æ•°æ®ä¸å®Œæ•´ï¼Œè·³è¿‡: {product.get('product', 'Unknown')}")
        
        logger.info(f"âœ… éªŒè¯é€šè¿‡ {len(valid_products)}/{len(products)} ä¸ªäº§å“")
        return valid_products


# å…¨å±€å•ä¾‹
bank_aggregator = BankLoanAggregator()

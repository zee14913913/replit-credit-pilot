#!/usr/bin/env python3
"""
è¶…å¼ºå®¹é”™çˆ¬è™« - æ¯ä¸ªå…¬å¸ç‹¬ç«‹è¿›ç¨‹ï¼Œç»ä¸å´©æºƒ
"""
import csv, time, requests, psycopg2, os, re, signal
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from psycopg2.extras import execute_values
from datetime import datetime
from contextlib import contextmanager

DATABASE_URL = os.getenv('DATABASE_URL')
CSV_FILE = "/home/runner/workspace/attached_assets/New é©¬æ¥è¥¿äºšè´·æ¬¾æœºæ„ä¸å¹³å°å…¨å®˜ç½‘_å®Œæ•´ç‰ˆ.csv_1762667764316.csv"

print("=" * 80)
print("ğŸ›¡ï¸  è¶…å¼ºå®¹é”™çˆ¬è™«å¯åŠ¨ - æ¯å®¶å…¬å¸ç‹¬ç«‹å¤„ç†ï¼Œç»ä¸å´©æºƒ")
print("=" * 80)

@contextmanager
def timeout(seconds):
    """è¶…æ—¶ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    def timeout_handler(signum, frame):
        raise TimeoutError(f"æ“ä½œè¶…æ—¶({seconds}ç§’)")
    
    old_handler = signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(seconds)
    try:
        yield
    finally:
        signal.alarm(0)
        signal.signal(signal.SIGALRM, old_handler)

def get_completed():
    try:
        con = psycopg2.connect(DATABASE_URL)
        cur = con.cursor()
        cur.execute("SELECT DISTINCT company FROM loan_products_ultimate")
        completed = set(r[0] for r in cur.fetchall())
        cur.close()
        con.close()
        return completed
    except:
        return set()

def load_institutions():
    inst = []
    with open(CSV_FILE, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        next(reader)
        for idx, row in enumerate(reader, 1):
            if len(row) >= 2:
                inst.append({'order': idx, 'name': row[0].strip(), 'url': row[1].strip()})
    return inst

def classify(text):
    t = text.lower()
    if 'credit' in t or 'kad' in t or 'card' in t or 'visa' in t or 'master' in t: return 'CREDIT_CARD'
    elif 'home' in t or 'housing' in t or 'mortgage' in t or 'property' in t: return 'HOME_LOAN'
    elif 'personal' in t or 'cash' in t: return 'PERSONAL_LOAN'
    elif 'car' in t or 'auto' in t or 'vehicle' in t: return 'CAR_LOAN'
    elif 'business' in t or 'sme' in t or 'commercial' in t: return 'SME_LOAN'
    elif 'deposit' in t or 'fd' in t or 'simpanan' in t: return 'FIXED_DEPOSIT'
    elif 'overdraft' in t or 'od' in t: return 'OVERDRAFT'
    elif 'refinanc' in t: return 'REFINANCE'
    else: return 'OTHER'

def scrape_company_safe(order, name, url):
    """å®‰å…¨çˆ¬å–å•ä¸ªå…¬å¸ï¼Œå¸¦å¤šé‡ä¿æŠ¤"""
    print(f"\n[{order}/67] {name}")
    print(f"   ğŸŒ {url[:70]}")
    
    products = []
    
    try:
        with timeout(90):  # æ€»è¶…æ—¶90ç§’
            session = requests.Session()
            session.headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
            
            # é™åˆ¶çˆ¬å–çš„URLæ•°é‡
            paths = [
                '/', '/personal', '/business', '/sme',
                '/cards', '/loans', '/deposits', '/financing',
                '/personal/cards', '/personal/loans', '/personal/deposits',
                '/business/loans', '/sme/loans',
                '/credit-cards', '/personal-loan', '/home-loan', '/fixed-deposit'
            ]
            
            for p in paths[:15]:  # åªçˆ¬15ä¸ªURL
                try:
                    full_url = urljoin(url, p)
                    r = session.get(full_url, timeout=6)
                    
                    if r.status_code != 200:
                        continue
                    
                    soup = BeautifulSoup(r.text[:50000], 'html.parser')  # åªè§£æå‰50KB
                    
                    # æŸ¥æ‰¾äº§å“æ ‡é¢˜
                    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'strong'])[:30]:
                        txt = tag.get_text(strip=True)
                        if 6 < len(txt) < 100:
                            keywords = ['card', 'loan', 'deposit', 'financing', 'kredit', 'pinjaman']
                            if any(k in txt.lower() for k in keywords):
                                rate = 'è¯·è”ç³»é“¶è¡Œ'
                                m = re.search(r'(\d+\.?\d*)\s*%', soup.get_text()[:3000])
                                if m: rate = m.group(0)
                                
                                products.append({
                                    'company': name,
                                    'name': txt,
                                    'type': classify(txt),
                                    'rate': rate,
                                    'url': full_url
                                })
                    
                    time.sleep(0.1)
                except:
                    pass
        
        # å»é‡
        seen = set()
        unique = []
        for p in products:
            k = f"{p['company']}|{p['name']}"
            if k not in seen:
                seen.add(k)
                unique.append(p)
        
        print(f"   âœ… {len(unique)} ä¸ªäº§å“")
        return unique
        
    except TimeoutError:
        print(f"   â±ï¸  è¶…æ—¶90ç§’")
        return products
    except Exception as e:
        print(f"   âŒ {str(e)[:40]}")
        return []

def save_to_db(products):
    if not products:
        return 0
    
    try:
        con = psycopg2.connect(DATABASE_URL)
        cur = con.cursor()
        
        sql = """
            INSERT INTO loan_products_ultimate(
                company, loan_type, product_name, required_doc, features, benefits,
                fees_charges, tenure, rate, application_form_url, product_disclosure_url,
                terms_conditions_url, preferred_customer_type, source_url, scraped_at
            ) VALUES %s
            ON CONFLICT (company, product_name) DO NOTHING
        """
        
        items = [(
            p['company'], p['type'], p['name'], 'è¯·è®¿é—®é“¶è¡Œå®˜ç½‘', 'è¯·è®¿é—®é“¶è¡Œå®˜ç½‘', 'è¯·è®¿é—®é“¶è¡Œå®˜ç½‘',
            'è¯·è”ç³»é“¶è¡Œ', 'è¯·è”ç³»é“¶è¡Œ', p['rate'], '', '', '', 'æ‰€æœ‰å®¢æˆ·', p['url'], datetime.now()
        ) for p in products]
        
        execute_values(cur, sql, items)
        con.commit()
        inserted = cur.rowcount
        cur.close()
        con.close()
        return inserted
    except Exception as e:
        print(f"   ğŸ’¥ æ•°æ®åº“é”™è¯¯: {str(e)[:30]}")
        return 0

# ä¸»ç¨‹åº
completed = get_completed()
all_inst = load_institutions()
remaining = [inst for inst in all_inst if inst['name'] not in completed]

print(f"\nğŸ“Š å·²å®Œæˆ: {len(completed)} å®¶")
print(f"ğŸ“‹ æ€»æœºæ„: {len(all_inst)} å®¶")
print(f"ğŸ¯ å‰©ä½™: {len(remaining)} å®¶\n")

total_new = 0
success_count = 0
failed = []

for inst in remaining:
    try:
        prods = scrape_company_safe(inst['order'], inst['name'], inst['url'])
        
        if prods:
            saved = save_to_db(prods)
            total_new += len(prods)
            success_count += 1
            print(f"   ğŸ’¾ å·²ä¿å­˜{saved}ä¸ª")
        else:
            failed.append(inst['name'])
            print(f"   âš ï¸  æ— æ•°æ®")
        
        time.sleep(0.5)
        
    except Exception as e:
        failed.append(inst['name'])
        print(f"   âŒ å¼‚å¸¸: {str(e)[:30]}")
        continue

print("\n" + "=" * 80)
print(f"ğŸ‰ çˆ¬å–å®Œæˆï¼")
print(f"   æˆåŠŸ: {success_count} å®¶å…¬å¸")
print(f"   æ–°å¢: {total_new} ä¸ªäº§å“")
print(f"   å¤±è´¥: {len(failed)} å®¶")
print("=" * 80)

# æœ€ç»ˆç»Ÿè®¡
try:
    con = psycopg2.connect(DATABASE_URL)
    cur = con.cursor()
    cur.execute("SELECT COUNT(*), COUNT(DISTINCT company) FROM loan_products_ultimate")
    r = cur.fetchone()
    if r:
        print(f"\nğŸ“Š æ•°æ®åº“æ€»è®¡: {r[0]} ä¸ªäº§å“ï¼Œ{r[1]} å®¶å…¬å¸")
    
    cur.execute("SELECT loan_type, COUNT(*) FROM loan_products_ultimate GROUP BY loan_type ORDER BY COUNT(*) DESC")
    print("\nğŸ“ˆ äº§å“åˆ†å¸ƒ:")
    for row in cur.fetchall():
        print(f"   {row[0]:15s}: {row[1]:4d}")
    
    cur.close()
    con.close()
except:
    pass

if failed:
    print(f"\nâš ï¸  æ— æ•°æ®å…¬å¸ ({len(failed)}):")
    for f in failed[:15]:
        print(f"   - {f}")

print("\nâœ… å®Œæˆï¼")

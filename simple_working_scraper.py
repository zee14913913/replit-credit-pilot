#!/usr/bin/env python3
"""
ç®€åŒ–å·¥ä½œçˆ¬è™« - ä»å‰©ä½™59å®¶æœºæ„ç»§ç»­
"""
print("=" * 80)
print("å¼€å§‹çˆ¬å–...")
print("=" * 80)

import csv, time, requests, psycopg2, os, re
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from psycopg2.extras import execute_values
from datetime import datetime

DATABASE_URL = os.getenv('DATABASE_URL')
CSV_FILE = "/home/runner/workspace/attached_assets/New é©¬æ¥è¥¿äºšè´·æ¬¾æœºæ„ä¸å¹³å°å…¨å®˜ç½‘_å®Œæ•´ç‰ˆ.csv_1762667764316.csv"

# è·å–å·²å®Œæˆçš„å…¬å¸
print("\nè·å–å·²å®Œæˆå…¬å¸...")
con = psycopg2.connect(DATABASE_URL)
cur = con.cursor()
cur.execute("SELECT DISTINCT company FROM loan_products_ultimate")
completed = set(r[0] for r in cur.fetchall())
cur.close()
con.close()
print(f"âœ… å·²å®Œæˆ: {len(completed)} å®¶")

# è¯»å–æ‰€æœ‰æœºæ„
print("è¯»å–æœºæ„åˆ—è¡¨...")
institutions = []
with open(CSV_FILE, 'r', encoding='utf-8') as f:
    reader = csv.reader(f)
    next(reader)
    for idx, row in enumerate(reader, 1):
        if len(row) >= 2:
            institutions.append({'order': idx, 'name': row[0].strip(), 'url': row[1].strip()})

print(f"âœ… æ€»æœºæ„: {len(institutions)} å®¶")

# è¿‡æ»¤å‰©ä½™
remaining = [inst for inst in institutions if inst['name'] not in completed]
print(f"ğŸ“Œ å‰©ä½™: {len(remaining)} å®¶\n")

# äº§å“åˆ†ç±»
def classify(text):
    t = text.lower()
    if any(k in t for k in ['credit card', 'kad kredit', 'visa', 'mastercard']): return 'CREDIT_CARD'
    elif any(k in t for k in ['home', 'housing', 'mortgage', 'property']): return 'HOME_LOAN'
    elif any(k in t for k in ['personal loan', 'cash']): return 'PERSONAL_LOAN'
    elif any(k in t for k in ['car', 'auto', 'vehicle']): return 'CAR_LOAN'
    elif any(k in t for k in ['business', 'sme', 'commercial']): return 'SME_LOAN'
    elif any(k in t for k in ['fixed deposit', 'fd', 'time deposit']): return 'FIXED_DEPOSIT'
    elif any(k in t for k in ['overdraft', 'od']): return 'OVERDRAFT'
    else: return 'OTHER'

# çˆ¬å–å•ä¸ªå…¬å¸
def scrape(order, name, url):
    print(f"\n[{order}/67] {name}")
    print(f"   {url}")
    
    products = []
    session = requests.Session()
    session.headers = {'User-Agent': 'Mozilla/5.0'}
    
    # URLåˆ—è¡¨
    paths = [
        '/', '/personal', '/business', '/sme', '/corporate',
        '/personal/cards', '/personal/loans', '/personal/financing', '/personal/fixed-deposit',
        '/business/loans', '/business/financing', '/sme/loans',
        '/cards', '/credit-cards', '/loans', '/financing', '/fixed-deposit', '/deposits', '/overdraft'
    ]
    
    for p in paths[:30]:  # é™åˆ¶30ä¸ªURL
        full_url = urljoin(url, p)
        try:
            r = session.get(full_url, timeout=8)
            if r.status_code != 200: continue
            
            soup = BeautifulSoup(r.text, 'html.parser')
            
            for h in soup.find_all(['h1', 'h2', 'h3', 'h4'])[:20]:
                txt = h.get_text(strip=True)
                if 5 < len(txt) < 120 and any(k in txt.lower() for k in ['card', 'loan', 'deposit', 'financing']):
                    rate = 'è¯·è”ç³»é“¶è¡Œ'
                    m = re.search(r'(\d+\.?\d*)\s*%', soup.get_text()[:3000])
                    if m: rate = m.group(0)
                    
                    products.append({
                        'company': name,
                        'name': txt,
                        'type': classify(txt),
                        'rate': rate,
                        'url': full_url
                    })
            
            time.sleep(0.2)
        except: pass
    
    # å»é‡
    seen = set()
    unique = []
    for p in products:
        k = f"{p['company']}_{p['name']}"
        if k not in seen:
            seen.add(k)
            unique.append(p)
    
    print(f"   âœ… {len(unique)} ä¸ªäº§å“")
    return unique

# ä¿å­˜åˆ°æ•°æ®åº“
def save(products):
    if not products: return
    
    con = psycopg2.connect(DATABASE_URL)
    cur = con.cursor()
    
    sql = """
        INSERT INTO loan_products_ultimate(
            company, loan_type, product_name, required_doc, features, benefits,
            fees_charges, tenure, rate, application_form_url, product_disclosure_url,
            terms_conditions_url, preferred_customer_type, source_url, scraped_at
        ) VALUES %s
        ON CONFLICT (company, product_name) DO NOTHING
    """
    
    items = [(
        p['company'], p['type'], p['name'], 'è¯·è®¿é—®é“¶è¡Œå®˜ç½‘', 'è¯·è®¿é—®é“¶è¡Œå®˜ç½‘', 'è¯·è®¿é—®é“¶è¡Œå®˜ç½‘',
        'è¯·è”ç³»é“¶è¡Œ', 'è¯·è”ç³»é“¶è¡Œ', p['rate'], '', '', '', 'æ‰€æœ‰å®¢æˆ·', p['url'], datetime.now()
    ) for p in products]
    
    execute_values(cur, sql, items)
    con.commit()
    cur.close()
    con.close()

# ä¸»å¾ªç¯
total = 0
for inst in remaining:
    try:
        prods = scrape(inst['order'], inst['name'], inst['url'])
        if prods:
            save(prods)
            total += len(prods)
            print(f"   ğŸ’¾ å·²ä¿å­˜")
    except Exception as e:
        print(f"   âŒ é”™è¯¯: {str(e)[:40]}")
    
    time.sleep(1)

print("\n" + "=" * 80)
print(f"ğŸ‰ å®Œæˆï¼æ–°å¢ {total} ä¸ªäº§å“")
print("=" * 80)

# æœ€ç»ˆç»Ÿè®¡
con = psycopg2.connect(DATABASE_URL)
cur = con.cursor()
cur.execute("SELECT COUNT(*), COUNT(DISTINCT company) FROM loan_products_ultimate")
r = cur.fetchone()
if r:
    print(f"ğŸ“Š æ•°æ®åº“: {r[0]} ä¸ªäº§å“ï¼Œ{r[1]} å®¶å…¬å¸")
cur.close()
con.close()

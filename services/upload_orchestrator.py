"""
Upload Intake Orchestratorï¼ˆä¸Šä¼ æ¥æ”¶ç¼–æ’å™¨ï¼‰
å¼ºåˆ¶æ€§æ–‡ä»¶å¤„ç†Pipeline - é˜²æ­¢å¥å¿˜æœºåˆ¶

Architectè®¾è®¡è¦æ±‚ï¼š
- å¼ºåˆ¶æ€§é˜¶æ®µåºåˆ—ï¼Œä¸èƒ½è·³è¿‡ä»»ä½•æ­¥éª¤
- æ¯ä¸ªé˜¶æ®µéƒ½æœ‰æ£€æŸ¥ç‚¹ï¼ˆCheckpointï¼‰
- æ‰€æœ‰çŠ¶æ€å˜æ›´éƒ½è®°å½•åˆ°audit log
- ç½®ä¿¡åº¦ä½äº0.98è‡ªåŠ¨è½¬äººå·¥å®¡æ ¸
"""
import os
import hashlib
import uuid
import sqlite3
import json
from datetime import datetime
from typing import Optional, Dict, Tuple, List
from pathlib import Path

class UploadOrchestrator:
    """
    ä¸Šä¼ ç¼–æ’å™¨ - å¼ºåˆ¶æ‰§è¡Œå®Œæ•´çš„æ–‡ä»¶å¤„ç†æµç¨‹
    
    Pipelineé˜¶æ®µï¼ˆå¼ºåˆ¶é¡ºåºï¼‰ï¼š
    1. File Receiptï¼ˆæ–‡ä»¶æ¥æ”¶ï¼‰â†’ PendingChecksum
    2. Checksum Validationï¼ˆæ ¡éªŒå’ŒéªŒè¯ï¼‰â†’ PendingParse
    3. Content Parsingï¼ˆå†…å®¹è§£æï¼‰â†’ PendingAttribution
    4. Entity Attributionï¼ˆå½’å±è¯†åˆ«ï¼‰â†’ PendingClassification
    5. Business Classificationï¼ˆä¸šåŠ¡åˆ†ç±»ï¼‰â†’ ApprovedForStorage
    6. Dual-Write Storageï¼ˆåŒå†™å­˜å‚¨ï¼‰â†’ StorageComplete
    7. Audit Loggingï¼ˆå®¡è®¡æ—¥å¿—ï¼‰
    
    ğŸš« ä»»ä½•é˜¶æ®µå¤±è´¥ â†’ Failed æˆ– PendingReview
    """
    
    # å¼ºåˆ¶æ€§ç½®ä¿¡åº¦é˜ˆå€¼
    MIN_CONFIDENCE_THRESHOLD = 0.98
    
    # å¼ºåˆ¶æ€§è§£æå­—æ®µ
    MANDATORY_PARSE_FIELDS = [
        'owner_name',
        'customer_code',
        'bank_name',
        'statement_date',
        'due_date',
        'statement_total',
        'minimum_payment'
    ]
    
    def __init__(self, db_path: str = 'db/smart_loan_manager.db'):
        self.db_path = db_path
        self.quarantine_dir = 'static/quarantine'
        os.makedirs(self.quarantine_dir, exist_ok=True)
    
    def get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        return sqlite3.connect(self.db_path)
    
    def _log_state_change(
        self,
        transaction_uuid: str,
        from_status: Optional[str],
        to_status: str,
        reason: str,
        metadata: Optional[Dict] = None
    ):
        """
        è®°å½•çŠ¶æ€å˜æ›´ï¼ˆå¼ºåˆ¶å®¡è®¡ï¼‰
        
        Architectè¦æ±‚ï¼šæ¯ä¸ªçŠ¶æ€å˜æ›´éƒ½å¿…é¡»è®°å½•
        """
        conn = self.get_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO upload_state_changes (
                transaction_uuid, from_status, to_status, reason, metadata
            ) VALUES (?, ?, ?, ?, ?)
        ''', (
            transaction_uuid,
            from_status,
            to_status,
            reason,
            json.dumps(metadata) if metadata else None
        ))
        
        conn.commit()
        conn.close()
        
        print(f"ğŸ“ çŠ¶æ€å˜æ›´: {from_status} â†’ {to_status} | {reason}")
    
    def _update_transaction_status(
        self,
        transaction_uuid: str,
        new_status: str,
        updates: Optional[Dict] = None
    ):
        """
        æ›´æ–°äº¤æ˜“çŠ¶æ€
        
        Args:
            transaction_uuid: äº¤æ˜“UUID
            new_status: æ–°çŠ¶æ€
            updates: å…¶ä»–è¦æ›´æ–°çš„å­—æ®µ
        """
        conn = self.get_connection()
        cursor = conn.cursor()
        
        # è·å–å½“å‰çŠ¶æ€
        cursor.execute('''
            SELECT status FROM upload_transactions
            WHERE transaction_uuid = ?
        ''', (transaction_uuid,))
        
        result = cursor.fetchone()
        old_status = result[0] if result else None
        
        # æ›´æ–°çŠ¶æ€
        update_fields = ['status = ?', 'updated_at = ?']
        values = [new_status, datetime.now().isoformat()]
        
        if updates:
            for key, value in updates.items():
                update_fields.append(f'{key} = ?')
                values.append(value)
        
        values.append(transaction_uuid)
        
        cursor.execute(f'''
            UPDATE upload_transactions
            SET {', '.join(update_fields)}
            WHERE transaction_uuid = ?
        ''', values)
        
        conn.commit()
        conn.close()
        
        # è®°å½•çŠ¶æ€å˜æ›´
        self._log_state_change(
            transaction_uuid,
            old_status,
            new_status,
            f"Status updated to {new_status}",
            updates
        )
    
    # ========================================
    # Stage 1: File Receiptï¼ˆæ–‡ä»¶æ¥æ”¶ï¼‰
    # ========================================
    
    def initiate_upload(
        self,
        file_path: str,
        original_filename: str,
        uploaded_by: Optional[str] = None,
        ip_address: Optional[str] = None
    ) -> str:
        """
        å¯åŠ¨ä¸Šä¼ äº‹åŠ¡
        
        Args:
            file_path: ä¸´æ—¶æ–‡ä»¶è·¯å¾„
            original_filename: åŸå§‹æ–‡ä»¶å
            uploaded_by: ä¸Šä¼ äºº
            ip_address: IPåœ°å€
            
        Returns:
            transaction_uuid
        """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # ç”Ÿæˆäº¤æ˜“UUID
        transaction_uuid = str(uuid.uuid4())
        
        # è·å–æ–‡ä»¶ä¿¡æ¯
        file_size = os.path.getsize(file_path)
        
        # ç§»åŠ¨åˆ°éš”ç¦»åŒº
        quarantine_path = os.path.join(
            self.quarantine_dir,
            f"{transaction_uuid}_{original_filename}"
        )
        import shutil
        shutil.copy2(file_path, quarantine_path)
        
        # åˆ›å»ºäº¤æ˜“è®°å½•
        conn = self.get_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO upload_transactions (
                transaction_uuid, original_filename, file_size,
                status, uploaded_by, ip_address
            ) VALUES (?, ?, ?, 'PendingChecksum', ?, ?)
        ''', (
            transaction_uuid, original_filename, file_size,
            uploaded_by, ip_address
        ))
        
        conn.commit()
        conn.close()
        
        # è®°å½•çŠ¶æ€å˜æ›´
        self._log_state_change(
            transaction_uuid,
            None,
            'PendingChecksum',
            'Upload initiated, file moved to quarantine',
            {'quarantine_path': quarantine_path}
        )
        
        print(f"âœ… ä¸Šä¼ äº‹åŠ¡å·²å¯åŠ¨: {transaction_uuid}")
        print(f"   æ–‡ä»¶: {original_filename}")
        print(f"   å¤§å°: {file_size} bytes")
        print(f"   éš”ç¦»åŒº: {quarantine_path}")
        
        return transaction_uuid
    
    # ========================================
    # Stage 2: Checksum Validationï¼ˆæ ¡éªŒå’ŒéªŒè¯ï¼‰
    # Checkpoint 1: é‡å¤æ£€æµ‹
    # ========================================
    
    def checkpoint_1_validate_checksum(self, transaction_uuid: str) -> Tuple[bool, Optional[str]]:
        """
        æ£€æŸ¥ç‚¹1ï¼šæ ¡éªŒå’ŒéªŒè¯ + é‡å¤æ£€æµ‹
        
        Architectè¦æ±‚ï¼š
        - å¿…é¡»è®¡ç®—æ–‡ä»¶MD5
        - æ£€æŸ¥æ˜¯å¦é‡å¤ä¸Šä¼ 
        - å¦‚æœé‡å¤ï¼Œæç¤ºç°æœ‰æ–‡ä»¶ä½ç½®
        
        Returns:
            (is_duplicate, existing_file_info)
        """
        print(f"\nğŸ” æ£€æŸ¥ç‚¹1ï¼šæ ¡éªŒå’ŒéªŒè¯...")
        
        conn = self.get_connection()
        cursor = conn.cursor()
        
        # è·å–äº¤æ˜“ä¿¡æ¯
        cursor.execute('''
            SELECT original_filename FROM upload_transactions
            WHERE transaction_uuid = ?
        ''', (transaction_uuid,))
        
        result = cursor.fetchone()
        if not result:
            conn.close()
            raise ValueError(f"äº¤æ˜“ä¸å­˜åœ¨: {transaction_uuid}")
        
        original_filename = result[0]
        
        # è·å–éš”ç¦»åŒºæ–‡ä»¶è·¯å¾„
        quarantine_path = os.path.join(
            self.quarantine_dir,
            f"{transaction_uuid}_{original_filename}"
        )
        
        # è®¡ç®—MD5
        md5_hash = self._calculate_md5(quarantine_path)
        
        # æ›´æ–°æ ¡éªŒå’Œ
        cursor.execute('''
            UPDATE upload_transactions
            SET file_checksum = ?
            WHERE transaction_uuid = ?
        ''', (md5_hash, transaction_uuid))
        
        # æ£€æŸ¥æ˜¯å¦é‡å¤ï¼ˆæŸ¥è¯¢file_registryï¼‰
        cursor.execute('''
            SELECT file_uuid, original_filename, file_path, upload_date
            FROM file_registry
            WHERE file_hash = ? AND status = 'active'
            ORDER BY upload_date DESC
            LIMIT 1
        ''', (md5_hash,))
        
        duplicate = cursor.fetchone()
        
        conn.commit()
        conn.close()
        
        if duplicate:
            # ğŸš« é‡å¤æ–‡ä»¶ï¼
            existing_info = {
                'file_uuid': duplicate[0],
                'filename': duplicate[1],
                'path': duplicate[2],
                'upload_date': duplicate[3]
            }
            
            print(f"âŒ é‡å¤æ–‡ä»¶æ£€æµ‹ï¼")
            print(f"   ç°æœ‰æ–‡ä»¶: {existing_info['filename']}")
            print(f"   ä½ç½®: {existing_info['path']}")
            print(f"   ä¸Šä¼ æ—¶é—´: {existing_info['upload_date']}")
            
            # æ ‡è®°ä¸ºå¤±è´¥
            self._update_transaction_status(
                transaction_uuid,
                'Failed',
                {
                    'failure_reason': f"Duplicate file detected: {existing_info['file_uuid']}",
                    'failure_stage': 'Checksum'
                }
            )
            
            return (True, json.dumps(existing_info))
        
        # âœ… æ ¡éªŒæˆåŠŸï¼Œè¿›å…¥ä¸‹ä¸€é˜¶æ®µ
        self._update_transaction_status(
            transaction_uuid,
            'PendingParse',
            {'file_checksum': md5_hash}
        )
        
        print(f"âœ… æ ¡éªŒæˆåŠŸï¼ŒMD5: {md5_hash[:16]}...")
        return (False, None)
    
    def _calculate_md5(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶MD5å“ˆå¸Œ"""
        md5_hash = hashlib.md5()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                md5_hash.update(chunk)
        return md5_hash.hexdigest()
    
    # ========================================
    # Stage 3: Content Parsingï¼ˆå†…å®¹è§£æï¼‰
    # Checkpoint 2: å¼ºåˆ¶å­—æ®µæå–
    # ========================================
    
    def checkpoint_2_parse_content(self, transaction_uuid: str, parser_service) -> Tuple[bool, Optional[Dict]]:
        """
        æ£€æŸ¥ç‚¹2ï¼šå†…å®¹è§£æ
        
        Architectè¦æ±‚ï¼š
        - å¿…é¡»æå–7ä¸ªå¼ºåˆ¶å­—æ®µ
        - owner_name, customer_code, bank_name
        - statement_date, due_date, statement_total, minimum_payment
        - ä»»ä½•å­—æ®µç¼ºå¤± â†’ äººå·¥å®¡æ ¸
        
        Args:
            transaction_uuid: äº¤æ˜“UUID
            parser_service: PDFè§£ææœåŠ¡
            
        Returns:
            (success, parsed_data)
        """
        print(f"\nğŸ” æ£€æŸ¥ç‚¹2ï¼šå†…å®¹è§£æ...")
        
        conn = self.get_connection()
        cursor = conn.cursor()
        
        # è·å–æ–‡ä»¶è·¯å¾„
        cursor.execute('''
            SELECT original_filename FROM upload_transactions
            WHERE transaction_uuid = ?
        ''', (transaction_uuid,))
        
        result = cursor.fetchone()
        original_filename = result[0]
        
        quarantine_path = os.path.join(
            self.quarantine_dir,
            f"{transaction_uuid}_{original_filename}"
        )
        
        # è°ƒç”¨è§£ææœåŠ¡
        try:
            parsed_data = parser_service.parse_pdf(quarantine_path)
            
            # éªŒè¯å¼ºåˆ¶å­—æ®µ
            missing_fields = []
            for field in self.MANDATORY_PARSE_FIELDS:
                if field not in parsed_data or not parsed_data[field]:
                    missing_fields.append(field)
            
            if missing_fields:
                # âš ï¸ å­—æ®µç¼ºå¤±ï¼Œè½¬äººå·¥å®¡æ ¸
                print(f"âš ï¸  ç¼ºå¤±å¼ºåˆ¶å­—æ®µ: {', '.join(missing_fields)}")
                
                self._update_transaction_status(
                    transaction_uuid,
                    'PendingReview',
                    {
                        'review_required': 1,
                        'review_reason': f"Missing mandatory fields: {', '.join(missing_fields)}",
                        'failure_stage': 'Parse'
                    }
                )
                
                conn.close()
                return (False, None)
            
            # âœ… è§£ææˆåŠŸï¼Œä¿å­˜ç»“æœ
            cursor.execute('''
                UPDATE upload_transactions
                SET 
                    parsed_owner_name = ?,
                    parsed_customer_code = ?,
                    parsed_bank_name = ?,
                    parsed_statement_date = ?,
                    parsed_due_date = ?,
                    parsed_statement_total = ?,
                    parsed_minimum_payment = ?,
                    status = 'PendingAttribution'
                WHERE transaction_uuid = ?
            ''', (
                parsed_data.get('owner_name'),
                parsed_data.get('customer_code'),
                parsed_data.get('bank_name'),
                parsed_data.get('statement_date'),
                parsed_data.get('due_date'),
                parsed_data.get('statement_total'),
                parsed_data.get('minimum_payment'),
                transaction_uuid
            ))
            
            conn.commit()
            conn.close()
            
            self._log_state_change(
                transaction_uuid,
                'PendingParse',
                'PendingAttribution',
                'Parsing successful, all mandatory fields extracted',
                parsed_data
            )
            
            print(f"âœ… è§£ææˆåŠŸ")
            print(f"   ä¸»äºº: {parsed_data.get('owner_name')}")
            print(f"   é“¶è¡Œ: {parsed_data.get('bank_name')}")
            print(f"   æ—¥æœŸ: {parsed_data.get('statement_date')}")
            
            return (True, parsed_data)
            
        except Exception as e:
            # âŒ è§£æå¤±è´¥
            print(f"âŒ è§£æå¤±è´¥: {e}")
            
            self._update_transaction_status(
                transaction_uuid,
                'PendingReview',
                {
                    'review_required': 1,
                    'review_reason': f"Parse failed: {str(e)}",
                    'failure_stage': 'Parse'
                }
            )
            
            conn.close()
            return (False, None)
    
    # ========================================
    # Stage 4: Entity Attributionï¼ˆå½’å±è¯†åˆ«ï¼‰
    # Checkpoint 3: å®¢æˆ·åŒ¹é… + ç½®ä¿¡åº¦è¯„åˆ†
    # ========================================
    
    def checkpoint_3_attribute_entity(self, transaction_uuid: str) -> Tuple[bool, Optional[Dict]]:
        """
        æ£€æŸ¥ç‚¹3ï¼šå½’å±è¯†åˆ«
        
        Architectè¦æ±‚ï¼š
        - äº¤å‰å¼•ç”¨customersè¡¨
        - è®¡ç®—åŒ¹é…ç½®ä¿¡åº¦
        - ç½®ä¿¡åº¦ < 0.98 â†’ äººå·¥å®¡æ ¸
        
        Returns:
            (success, attribution_result)
        """
        print(f"\nğŸ” æ£€æŸ¥ç‚¹3ï¼šå½’å±è¯†åˆ«...")
        
        conn = self.get_connection()
        cursor = conn.cursor()
        
        # è·å–è§£æç»“æœ
        cursor.execute('''
            SELECT parsed_owner_name, parsed_customer_code
            FROM upload_transactions
            WHERE transaction_uuid = ?
        ''', (transaction_uuid,))
        
        result = cursor.fetchone()
        parsed_owner_name, parsed_customer_code = result
        
        # æŸ¥è¯¢customersè¡¨åŒ¹é…
        cursor.execute('''
            SELECT id, customer_code, name
            FROM customers
            WHERE LOWER(name) LIKE LOWER(?) 
               OR customer_code = ?
            LIMIT 5
        ''', (f'%{parsed_owner_name}%', parsed_customer_code))
        
        matches = cursor.fetchall()
        
        if not matches:
            # âš ï¸ æ— åŒ¹é…å®¢æˆ·ï¼Œè½¬äººå·¥å®¡æ ¸
            print(f"âš ï¸  æœªæ‰¾åˆ°åŒ¹é…å®¢æˆ·: {parsed_owner_name}")
            
            self._update_transaction_status(
                transaction_uuid,
                'PendingReview',
                {
                    'review_required': 1,
                    'review_reason': f"No customer match found for: {parsed_owner_name}",
                    'failure_stage': 'Attribution'
                }
            )
            
            conn.close()
            return (False, None)
        
        # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆç®€å•å®ç°ï¼šç²¾ç¡®åŒ¹é…=1.0ï¼Œæ¨¡ç³ŠåŒ¹é…<1.0ï¼‰
        best_match = matches[0]
        customer_id, customer_code, customer_name = best_match
        
        # ç½®ä¿¡åº¦è¯„åˆ†
        confidence = self._calculate_attribution_confidence(
            parsed_owner_name,
            parsed_customer_code,
            customer_name,
            customer_code
        )
        
        print(f"   æœ€ä½³åŒ¹é…: {customer_name} ({customer_code})")
        print(f"   ç½®ä¿¡åº¦: {confidence:.4f}")
        
        if confidence < self.MIN_CONFIDENCE_THRESHOLD:
            # âš ï¸ ç½®ä¿¡åº¦ä¸è¶³ï¼Œè½¬äººå·¥å®¡æ ¸
            print(f"âš ï¸  ç½®ä¿¡åº¦ä½äºé˜ˆå€¼({self.MIN_CONFIDENCE_THRESHOLD})")
            
            self._update_transaction_status(
                transaction_uuid,
                'PendingReview',
                {
                    'review_required': 1,
                    'review_reason': f"Low attribution confidence: {confidence:.4f} < {self.MIN_CONFIDENCE_THRESHOLD}",
                    'attributed_customer_id': customer_id,
                    'attributed_customer_code': customer_code,
                    'attribution_confidence': confidence,
                    'failure_stage': 'Attribution'
                }
            )
            
            conn.close()
            return (False, None)
        
        # âœ… å½’å±æˆåŠŸ
        self._update_transaction_status(
            transaction_uuid,
            'PendingClassification',
            {
                'attributed_customer_id': customer_id,
                'attributed_customer_code': customer_code,
                'attribution_confidence': confidence
            }
        )
        
        print(f"âœ… å½’å±æˆåŠŸ: {customer_name}")
        
        conn.close()
        return (True, {
            'customer_id': customer_id,
            'customer_code': customer_code,
            'customer_name': customer_name,
            'confidence': confidence
        })
    
    def _calculate_attribution_confidence(
        self,
        parsed_name: str,
        parsed_code: str,
        db_name: str,
        db_code: str
    ) -> float:
        """
        è®¡ç®—å½’å±ç½®ä¿¡åº¦
        
        è§„åˆ™ï¼š
        - customer_codeç²¾ç¡®åŒ¹é… = 1.0
        - nameç²¾ç¡®åŒ¹é…ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰= 1.0
        - nameåŒ…å«åŒ¹é… = 0.9
        - å…¶ä»– = 0.7
        """
        # ç²¾ç¡®åŒ¹é…customer_code
        if parsed_code and parsed_code.upper() == db_code.upper():
            return 1.0
        
        # ç²¾ç¡®åŒ¹é…nameï¼ˆå¿½ç•¥å¤§å°å†™å’Œç©ºæ ¼ï¼‰
        parsed_clean = parsed_name.upper().replace(' ', '')
        db_clean = db_name.upper().replace(' ', '')
        
        if parsed_clean == db_clean:
            return 1.0
        
        # åŒ…å«åŒ¹é…
        if parsed_clean in db_clean or db_clean in parsed_clean:
            return 0.9
        
        # æ¨¡ç³ŠåŒ¹é…
        return 0.7
    
    # ========================================
    # Stage 5: Business Classificationï¼ˆä¸šåŠ¡åˆ†ç±»ï¼‰
    # Checkpoint 4: è‡ªåŠ¨åˆ†ç±»
    # ========================================
    
    def checkpoint_4_classify_business_type(
        self,
        transaction_uuid: str,
        business_type: Optional[str] = None
    ) -> Tuple[bool, str]:
        """
        æ£€æŸ¥ç‚¹4ï¼šä¸šåŠ¡åˆ†ç±»
        
        Args:
            transaction_uuid: äº¤æ˜“UUID
            business_type: ä¸šåŠ¡ç±»å‹ï¼ˆå¦‚æœç”¨æˆ·å·²æŒ‡å®šï¼‰
                          None = è‡ªåŠ¨åˆ†ç±»
                          'personal' / 'company' / 'mixed' = ç”¨æˆ·æŒ‡å®š
        
        Returns:
            (success, business_type)
        """
        print(f"\nğŸ” æ£€æŸ¥ç‚¹4ï¼šä¸šåŠ¡åˆ†ç±»...")
        
        if business_type:
            # ç”¨æˆ·å·²æŒ‡å®š
            print(f"âœ… ç”¨æˆ·æŒ‡å®šä¸šåŠ¡ç±»å‹: {business_type}")
            
            self._update_transaction_status(
                transaction_uuid,
                'ApprovedForStorage',
                {
                    'classified_business_type': business_type,
                    'classification_confidence': 1.0,
                    'classification_reason': 'User specified'
                }
            )
            
            return (True, business_type)
        
        # TODO: è‡ªåŠ¨åˆ†ç±»é€»è¾‘
        # ç®€å•å®ç°ï¼šé»˜è®¤ä¸ºpersonalï¼Œç­‰å¾…äººå·¥ç¡®è®¤
        
        conn = self.get_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT parsed_owner_name, attributed_customer_code
            FROM upload_transactions
            WHERE transaction_uuid = ?
        ''', (transaction_uuid,))
        
        result = cursor.fetchone()
        owner_name, customer_code = result
        
        # ç®€å•è§„åˆ™ï¼šå¦‚æœowner_nameåŒ…å«å…¬å¸å…³é”®å­—
        company_keywords = ['SDN BHD', 'PTY LTD', 'COMPANY', 'CORPORATION', 'INFINITE']
        is_company = any(kw in owner_name.upper() for kw in company_keywords)
        
        if is_company:
            classified_type = 'company'
            confidence = 0.9
        else:
            classified_type = 'personal'
            confidence = 0.85
        
        if confidence < self.MIN_CONFIDENCE_THRESHOLD:
            # è½¬äººå·¥å®¡æ ¸
            print(f"âš ï¸  åˆ†ç±»ç½®ä¿¡åº¦ä¸è¶³: {confidence:.4f}")
            
            self._update_transaction_status(
                transaction_uuid,
                'PendingReview',
                {
                    'review_required': 1,
                    'review_reason': f"Low classification confidence: {confidence:.4f}",
                    'classified_business_type': classified_type,
                    'classification_confidence': confidence,
                    'failure_stage': 'Classification'
                }
            )
            
            conn.close()
            return (False, classified_type)
        
        # âœ… åˆ†ç±»æˆåŠŸ
        self._update_transaction_status(
            transaction_uuid,
            'ApprovedForStorage',
            {
                'classified_business_type': classified_type,
                'classification_confidence': confidence,
                'classification_reason': 'Auto-classified based on keywords'
            }
        )
        
        print(f"âœ… è‡ªåŠ¨åˆ†ç±»: {classified_type} (ç½®ä¿¡åº¦: {confidence:.4f})")
        
        conn.close()
        return (True, classified_type)
    
    # ========================================
    # Stage 6: Dual-Write Storageï¼ˆåŒå†™å­˜å‚¨ï¼‰
    # Final Checkpoint: å¼ºåˆ¶åŒå†™
    # ========================================
    
    def final_checkpoint_dual_write_storage(
        self,
        transaction_uuid: str,
        file_storage_manager,
        file_integrity_service
    ) -> bool:
        """
        æœ€ç»ˆæ£€æŸ¥ç‚¹ï¼šåŒå†™å­˜å‚¨
        
        Architectè¦æ±‚ï¼š
        - å¿…é¡»åŒæ—¶å†™å…¥ä¸»å­˜å‚¨å’Œå¤‡ä»½
        - å¿…é¡»æ³¨å†Œåˆ°file_registry
        - ä»»ä½•å¤±è´¥ â†’ å›æ»š
        
        Returns:
            success
        """
        print(f"\nğŸ” æœ€ç»ˆæ£€æŸ¥ç‚¹ï¼šåŒå†™å­˜å‚¨...")
        
        conn = self.get_connection()
        cursor = conn.cursor()
        
        # è·å–äº‹åŠ¡ä¿¡æ¯
        cursor.execute('''
            SELECT 
                original_filename, file_checksum,
                attributed_customer_id, attributed_customer_code,
                classified_business_type,
                parsed_bank_name, parsed_statement_date
            FROM upload_transactions
            WHERE transaction_uuid = ? AND status = 'ApprovedForStorage'
        ''', (transaction_uuid,))
        
        result = cursor.fetchone()
        if not result:
            print(f"âŒ äº‹åŠ¡æœªæ‰¹å‡†å­˜å‚¨")
            conn.close()
            return False
        
        (original_filename, file_checksum, customer_id, customer_code,
         business_type, bank_name, statement_date) = result
        
        # è·å–éš”ç¦»åŒºæ–‡ä»¶
        quarantine_path = os.path.join(
            self.quarantine_dir,
            f"{transaction_uuid}_{original_filename}"
        )
        
        try:
            # 1. ç”Ÿæˆæ ‡å‡†è·¯å¾„
            from datetime import datetime as dt
            stmt_date = dt.fromisoformat(statement_date)
            
            final_path = file_storage_manager.generate_credit_card_path(
                customer_code,
                bank_name,
                '0000',  # å¡å·å4ä½ï¼ˆä»parsed_dataè·å–ï¼‰
                stmt_date
            )
            
            # 2. åŒå†™ï¼šä¸»å­˜å‚¨ + å¤‡ä»½
            # ä¸»å­˜å‚¨
            os.makedirs(os.path.dirname(final_path), exist_ok=True)
            import shutil
            shutil.copy2(quarantine_path, final_path)
            
            # å¤‡ä»½
            backup_path = final_path.replace('static/uploads', 'static/uploads_backup')
            os.makedirs(os.path.dirname(backup_path), exist_ok=True)
            shutil.copy2(quarantine_path, backup_path)
            
            # 3. æ³¨å†Œåˆ°file_registry
            file_uuid = file_integrity_service.register_file(
                file_path=final_path,
                customer_id=customer_id,
                customer_code=customer_code,
                business_type=business_type,
                file_category='credit_card_statement',
                original_filename=original_filename
            )
            
            if not file_uuid:
                raise Exception("Failed to register file in file_registry")
            
            # 4. æ›´æ–°äº‹åŠ¡çŠ¶æ€
            cursor.execute('''
                UPDATE upload_transactions
                SET 
                    final_file_path = ?,
                    backup_file_path = ?,
                    file_registry_id = (SELECT id FROM file_registry WHERE file_uuid = ?),
                    status = 'StorageComplete'
                WHERE transaction_uuid = ?
            ''', (final_path, backup_path, file_uuid, transaction_uuid))
            
            conn.commit()
            
            # 5. åˆ é™¤éš”ç¦»åŒºæ–‡ä»¶
            os.remove(quarantine_path)
            
            # è®°å½•çŠ¶æ€å˜æ›´
            self._log_state_change(
                transaction_uuid,
                'ApprovedForStorage',
                'StorageComplete',
                'File stored successfully with dual-write',
                {
                    'final_path': final_path,
                    'backup_path': backup_path,
                    'file_uuid': file_uuid
                }
            )
            
            print(f"âœ… å­˜å‚¨å®Œæˆï¼")
            print(f"   ä¸»è·¯å¾„: {final_path}")
            print(f"   å¤‡ä»½è·¯å¾„: {backup_path}")
            print(f"   File UUID: {file_uuid}")
            
            conn.close()
            return True
            
        except Exception as e:
            # å›æ»š
            print(f"âŒ å­˜å‚¨å¤±è´¥: {e}")
            
            # æ¸…ç†å·²åˆ›å»ºçš„æ–‡ä»¶
            if os.path.exists(final_path):
                os.remove(final_path)
            if os.path.exists(backup_path):
                os.remove(backup_path)
            
            self._update_transaction_status(
                transaction_uuid,
                'Failed',
                {
                    'failure_reason': f"Storage failed: {str(e)}",
                    'failure_stage': 'Storage'
                }
            )
            
            conn.close()
            return False
    
    # ========================================
    # å®Œæ•´Pipelineæ‰§è¡Œ
    # ========================================
    
    def execute_full_pipeline(
        self,
        file_path: str,
        original_filename: str,
        parser_service,
        file_storage_manager,
        file_integrity_service,
        business_type: Optional[str] = None,
        uploaded_by: Optional[str] = None
    ) -> Dict:
        """
        æ‰§è¡Œå®Œæ•´çš„ä¸Šä¼ Pipeline
        
        å¼ºåˆ¶æ€§é˜¶æ®µï¼ˆä¸å¯è·³è¿‡ï¼‰ï¼š
        1. Initiate Upload
        2. Checkpoint 1: Checksum Validation
        3. Checkpoint 2: Content Parsing
        4. Checkpoint 3: Entity Attribution
        5. Checkpoint 4: Business Classification
        6. Final Checkpoint: Dual-Write Storage
        
        Returns:
            ç»“æœå­—å…¸
        """
        print("=" * 80)
        print("ğŸš€ å¯åŠ¨å¼ºåˆ¶æ€§ä¸Šä¼ Pipeline")
        print("=" * 80)
        
        # Stage 1: å¯åŠ¨
        transaction_uuid = self.initiate_upload(
            file_path, original_filename, uploaded_by
        )
        
        # Stage 2: Checkpoint 1 - Checksum
        is_duplicate, duplicate_info = self.checkpoint_1_validate_checksum(transaction_uuid)
        if is_duplicate:
            return {
                'success': False,
                'reason': 'Duplicate file detected',
                'duplicate_info': json.loads(duplicate_info),
                'transaction_uuid': transaction_uuid
            }
        
        # Stage 3: Checkpoint 2 - Parse
        parse_success, parsed_data = self.checkpoint_2_parse_content(
            transaction_uuid, parser_service
        )
        if not parse_success:
            return {
                'success': False,
                'reason': 'Parsing failed, pending review',
                'transaction_uuid': transaction_uuid
            }
        
        # Stage 4: Checkpoint 3 - Attribution
        attribution_success, attribution_result = self.checkpoint_3_attribute_entity(
            transaction_uuid
        )
        if not attribution_success:
            return {
                'success': False,
                'reason': 'Attribution failed, pending review',
                'transaction_uuid': transaction_uuid
            }
        
        # Stage 5: Checkpoint 4 - Classification
        classification_success, classified_type = self.checkpoint_4_classify_business_type(
            transaction_uuid, business_type
        )
        if not classification_success:
            return {
                'success': False,
                'reason': 'Classification confidence too low, pending review',
                'transaction_uuid': transaction_uuid
            }
        
        # Stage 6: Final Checkpoint - Storage
        storage_success = self.final_checkpoint_dual_write_storage(
            transaction_uuid,
            file_storage_manager,
            file_integrity_service
        )
        
        if not storage_success:
            return {
                'success': False,
                'reason': 'Storage failed',
                'transaction_uuid': transaction_uuid
            }
        
        # âœ… Pipelineå®Œæˆï¼
        print("\n" + "=" * 80)
        print("âœ… ä¸Šä¼ Pipelineå®Œæˆï¼")
        print("=" * 80)
        
        return {
            'success': True,
            'transaction_uuid': transaction_uuid,
            'customer': attribution_result,
            'business_type': classified_type,
            'parsed_data': parsed_data
        }


# å…¨å±€å®ä¾‹
upload_orchestrator = UploadOrchestrator()

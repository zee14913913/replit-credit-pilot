#!/usr/bin/env python
"""
è¶…çº§å¿«é€Ÿçˆ¬è™« - 68å®¶é‡‘èæœºæ„å…¨é¢äº§å“æå–
ç­–ç•¥ï¼šç›´æ¥ä»äº§å“åˆ—è¡¨é¡µæå–ï¼Œæ— éœ€è®¿é—®æ¯ä¸ªè¯¦æƒ…é¡µ
ç›®æ ‡ï¼š100%å®Œæ•´æ€§ - ä¿¡ç”¨å¡ã€è´·æ¬¾ã€ODã€FDç­‰æ‰€æœ‰äº§å“
"""
import csv
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import psycopg2
from psycopg2.extras import execute_values
import os
from datetime import datetime
import re

DATABASE_URL = os.getenv('DATABASE_URL')
CSV_INPUT = "/home/runner/workspace/attached_assets/New é©¬æ¥è¥¿äºšè´·æ¬¾æœºæ„ä¸å¹³å°å…¨å®˜ç½‘_å®Œæ•´ç‰ˆ.csv_1762667764316.csv"

# äº§å“è·¯å¾„ - é’ˆå¯¹æ¯å®¶é“¶è¡Œå°è¯•
PRODUCT_PATHS = {
    'credit_card': ['/personal/cards', '/Cards/Credit-Cards', '/credit-cards', '/cards', '/en/cards', '/my/en/personal/cards'],
    'personal_loan': ['/personal/loans', '/personal-loans', '/personal-financing', '/cash-loan', '/my/en/personal/loans'],
    'home_loan': ['/personal/home-loans', '/home-loans', '/housing-loan', '/mortgage', '/property-financing'],
    'car_loan': ['/personal/car-loan', '/auto-loan', '/hire-purchase', '/vehicle-financing'],
    'business_loan': ['/business/loans', '/business/financing', '/sme', '/sme-loans', '/business-financing'],
    'overdraft': ['/personal/overdraft', '/overdraft', '/od'],
    'fixed_deposit': ['/personal/fixed-deposit', '/fixed-deposit', '/fd', '/deposits', '/time-deposit'],
}

def load_institutions():
    """åŠ è½½68å®¶æœºæ„"""
    institutions = []
    with open(CSV_INPUT, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        next(reader)  # Skip header
        for row in reader:
            if len(row) >= 2:
                institutions.append({'name': row[0], 'website': row[1]})
    return institutions

def extract_rate(text):
    """æå–åˆ©ç‡"""
    # åŒ¹é… "X.XX%" or "X%" æ ¼å¼
    rate_match = re.search(r'(\d+\.?\d*)\s*%', text)
    if rate_match:
        return rate_match.group(0)
    return 'è¯·è”ç³»é“¶è¡Œ'

def classify_loan_type(product_name, url_path=''):
    """åˆ†ç±»äº§å“ç±»å‹"""
    text = (product_name + ' ' + url_path).lower()
    
    if 'credit card' in text or 'kad kredit' in text or '/card' in text:
        return 'CREDIT_CARD'
    elif 'home' in text or 'housing' in text or 'mortgage' in text or 'property' in text:
        return 'HOME_LOAN'
    elif 'personal' in text or 'cash loan' in text:
        return 'PERSONAL_LOAN'
    elif 'car' in text or 'auto' in text or 'vehicle' in text or 'hire purchase' in text:
        return 'CAR_LOAN'
    elif 'business' in text or 'sme' in text or 'commercial' in text:
        return 'SME_LOAN'
    elif 'deposit' in text or 'fd' in text or 'time deposit' in text:
        return 'FIXED_DEPOSIT'
    elif 'overdraft' in text or 'od' in text:
        return 'OVERDRAFT'
    elif 'refinance' in text or 'refinancing' in text:
        return 'REFINANCE'
    elif 'debt consolidation' in text:
        return 'DEBT_CONSOLIDATION'
    else:
        return 'OTHER'

def extract_products_from_page(url, company_name):
    """ä»å•ä¸ªé¡µé¢æå–æ‰€æœ‰äº§å“"""
    products = []
    
    try:
        response = requests.get(url, timeout=10, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0'
        })
        
        if response.status_code != 200:
            return products
        
        soup = BeautifulSoup(response.text, 'html.parser')
        text = soup.get_text()
        
        # ç­–ç•¥1: æå–æ‰€æœ‰headingæ ‡ç­¾ï¼ˆäº§å“åç§°é€šå¸¸åœ¨headingä¸­ï¼‰
        for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5']):
            heading_text = heading.get_text(strip=True)
            
            # è¿‡æ»¤ï¼šé•¿åº¦åˆç†ä¸”åŒ…å«äº§å“å…³é”®è¯
            if 5 < len(heading_text) < 150:
                if any(kw in heading_text.lower() for kw in [
                    'card', 'loan', 'financing', 'deposit', 'overdraft', 'mortgage',
                    'cash', 'personal', 'business', 'sme', 'home', 'auto', 'car'
                ]):
                    # æå–å‘¨å›´çš„åˆ©ç‡ä¿¡æ¯
                    rate = 'è¯·è”ç³»é“¶è¡Œ'
                    parent = heading.find_parent(['div', 'section', 'article'])
                    if parent:
                        parent_text = parent.get_text()
                        rate = extract_rate(parent_text)
                    
                    # æå–äº§å“é“¾æ¥
                    product_url = url
                    link = heading.find('a', href=True)
                    if link:
                        product_url = urljoin(url, link.get('href'))
                    
                    products.append({
                        'company': company_name,
                        'product_name': heading_text,
                        'loan_type': classify_loan_type(heading_text, url),
                        'rate': rate,
                        'source_url': product_url,
                        'required_doc': 'è¯·è®¿é—®é“¶è¡Œå®˜ç½‘',
                        'features': 'è¯·è®¿é—®é“¶è¡Œå®˜ç½‘',
                        'benefits': 'è¯·è®¿é—®é“¶è¡Œå®˜ç½‘',
                        'fees_charges': 'è¯·è”ç³»é“¶è¡Œ',
                        'tenure': 'è¯·è”ç³»é“¶è¡Œ',
                        'application_form_url': '',
                        'product_disclosure_url': '',
                        'terms_conditions_url': '',
                        'preferred_customer_type': 'æ‰€æœ‰å®¢æˆ·',
                        'scraped_at': datetime.now()
                    })
        
        # ç­–ç•¥2: æå–äº§å“å¡ç‰‡ï¼ˆé€šå¸¸åŒ…å«class="card"æˆ–class="product"ï¼‰
        for card in soup.find_all(['div', 'article'], class_=lambda x: x and (
            'card' in str(x).lower() or 'product' in str(x).lower()
        )):
            # æå–äº§å“åç§°
            name_tag = card.find(['h1', 'h2', 'h3', 'h4', 'h5'])
            if name_tag:
                name = name_tag.get_text(strip=True)
                if 5 < len(name) < 150:
                    card_text = card.get_text()
                    rate = extract_rate(card_text)
                    
                    link = card.find('a', href=True)
                    product_url = urljoin(url, link.get('href')) if link else url
                    
                    products.append({
                        'company': company_name,
                        'product_name': name,
                        'loan_type': classify_loan_type(name, url),
                        'rate': rate,
                        'source_url': product_url,
                        'required_doc': 'è¯·è®¿é—®é“¶è¡Œå®˜ç½‘',
                        'features': 'è¯·è®¿é—®é“¶è¡Œå®˜ç½‘',
                        'benefits': 'è¯·è®¿é—®é“¶è¡Œå®˜ç½‘',
                        'fees_charges': 'è¯·è”ç³»é“¶è¡Œ',
                        'tenure': 'è¯·è”ç³»é“¶è¡Œ',
                        'application_form_url': '',
                        'product_disclosure_url': '',
                        'terms_conditions_url': '',
                        'preferred_customer_type': 'æ‰€æœ‰å®¢æˆ·',
                        'scraped_at': datetime.now()
                    })
    
    except Exception as e:
        pass
    
    return products

def scrape_institution(company_name, base_url):
    """çˆ¬å–å•ä¸ªæœºæ„çš„æ‰€æœ‰äº§å“"""
    print(f"\nğŸ¦ {company_name}")
    print(f"   {base_url}")
    
    all_products = []
    
    # éå†æ‰€æœ‰äº§å“ç±»å‹çš„è·¯å¾„
    for category, paths in PRODUCT_PATHS.items():
        for path in paths:
            url = urljoin(base_url, path)
            products = extract_products_from_page(url, company_name)
            if products:
                all_products.extend(products)
                print(f"   âœ… {path} â†’ {len(products)} products")
            time.sleep(0.2)  # ç¤¼è²Œå»¶è¿Ÿ
    
    # å»é‡
    seen = set()
    unique_products = []
    for p in all_products:
        key = f"{p['company']}_{p['product_name']}"
        if key not in seen:
            seen.add(key)
            unique_products.append(p)
    
    print(f"   ğŸ“¦ æ€»è®¡: {len(unique_products)} ä¸ªäº§å“")
    return unique_products

def save_to_db(products):
    """æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“"""
    if not products:
        return
    
    con = psycopg2.connect(DATABASE_URL)
    cur = con.cursor()
    
    sql = """
        INSERT INTO loan_products_ultimate(
            company, loan_type, product_name, required_doc, features, benefits,
            fees_charges, tenure, rate, application_form_url, product_disclosure_url,
            terms_conditions_url, preferred_customer_type, source_url, scraped_at
        ) VALUES %s
        ON CONFLICT (company, product_name) DO UPDATE SET
            loan_type = EXCLUDED.loan_type,
            rate = EXCLUDED.rate,
            source_url = EXCLUDED.source_url,
            scraped_at = EXCLUDED.scraped_at
    """
    
    items = [(
        p['company'], p['loan_type'], p['product_name'], p['required_doc'],
        p['features'], p['benefits'], p['fees_charges'], p['tenure'], p['rate'],
        p['application_form_url'], p['product_disclosure_url'], p['terms_conditions_url'],
        p['preferred_customer_type'], p['source_url'], p['scraped_at']
    ) for p in products]
    
    execute_values(cur, sql, items)
    con.commit()
    cur.close()
    con.close()
    print(f"   ğŸ’¾ å·²ä¿å­˜åˆ°æ•°æ®åº“")

def main():
    print("=" * 80)
    print("ğŸš€ è¶…çº§å¿«é€Ÿçˆ¬è™« - 68å®¶é‡‘èæœºæ„å…¨é¢äº§å“æå–")
    print("   ç›®æ ‡: 100%å®Œæ•´æ€§ - ä¿¡ç”¨å¡ã€è´·æ¬¾ã€ODã€FDç­‰æ‰€æœ‰äº§å“")
    print("=" * 80)
    
    # æ¸…ç©ºæ•°æ®åº“
    con = psycopg2.connect(DATABASE_URL)
    cur = con.cursor()
    cur.execute("TRUNCATE TABLE loan_products_ultimate RESTART IDENTITY;")
    con.commit()
    cur.close()
    con.close()
    print("âœ… æ•°æ®åº“å·²æ¸…ç©º\n")
    
    institutions = load_institutions()
    print(f"ğŸ“‹ å…± {len(institutions)} å®¶æœºæ„\n")
    
    total_products = 0
    for idx, inst in enumerate(institutions, 1):
        print(f"\n[{idx}/{len(institutions)}]", end=" ")
        try:
            products = scrape_institution(inst['name'], inst['website'])
            if products:
                total_products += len(products)
                save_to_db(products)
        except Exception as e:
            print(f"   âŒ é”™è¯¯: {str(e)[:50]}")
        
        time.sleep(0.5)  # ç¤¼è²Œå»¶è¿Ÿ
    
    print("\n" + "=" * 80)
    print(f"ğŸ‰ å®Œæˆï¼æ€»è®¡: {total_products} ä¸ªäº§å“")
    print("=" * 80)
    
    # æœ€ç»ˆç»Ÿè®¡
    con = psycopg2.connect(DATABASE_URL)
    cur = con.cursor()
    
    cur.execute("SELECT COUNT(*) FROM loan_products_ultimate")
    total = cur.fetchone()[0]
    
    cur.execute("SELECT loan_type, COUNT(*) FROM loan_products_ultimate GROUP BY loan_type ORDER BY COUNT(*) DESC")
    breakdown = cur.fetchall()
    
    print(f"\nğŸ“Š æ•°æ®åº“ç»Ÿè®¡:")
    print(f"   æ€»äº§å“æ•°: {total}")
    print(f"\n   åˆ†ç±»ç»Ÿè®¡:")
    for loan_type, count in breakdown:
        print(f"   - {loan_type}: {count}")
    
    cur.close()
    con.close()

if __name__ == '__main__':
    main()
